<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yanxuanshaozhu.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Lecture 01 2022&#x2F;01&#x2F;18">
<meta property="og:type" content="article">
<meta property="og:title" content="COSI 131A Operating System">
<meta property="og:url" content="https://yanxuanshaozhu.github.io/2022/01/22/COSI-131A-Operating-System/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:description" content="Lecture 01 2022&#x2F;01&#x2F;18">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/hardware_hierarchy.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/process_state.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/process_state.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/memory_management.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/interrupt_driven_io_cycle.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/storage_hierarchy.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/os_services.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/process_state_book.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/process_scheduling_queues.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/context_switch.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/resource_allocation_graph.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/memory_address.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/paging_model.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/ia_32_translation.png">
<meta property="article:published_time" content="2022-01-23T03:50:39.000Z">
<meta property="article:modified_time" content="2022-04-28T16:22:47.000Z">
<meta property="article:author" content="Yanxuanshaozhu">
<meta property="article:tag" content="Operating System">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yanxuanshaozhu.github.io/images/Courses/COSI131A/hardware_hierarchy.png">

<link rel="canonical" href="https://yanxuanshaozhu.github.io/2022/01/22/COSI-131A-Operating-System/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>COSI 131A Operating System | My Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">My Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-portfolio">

    <a href="/portfolio/" rel="section"><i class="fas fa-wallet fa-fw"></i>Portfolio</a>

  </li>
        <li class="menu-item menu-item-memories">

    <a href="/memories/" rel="section"><i class="fas fa-camera fa-fw"></i>Memories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://yanxuanshaozhu.github.io/2022/01/22/COSI-131A-Operating-System/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Yanxuanshaozhu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          COSI 131A Operating System
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-22 22:50:39" itemprop="dateCreated datePublished" datetime="2022-01-22T22:50:39-05:00">2022-01-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-04-28 12:22:47" itemprop="dateModified" datetime="2022-04-28T12:22:47-04:00">2022-04-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Science/" itemprop="url" rel="index"><span itemprop="name">Computer Science</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>132k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2:01</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="lecture-01-20220118">Lecture 01 2022/01/18</h2>
<span id="more"></span>
<ol type="1">
<li>Why study OS
<ul>
<li>To learn the language of systems: abstraction</li>
<li>To understand issues of system design</li>
<li>To under stand how computers work</li>
</ul></li>
<li>What is OS
<ul>
<li>An OS is a program that acts as an intermediary between a user and
the computer hardware</li>
<li>Goals
<ul>
<li>Execute user programs and maker solving user problems easier</li>
<li>Make the computer system convenient to user</li>
<li>Use the computer hardware in an efficient manner</li>
</ul></li>
<li>Computer system components
<ul>
<li>Hardware: CPU, memory, I/O devices</li>
<li>OS</li>
<li>Application programs</li>
<li>Users: people, machines, other computers</li>
</ul></li>
<li>What the OS does
<ul>
<li>OS is a resource allocator: manage all resources</li>
<li>OS is a control program: control execution of programs</li>
</ul></li>
<li>Resource Management
<ul>
<li>Resource abstraction (top-down view)
<ul>
<li>Abstract from functionality of hardware, to achieve ease of use and
device independence</li>
</ul></li>
<li>Resource sharing (bottom-up view)
<ul>
<li>manage contention for resources</li>
</ul></li>
<li>The one program running at all times on a computer is the kernel,
everything else is either a system program or an application
program</li>
</ul></li>
</ul></li>
<li>Key OS concepts
<ul>
<li>Processes = programs + state
<ul>
<li>Program in execution, also include some state information</li>
<li>Context switch: change control from one process to another</li>
</ul></li>
<li>Concurrency
<ul>
<li>Used for performance</li>
<li>Programs do not corrupt, programs can achieve goals (fairness,
performance,...), do not deadlock</li>
</ul></li>
<li>Memory management
<ul>
<li>Each process has access to entire machine</li>
<li>Abstraction + sharing</li>
</ul></li>
<li>Device management
<ul>
<li>Each process has access to all devices</li>
<li>Abstraction + sharing</li>
</ul></li>
</ul></li>
<li>Overview of machine organization
<ul>
<li>CPU: a processor for interpreting and executing programs</li>
<li>Memory: for storing data and programs</li>
<li>Controller: mechanism for transferring data from/to outside world,
there is a controller for each device</li>
<li>Buss: set of wires, move data and instructions within units of the
machine
<ul>
<li>Buss should contains both data and address</li>
<li>Primary movement on buss
<ul>
<li>CPU <span class="math inline">\(\rightleftarrows\)</span>
memory</li>
<li>CPU <span class="math inline">\(\rightleftarrows\)</span> device
controllers</li>
<li>Memory <span class="math inline">\(\rightleftarrows\)</span> device
controllers (sometimes)</li>
</ul></li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-02-20220120">Lecture 02 2022/01/20</h2>
<ol type="1">
<li>CPU
<ul>
<li>CPU = ALU + Control Unit</li>
<li>ALU (arithmetic logic unit): performs computations</li>
<li>Control unit: interprets program instructions</li>
<li>CPU fetches, decodes and executes instructions on the correct data
<ul>
<li>Control unit makes sure the correct data is where is needs to
be</li>
<li>ALU: stores and performs operations on data</li>
</ul></li>
<li>CPU understands only assembly language</li>
</ul></li>
<li>ALU
<ul>
<li>ALU = Registers + Function unit</li>
<li>Registers: storage units for data (addresses, PC, operands, ...)
<ul>
<li>Registers are fast and expensive</li>
<li>Cetegories:
<ul>
<li>GPR (general purpose registers): set directly by the software,
stores data</li>
<li>SR (status register): set as a side-effect of software</li>
</ul></li>
<li>The amount of registers is the power to 2, typically 32-64</li>
<li>Each register holds a word, the size of a word is the amount that
could be stored in a register, it's also the amount of data that could
be transferred by one bus at a time</li>
</ul></li>
<li>Function unit: evaluate arithmetic and logic units
<ul>
<li>Operations in FU can affect bits in the SR</li>
<li>FU is controlled by signals from the CU, so it knows which operation
to perform</li>
</ul></li>
<li>Registers and units are connected through the CPU bus, timing of the
movement of data is controlled by CPU clock</li>
</ul></li>
<li>Generate assembly language
<ul>
<li>Higher level languages are converted to assembly language using a
compiler</li>
<li>Movement of data
<ul>
<li>Memory <span class="math inline">\(\rightarrow\)</span> Register:
<code>LOAD</code></li>
<li>Register <span class="math inline">\(\rightarrow\)</span> Memory:
<code>STORE</code></li>
<li>Memory <span class="math inline">\(\rightarrow\)</span> Disk:
<code>READ</code></li>
<li>Disk <span class="math inline">\(\rightarrow\)</span> Memory:
<code>WRITE</code></li>
</ul></li>
<li>System call: request to OS to perform service, change from user mode
to kernel mode</li>
<li>Source code <span class="math inline">\(\xrightarrow{compiler}\)</span> assembly code
<span class="math inline">\(\xrightarrow{assembler}\)</span> machine
language on hardware (executable) <span class="math inline">\(\xrightarrow{loader}\)</span> instructions on
memory</li>
</ul></li>
<li>Control unit
<ul>
<li>PC (program counter): a register for the CPU, stores the address
where the program is stored in memory</li>
<li>IR (instruction register): a register for the CPU, holds instruction
currently being executed or decoded</li>
<li>CU performs the "Fetch-Decode-Execute" cycle after instructions are
loaded into memory by the loader
<ul>
<li>Get the instruction whose address is at PC, store the instruction in
IR</li>
<li>Decode instruction in IR to identify operation type and inputs</li>
<li>Execute instructions with ALU</li>
<li>Increment PC (by 4 in the 32-bit configuration)and go back to fetch
another instruction</li>
<li>This cycle is controlled by the halt flag. THe halt flag is the
first bit in the SR, when it is set to CLEAR, the bit is set to 0, and
the program runs. When the last line of the program is executed, IR has
the halt instruction, and the halt flag is set to 1, then the program
stops <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PC &#x3D; &lt;start address&gt;</span><br><span class="line">IR &#x3D; M[PC]</span><br><span class="line">haltflag &#x3D; CLEAR   </span><br><span class="line">while (haltflag is CLEAR)</span><br><span class="line">  Execute (IR)</span><br><span class="line">  Increment (PC)</span><br><span class="line">  IR &#x3D; M[PC]</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
<li>Booting
<ul>
<li>OS is not in ROM, but on the disk
<ul>
<li>OS is large</li>
<li>OS needs to be changed when needed (update)</li>
</ul></li>
<li>BIOS (non-volatile firmware on flash memory of motherboards): part
of hardware, initializes hardware and runs the bootstrap loader</li>
<li>Bootstrap loader: part of software, resides in ROM, it loads OS into
memory</li>
<li>BL algorithm <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">R3 :&#x3D; 0       &#x2F;&#x2F; Size of transferred data</span><br><span class="line">R2 :&#x3D; SIZE_OR_TARGET  &#x2F;&#x2F; Size of OS</span><br><span class="line">while R3 &lt; R2 do</span><br><span class="line">  R1 :&#x3D; disk[FIXED_DISK_ADD + R3]</span><br><span class="line">  M[FIXED_MEM_ADD +R3] :&#x3D; R1</span><br><span class="line">  R3 : R3 + 4</span><br><span class="line">goto FIXED_MEM_ADD</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol>
<h2 id="lecture-03-20220125">Lecture 03 2022/01/25</h2>
<ol type="1">
<li>Assumption and notation
<ul>
<li>Assumptions
<ul>
<li>16 GPR (R1, ..., R16)</li>
<li>16 SR (S1, ..., S16)</li>
<li>Instruction size: 4 bytes</li>
</ul></li>
<li>Notation
<ul>
<li><code>M[x]</code>: contents of memory at address x</li>
<li><code>D[x]</code>: contents of disk at address x</li>
<li><code>Ri</code>: contents of GPR Ri</li>
<li>PC: program counter</li>
<li><code>&lt;-</code>: assignment</li>
</ul></li>
</ul></li>
<li>Load: transfer data from memory to cpu
<ul>
<li>Direct addressing
<ul>
<li>Syntax: <code>LOAD Ri, Addr</code></li>
<li>Meaning: <code>Ri &lt;- M[Addr]</code></li>
<li>Example: <code>LOAD R1, 3000</code></li>
</ul></li>
<li>Immediate operand
<ul>
<li>Syntax: <code>LOAD Ri, =Num</code></li>
<li>Meaning: <code>Ri &lt;- Num</code></li>
<li>Example: <code>Load R2, =100</code></li>
</ul></li>
<li>Index addressing
<ul>
<li>Syntax: <code>LOAD Ri, [Addr, Rj]</code></li>
<li>Meaning: <code>Ri &lt;- M[Addr + Rj]</code></li>
<li>Example: <code>Load R3, [3000, R4]</code></li>
</ul></li>
<li>Indirect addressing
<ul>
<li>Syntax: <code>LOAD Ri, @ Addr</code></li>
<li>Meaning: <code>Ri &lt;- M[M[Addr]]</code></li>
<li>Example: <code>Load R5, @ 3000</code></li>
</ul></li>
<li>Relative Addressing
<ul>
<li>Syntax: <code>LOAD Ri, $Num</code></li>
<li>Meaning: <code>Ri &lt;- M[PC + Num]</code></li>
<li>Example: <code>LOAD R6, $100</code>, <code>LOAD R7 $R8</code></li>
</ul></li>
</ul></li>
<li>Store: transfer data from cpu to memory
<ul>
<li>Direct addressing
<ul>
<li>Syntax: <code>STORE Ri, Addr</code></li>
<li>Meaning: <code>M[Addr] &lt;- Ri</code></li>
<li>Example: <code>STORE R1, 3000</code></li>
</ul></li>
<li>Index addressing
<ul>
<li>Syntax: <code>STORE Ri, [Addr, Rj]</code></li>
<li>Meaning: <code>M[Addr + Rj] &lt;- Ri</code></li>
<li>Example: <code>STORE R2, [3000, R3]</code></li>
</ul></li>
<li>Relative addressing
<ul>
<li>Syntax: <code>STORE Ri, $Num,</code></li>
<li>Meaning: <code>M[PC + Num] &lt;- Ri</code></li>
<li>Example: <code>STORE R4, $100</code>,
<code>STORE R5, $R6</code></li>
</ul></li>
</ul></li>
<li>Arithmetic operations
<ul>
<li>ADD
<ul>
<li>Syntax: <code>Add Ri, Rj</code></li>
<li>Meaning: <code>Ri &lt;-Ri + Rj</code></li>
</ul></li>
<li>SUB
<ul>
<li>Syntax: <code>SUB Ri, Rj</code></li>
<li>Meaning: <code>Ri &lt;- Ri - Rj</code></li>
</ul></li>
<li>MUL
<ul>
<li>Syntax: <code>MUL Ri, Rj</code></li>
<li>Meaning: <code>Ri &lt;- Ri * Rj</code></li>
</ul></li>
<li>DIV
<ul>
<li>Syntax: <code>DIV Ri, Rj</code></li>
<li>Meaning: <code>Ri &lt;- Ri / Rj</code> (integer division),
<code>Rj &lt;- Ri % Rj</code></li>
<li>Example <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LOAD R1, &#x3D; 11</span><br><span class="line">LOAD R2, &#x3D; 4</span><br><span class="line">DIV R1, R2   &#x2F;&#x2F; R1 &#x3D; 2, R2 &#x3D; 3</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>INC
<ul>
<li>Syntax: <code>INC Ri</code></li>
<li>Meaning: <code>Ri &lt;- Ri + 1</code></li>
</ul></li>
</ul></li>
<li>Labels
<ul>
<li>Syntax: <code>LOOP: instruction</code></li>
<li>This is the same meaning as the instruction alone, but it is used
with branch instructions to construct a loop</li>
<li>If the "LOOP" label appears at memory address 3000, then throughout
the code, "LOOP" is equivalent to 3000</li>
</ul></li>
<li>SKIP
<ul>
<li>Syntax: <code>SKIP</code></li>
<li>Meaning: time delay, cpu pauses for about 300 cpu cycles</li>
</ul></li>
<li>Branch
<ul>
<li>BR
<ul>
<li>Syntax: <code>BR label</code></li>
<li>Meaning: branch to label</li>
</ul></li>
<li>BLT/BGT/BLEQ/BGEQ
<ul>
<li>Syntax: <code>BLT Ri, Rj, label</code></li>
<li>Meaning: branch to label if Ri &lt; Rj</li>
</ul></li>
</ul></li>
<li>READ: transfer data from hardware to cpu
<ul>
<li>Direct addressing
<ul>
<li>Syntax: <code>READ Ri, DiskAddr</code></li>
<li>Meaning: <code>Ri &lt;-D[DiskAddr]</code></li>
<li>Example: <code>READ R1, 30000</code></li>
</ul></li>
<li>Index addressing
<ul>
<li>Syntax: <code>READ Ri, [DiskAddr, Rj]</code></li>
<li>Meaning: <code>Ri &lt;- D[DiskAddr + Rj]</code></li>
<li>Example: <code>Read R2, [30000, R3]</code></li>
</ul></li>
</ul></li>
<li>WRITE: transfer data from cpu to hardware
<ul>
<li>Direct addressing
<ul>
<li>Syntax: <code>WRITE Ri, DiskAddr</code></li>
<li>Meaning: <code>D[DiskAddr] &lt;- Ri</code></li>
<li>Example: <code>WRITE R1, 30000</code></li>
</ul></li>
<li>Index addressing
<ul>
<li>Syntax: <code>WRITE Ri, [DiskAddr, Rj]</code></li>
<li>Meaning: <code>D[DiskAddr + Rj] &lt;- Ri</code></li>
<li>Example: <code>WRITE R2, [30000, R3]</code></li>
</ul></li>
</ul></li>
<li>HALT
<ul>
<li>Syntax: <code>HALT</code></li>
<li>Meaning: <code>haltflag &lt;- 1</code></li>
</ul></li>
<li>Exercise
<ul>
<li>Given <span class="math inline">\(b\)</span> on disk at <span class="math inline">\(d_1\)</span>, <span class="math inline">\(p\)</span> on disk at <span class="math inline">\(d_2\)</span>, assume program is loaded into memory
at <span class="math inline">\(x\)</span>, store <span class="math inline">\(b^i\)</span> at <span class="math inline">\(x +
1000 + (i - 1) * 4\)</span>, for <span class="math inline">\(i \in
\{1,..., p\}\)</span> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">at x      READ R1, d1       &#x2F;&#x2F; b</span><br><span class="line">at x+ 4   READ R2, d2       &#x2F;&#x2F; p</span><br><span class="line">at x + 8  LOAD R3, &#x3D;1       &#x2F;&#x2F; i</span><br><span class="line">at x + 12 LOAD R4, &#x3D;4       &#x2F;&#x2F; step</span><br><span class="line">at x + 16 LOAD R5, &#x3D;1       &#x2F;&#x2F; res</span><br><span class="line">at x + 20 LOAD R6, &#x3D; 972    &#x2F;&#x2F; initial offset</span><br><span class="line">at x + 24 LOOP: MUL R5, R1  &#x2F;&#x2F; res *&#x3D; b</span><br><span class="line">at x + 28 STORE R5, $R6     &#x2F;&#x2F; M[offset] &#x3D; res</span><br><span class="line">at x + 32 Add R6, R4        &#x2F;&#x2F; offset +&#x3D; step</span><br><span class="line">at x + 36 INC R3            &#x2F;&#x2F; i+&#x3D; 1</span><br><span class="line">at x + 40 BLEQ R3, R2, LOOP &#x2F;&#x2F; while i &lt;&#x3D; p</span><br><span class="line">at x + 44 HALT              &#x2F;&#x2F; stop</span><br></pre></td></tr></table></figure> ## Lecture 04 2022/01/27</li>
</ul></li>
<li>Binary representation
<ul>
<li>Convert decimal number to binary form: <span class="math inline">\(x
= _{i = 0}^{k} 2^{i}\)</span></li>
</ul></li>
<li>Memory
<ul>
<li>Memory is divided into cells, typical cell size is 1 byte = 8 bits,
each cell has an address</li>
<li>If the computer is m-bit (the length of cpu register is m), then it
can be mapped to <span class="math inline">\(2^{m}\)</span> integers,
each representing one memory address, so there size of memory space is
<span class="math inline">\(2^{m}\)</span> bytes. Take <span class="math inline">\(m = 32\)</span>, then the size of memory space is
around <span class="math inline">\(4GB\)</span></li>
<li>There are three registers used to handle data transaction between
cpu and memory
<ul>
<li>MAR (memory address register): stores memory address from which data
will be fetched or to which data will be stored</li>
<li>MDR (memory data register): stores data being transferred to and
from memory address specified by MAR, it can both load data and store
data</li>
<li>CMD (command register): indicates whether the command is load or
store</li>
<li>Example <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">STORE R1, 3000</span><br><span class="line">MDR &lt;- R1</span><br><span class="line">MAR &lt;- 3000</span><br><span class="line">CMD &lt;- WRITE</span><br><span class="line"></span><br><span class="line">LOAD R1, 3000</span><br><span class="line">MDR &lt;- R1</span><br><span class="line">MAR &lt;- 3000</span><br><span class="line">CMD &lt;- READ</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Memory categories
<ul>
<li>Main memory: CPU can access directly, this is volatile (when the
computer is shut down, the data in the main memory is lost)</li>
<li>Secondary storage: extension of main memory that provides large,
nonvolatile storage capacity</li>
</ul></li>
<li>Hardware hierarchy <img src="/images/Courses/COSI131A/hardware_hierarchy.png"></li>
</ul></li>
<li>Devices and controllers
<ul>
<li>I/O devices: keyboard, mouse, monitor, printer, disk, ...</li>
<li>Device controllers: hardware interface between device and bus,
handlers the I/O signals of CPU, device controllers are physical chips,
but device drivers are softwares</li>
</ul></li>
<li>Interrupts
<ul>
<li>One I/O interrupt example:
<ul>
<li>CPU issues a read request to the device through the device
driver</li>
<li>Device driver signals the device controller to get the data while
CPU does other work</li>
<li>Device will interrupt the CPU when ready to transfer the data</li>
<li>CPU transfers control to the device</li>
<li>Device transfers the data to CPU</li>
</ul></li>
<li>A trap/exception is a software-generated interrupt caused either by
an error or a user request</li>
<li>How interrupt notifies CPU
<ul>
<li>Method 1: interrupt request flag(IR flag)
<ul>
<li>One status register directly wired to the device controller's DONE
flag, when the DONE flag is set to be true, the CPU is notified through
the IR flag</li>
<li>For this approach, if there are multiple devices in the computer,
cpu cannot know which one sends the interrupt request, so it needs to
ask each device in turn (called polling procedure), which slows
interrupt processing</li>
</ul></li>
<li>Method 2: interrupt vectors
<ul>
<li>If the length of the interrupt vectors is <span class="math inline">\(n\)</span>, then the cpu can distinguish <span class="math inline">\(2^{n} - 1\)</span> devices, because we need one
bit combination to represent that there is no interrupt at all</li>
<li>The interrupt vector is part of the hardware</li>
</ul></li>
</ul></li>
<li>Fetch-decode-execute with interrupt involved <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">PC &#x3D; &lt;start address&gt;</span><br><span class="line">IR &#x3D; M[PC]</span><br><span class="line">haltflag &#x3D; CLEAR   </span><br><span class="line">while (haltflag is CLEAR)</span><br><span class="line">  Execute (IR)</span><br><span class="line">  Increment (PC)</span><br><span class="line">  if (interrupt request)</span><br><span class="line">    M[0] &lt;- PC               &#x2F;&#x2F; reserved to store PC</span><br><span class="line">    PC &lt;- M[1]&#x2F; PC &lt;- M[i]   &#x2F;&#x2F; if IR flag is used, use M[1] is the interrupt handler, if interrupt vector is used, use M[i] as the interrupt handler for ith device</span><br><span class="line">  IR &#x3D; M[PC]</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol>
<h2 id="lecture-05-20220201">Lecture 05 2022/02/01</h2>
<ol type="1">
<li>Respond to interrupt
<ul>
<li>Procedures
<ul>
<li>Save processor state (PC, registers, ...)</li>
<li>Identify interrupting device and sets PC to address the appropriate
device handler</li>
<li>Device handler performs data transfer from device controller
registers to cpu</li>
</ul></li>
<li>Interrupt during interrupt
<ul>
<li>Problem 1: the later interrupt overwrites M[0]
<ul>
<li>PC = 108 =&gt; 1st interrupt =&gt; M[0] &lt;- 108, PC &lt;-
M[1]</li>
<li>2nd interrupt =&gt; M[0] &lt;- M[1], PC &lt;- M[2]</li>
<li>When the 2nd interrupt id done, PC &lt;- M[1], but we lost the first
PC &lt;- 108</li>
</ul></li>
<li>Solution: reserve additional memory for PC values, so different
interrupt store their respective PC at different memory addresses</li>
<li>Problem 2: wrong timing leads to unhandled interrupts
<ul>
<li>Two interrupt arrives almost at the same time, then M[0] is set to
the PC for the 2nd interrupt, then the 1st interrupt is unhandled</li>
<li>Solution: set an interrupt disable other interrupts until cpu
recognizes which devices are interrupting</li>
</ul></li>
</ul></li>
</ul></li>
<li>Kernel vs User mode
<ul>
<li>OS and users share the hardware and software resources of a computer
system</li>
<li>OS should be able to protect the integrity of the computer, so OS
needs to distinguish between kernel mode and user mode</li>
<li>Switch between 2 modes of operation: a mode bit on one register:
<ul>
<li>Uer mode: execution in user role, limited instructions, no
privileged instructions</li>
<li>Kernel mode: execution in OS role, privileged instructions are
allowed, (interrupt, trap, fault are handled in this mode)</li>
</ul></li>
<li>All I/O instructions are privileged, so there is switch between user
mode and kernel mode</li>
<li>To prevent infinite loop/ process hogging resources, there is also
transition from user to kernel mode, a timer is involved</li>
<li>When the program logic is focused, the user does not need to care
about which call triggers the switch between modes, but if efficiency is
an issue, calls without switch between modes are preferred</li>
</ul></li>
<li>Processes
<ul>
<li>Processes are abstraction of the CPU and the main memory</li>
<li>Process is an abstraction of the cpu
<ul>
<li>It gives an illusion of a uni-programming environment, every process
acts as if it has exclusive access to cpu and memory (in reality, cpu is
time-sliced, memory is space-sliced)</li>
<li>Multiple processes increase cpu utilization</li>
<li>Multiple processes reduce latency</li>
</ul></li>
<li>A thread is a lightweight process, it's an abstraction of the cpu,
but not of the main memory</li>
<li>Threads are mostly cooperative, processes are mostly
independent</li>
<li>A process is more than the program code, it consists of code
section, data section, stack, registers(include the PC)</li>
</ul></li>
</ol>
<h2 id="lecture-06-20220204">Lecture 06 2022/02/04</h2>
<ol type="1">
<li>Process state
<ul>
<li>Processes are not always running, they have different states</li>
<li>State:
<ul>
<li>New: process is being created</li>
<li>Running: instructions are being executed</li>
<li>Waiting: process is waiting for some event to occur</li>
<li>Ready: process is waiting to be assigned the cpu</li>
<li>Terminated: process has finished execution <img src="/images/Courses/COSI131A/process_state.png"></li>
</ul></li>
</ul></li>
<li>Process control block
<ul>
<li>Each process is represented in the os by a process control block,
also called a task control block. It contains many information
associated with a specific process</li>
<li>Information
<ul>
<li>Process state: new, running, etc.</li>
<li>Process id</li>
<li>PC</li>
<li>Registers</li>
<li>Memory limits</li>
<li>Other information: list of opened files, etc.</li>
</ul></li>
<li>Context switch:
<ul>
<li>Process with access to cpu switched out (due to interrupt or system
call), with state copied to pcb, another ready process switched in, also
with state copied from pcb</li>
<li>There is a queue managing processes, if a process is switched out,
then it's enqueued, then another process is dequeued from the scheduling
queue based on some criterion and switched in</li>
<li>Pure overhead: during context switching, no useful work is done in
cpu</li>
</ul></li>
</ul></li>
<li>Process scheduling
<ul>
<li>Multiprogramming goal: maximize cpu utilization and enable users
interact with their running programs. This is achieved by process
scheduler, it selects the progress for program execution on the cpu <img src="/images/Courses/COSI131A/process_state.png"></li>
<li>Scheduler
<ul>
<li>Long term scheduler (job selector): moves process from disk to main
memory
<ul>
<li>Select which processes should be moved into ready queue</li>
<li>Control degree of multiprogramming (# of processes in memory)</li>
<li>Invoked infrequently, can be slow</li>
<li>Need to make a mix of I/O bound processes (spend more time doing
I/O)and cpu bound processes (spends more time doing computation)</li>
</ul></li>
<li>Short term scheduler: selects one process from memory and allocate
the cpu to it
<ul>
<li>Similar to interrupt handler</li>
<li>Select processes from ready queue</li>
<li>Invoked frequently, should be fast</li>
</ul></li>
</ul></li>
</ul></li>
<li>Process and memory
<ul>
<li>Process address space contains text segment, data segment and stack
segment</li>
<li>Abstraction
<ul>
<li>The process has access to whole memory, in reality memory is
partitioned amongst processes</li>
<li>Memory is contiguous, in reality memory consists of non-contiguous
chunks (if a process is terminated, its memory is free, then there are
empty chunks)</li>
</ul></li>
<li>Abstraction 1: process has exclusive access to all memory that it
can address
<ul>
<li>Exclusivity: each process is assigned a base register and limit
register defining its address space, so the memory space is [base
register, base register + limit register]</li>
<li>Access to entire address space: in reality, the entire memory space
occupied by processes can be larger than physical memory using the
virtual memory technique</li>
</ul></li>
<li>Abstraction 2: a process's address space is contiguous
<ul>
<li>Store process's address space as non-contiguous set of contiguous
chunks</li>
</ul></li>
</ul></li>
<li>Process creation
<ul>
<li>Process creation
<ul>
<li>Parent process creates child processes, forming a tree of
processes</li>
<li>Child process possibilities
<ul>
<li>Resource sharing: child process shares all sources from parent
process, subset of parents's, none resource with the parent</li>
<li>Execution: parent executes concurrently with children processes,
parent process waits until all children processes are terminated</li>
<li>Address space: child is a duplicate of parent, child has specified
new program loaded</li>
</ul></li>
<li>Process creation in UNIX: <code>fork()</code>,
<code>execv(path, argv)</code>
<ul>
<li><code>fork()</code>
<ul>
<li>Creates new child processes by creating a copy of executing process
within new address space, return 0 to newly created child process and
return the pid of the child process to the parent</li>
<li>Shared memory are shared between parent and child processes, stack
and heap are copied between parent and child processes</li>
</ul></li>
<li><code>execv(path, argv)</code>: replaces the process memory space
with respect to specified program in path and arguments argv</li>
<li>Example 1 <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> vaule = <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">pid_t</span> pid;</span><br><span class="line"></span><br><span class="line">  pid = fork();</span><br><span class="line">  <span class="keyword">if</span> (pid == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">/*Only child executes*/</span></span><br><span class="line">    value += <span class="number">15</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (pid &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">/*Only parent executes*/</span></span><br><span class="line">    wait(<span class="literal">NULL</span>);</span><br><span class="line">    <span class="comment">/*This should print 5*/</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Parent: value = %d\n&quot;</span>, value);  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Example 2 <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  fork();</span><br><span class="line">  fork(); <span class="comment">/*Both child and parent execute this*/</span></span><br><span class="line">  fork(); <span class="comment">/*Both child and parent execute this*/</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-07-20220207">Lecture 07 2022/02/07</h2>
<ol type="1">
<li>Processes
<ul>
<li>The process id for child is larger thant that of the parent process,
but may not be consecutive</li>
<li>They have separate copies of data and separate PC</li>
<li>Global variables are the same after copy, but if one processes
changes it, other processes cannot notice the change</li>
<li>Process termination
<ul>
<li>Exit: When a process executes last statement and asks OS to kill
it</li>
<li>Abort: parent process terminates execution of child process</li>
</ul></li>
</ul></li>
<li>Threads
<ul>
<li>Every thread has its own registers, PC and stack, code and data can
be shared</li>
<li>Motivation for threads
<ul>
<li>Multi-processing is expensive, processes have separate memory, so
each time all state information needs to be copied</li>
<li>Multi-threading is cheaper, only stack, PC, registers need to be
copied</li>
</ul></li>
<li>Benefits or threads
<ul>
<li>Responsiveness: interactive program can keep running even if one
thread blocks</li>
<li>Resource sharing: code, address space sharing comes for free with
threads, for processes, code copying required for former, IPC for
latter</li>
<li>Economy: slightly cheaper to context switch, much cheaper to
create</li>
<li>Utilization of multiprocessors, better parallelism and
scalability</li>
</ul></li>
<li>Thread Control Blocks (TCB)
<ul>
<li>Break the PCB into two pieces:
<ul>
<li>Information related to process execution in TCB: thread register,
stack pointer, cpu registers, cpu scheduling information, pending I/O
information</li>
<li>Other information associated with process: memory management
information (heap, global variables), accounting information</li>
</ul></li>
</ul></li>
</ul></li>
<li>User threads v.s. kernel threads
<ul>
<li>Kernel thread: managed by OS with multi-threading architecture, it's
the unit of execution that is scheduled by the kernel on the cpu</li>
<li>User thread: implemented with treads library, supported above the
kernel and are managed without kernel support</li>
<li>The mapping between user thread and kernel thread can be
many-to-one, one-to-one and many-to-many</li>
<li>Pros and cons
<ul>
<li>User thread
<ul>
<li>Pros
<ul>
<li>Can be implemented on an os that does not support threads</li>
<li>Thread switching is faster trapping to the kernel</li>
<li>Thread scheduling is very fast</li>
<li>Each process can have its own scheduling algorithm</li>
</ul></li>
<li>Cons
<ul>
<li>Blocking system calls (if one thread sends a system call, the os
does not know the process has multi-threading, so the whole process is
blocked)</li>
<li>Page faults</li>
<li>Threads need to voluntarily give up the cpu for
multiprogramming</li>
</ul></li>
</ul></li>
<li>Kernel thread
<ul>
<li>Pros
<ul>
<li>No run-time system needed in each process</li>
<li>No thread table in each process</li>
<li>Blocking system calls are not a problem (kernel knows there is
multi-threading in the process, so cpu scheduler assigns another thread
to run)</li>
</ul></li>
<li>Cons
<ul>
<li>If thread operations are common, much kernel overhead will be
incurred</li>
<li>If a signal is sent to a process, should the process handles it or
assigns a thread to handle it?</li>
<li>Slower that user-space threads</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>Thread library in Java
<ul>
<li>Threads in Java are managed by JVM, mapping to kernel threads
depends on the OS</li>
<li>Thread creation
<ul>
<li>Extends <code>Thread</code> class, override <code>run()</code>
method <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// code here</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Thread thread = <span class="keyword">new</span> MyThread();</span><br><span class="line">    thread.start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li>Implements <code>Runnable</code> interface, override
<code>run()</code> method <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// code here</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Thread thread = <span class="keyword">new</span> Thread(<span class="keyword">new</span> MyThread());</span><br><span class="line">    thread.start();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-08-20220210">Lecture 08 2022/02/10</h2>
<ol type="1">
<li>Status of a thread
<ul>
<li>init, run, blocked, dead</li>
</ul></li>
<li>Interaction between parent and child thread
<ul>
<li>Execute concurrently: call <code>child.start()</code> in the parent
thread</li>
<li>Parent suspend until child is done: call <code>child.join()</code>
in the parent thread, notice <code>join</code> method can throw
<code>Interruptedexception</code>, so you should use a try-catch clause.
Notice <code>child.start()</code> should be called before
<code>child.join()</code> is called</li>
<li>Example <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyThread</span> <span class="keyword">implements</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span> (Thread.currentThread().isInterrupted()) &#123;</span><br><span class="line">          System.out.println(<span class="string">&quot;Child is interrupted!&quot;</span>);</span><br><span class="line">          <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Thread thread = <span class="keyword">new</span> Thread(<span class="keyword">new</span> MyThread());</span><br><span class="line">    thread.start();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      thread.interrupt();</span><br><span class="line">      thread.join();</span><br><span class="line">    &#125; <span class="keyword">catch</span>(InterruptedException e) &#123;</span><br><span class="line">      System.out.println(<span class="string">&quot;Parent interrupts child!&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    System.out.println(<span class="string">&quot;Parent is done!&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Cancel threads
<ul>
<li>Call <code>child.interrupt()</code> to interrupt the child thread,
it sets the interrupt flag in the child thread</li>
<li><code>Thread.currentThread().isInterrupted()</code> method in the
child thread: returns boolean value</li>
<li><code>interrupted()</code> method in the child thread: returns
boolean value, and clears the interrupt flag</li>
</ul></li>
<li>Inter-process communication
<ul>
<li>Most modern Os does not support inter-process communication</li>
<li>Independent processes are not affected by execution of other
processes</li>
<li>Communication methods
<ul>
<li>Message passing: direct/indirect messaging, asynchronous/synchronous
messaging</li>
<li>Shared memory</li>
</ul></li>
<li>Producer-consumer problem
<ul>
<li>Buffer size: unbounded/bounded</li>
</ul></li>
<li>Reader/writer problem</li>
<li>Mutual exclusion problem</li>
</ul></li>
</ol>
<h2 id="lecture-09-20220215">Lecture 09 2022/02/15</h2>
<ol type="1">
<li>Scheduling
<ul>
<li>Multiprogramming goal: avoid cpu idle time, some process is using
the cpu all the time</li>
<li>Typical process execution flow: cpu burst and I/O burst</li>
<li>CPU-bound and I/O-bound process</li>
<li>Two types of scheduling:
<ul>
<li>No-preemptive : processes only give up cpu voluntarily</li>
<li>Preemptive: processes also may be preempted by an interrupt</li>
</ul></li>
<li>Clock interrupt
<ul>
<li>This is how system keeps track of time, the interrupt happens at
periodic intervals</li>
<li>This is a hardware system clock on the motherboard, it's not the
same as the cpu clock</li>
<li>The clock interrupt has very high priority</li>
</ul></li>
</ul></li>
<li>Scheduling policies
<ul>
<li>Criteria for effectiveness of scheduling policies
<ul>
<li>Throughput
<ul>
<li>Measured in amount of processes completed/ time unit</li>
</ul></li>
<li>Turnaround time
<ul>
<li>Turnaround time = process completion time - process creation
time(arrival time)</li>
<li>Turnaround time = waiting time + execution time</li>
</ul></li>
<li>Waiting time
<ul>
<li>Waiting time = waiting time in the ready queue</li>
<li>If a process is preempted, then the waiting time is the sum of all
time waiting time</li>
<li>Compared with turnaround time, waiting time does not have a penalty
for processes with long execution time</li>
</ul></li>
<li>Response time
<ul>
<li>Response time = time of 1st response is produced- time of
creation</li>
</ul></li>
<li>Overhead
<ul>
<li>Time spent related to scheduling</li>
</ul></li>
<li>Fairness
<ul>
<li>How much variation in waiting time, want to avoid process
starvation</li>
</ul></li>
<li>Dispatch time
<ul>
<li>The time it takes to choose next running process</li>
<li>Dispatch time = schedule time + context switch time</li>
</ul></li>
</ul></li>
<li>Generally it's not possible to meet all performance criteria for one
scheduling policy</li>
<li>Measures
<ul>
<li>CPU utilization: the higher, the better</li>
<li>Throughput: the higher, the better</li>
<li>Turnaround time: the lower, the better</li>
<li>Waiting time: the lower, the better</li>
<li>Response time: the lower, the better</li>
</ul></li>
</ul></li>
<li>First Come First Serve(FCFS)
<ul>
<li>Idea: processes are served according their arrival order</li>
<li>This is a non-preemptive scheduling policy</li>
<li>Pros:
<ul>
<li>Easy to implement</li>
<li>No starvation, so it's fair</li>
</ul></li>
<li>Cons
<ul>
<li>Convoy effect: order of arrival determines performance. If a long
process arrives first, then the average turnaround time, average waiting
time will be long</li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-10-20220217">Lecture 10 2022/02/17</h2>
<ol type="1">
<li>Shortest Job First
<ul>
<li>Idea:
<ul>
<li>Non-preemptive version: If processes arrive at the same time, choose
the one with the minimal cpu burst execute first. If processes arrive at
different time, whenever a process finishes, select from the arrived
processes the one that has the minimal cup burst</li>
<li>Preemptive version: currently running process can be preempted if
new process arrives with shorter remaining CPU burst</li>
</ul></li>
<li>A problem: in reality, we do not know the amount of cpu burst, so we
need to use prediction
<ul>
<li>Equation: <span class="math inline">\(\tau_{n + 1} = \alpha t_{n} +
(1-\alpha) \tau_{n}\)</span>, where <span class="math inline">\(\tau_{n
+ 1}\)</span> is the next guess, <span class="math inline">\(\tau_{n}\)</span> is previous guess, <span class="math inline">\(t_{n}\)</span> is previous actual burst time,
<span class="math inline">\(\alpha\)</span> is a weighting factor</li>
<li>If <span class="math inline">\(\alpha = 0\)</span>, then <span class="math inline">\(\tau_{n + 1} = \tau_{n}\)</span>, which means
recent history does not count</li>
<li>If <span class="math inline">\(\alpha = 1\)</span>, then <span class="math inline">\(\tau_{n + 1} = t_{n}\)</span>, which means only
the recent history counts</li>
</ul></li>
<li>Pros
<ul>
<li>Optimal(minimal) for average turnaround time and for average waiting
time</li>
</ul></li>
<li>Cons
<ul>
<li>Starves processes with long cpu bursts</li>
<li>It's hard to predict cpu burst time</li>
</ul></li>
</ul></li>
<li>Priority-Based
<ul>
<li>Idea: the process withe the highest priority is executed first</li>
<li>SJF can be seen as a special case for priority-based scheduling in
that processes with shorter cpu burst are assigned higher
priorities</li>
<li>Priority-based scheduling can be either non-preemptive or
preemptive</li>
<li>Pros
<ul>
<li>Reflects relative importance of processes</li>
</ul></li>
<li>Cons
<ul>
<li>Can cause starvation of low priority processes (can use aging to
tackle this problem)</li>
</ul></li>
</ul></li>
<li>Round-robin
<ul>
<li>Each process gets a small unit of cpu time (time quantum), after
quantum has elapsed, the running process is preempted and added to the
end of ready queue. If a process finished in one time quantum, the next
process starts immediately</li>
<li>This is a preemptive only scheduling policy</li>
<li>If there are n processes, and time quantum is q, then in one round,
each process is executed for at most q/n time, each process waits up to
(n - 1)q/n time</li>
<li>If the time quantum is too large, then Round-robin becomes FCFS, if
the time quantum is too small, then there will be too many context
switch overheads</li>
<li>Pros
<ul>
<li>Fairness</li>
<li>Lower response time than SJF</li>
</ul></li>
<li>Cons
<ul>
<li>High context switch overhead</li>
<li>Higher turnaround time than SJF</li>
</ul></li>
</ul></li>
<li>Multilevel Queue
<ul>
<li>Idea: ready queue is partitioned into separate queues, each with its
own scheduling algorithm</li>
<li>Inter-queue scheduling
<ul>
<li>Fixed priority: each queue has its own priority, high priority
queues are served before low priority queues, this may lead to
starvation in low priority queues</li>
<li>Time-slice: each queue gets a reserved slice of cpu time to schedule
amongst its processes (e.g. 80% for higher priority queue, 20% for lower
priority queue, this can be achieved using round-robin inter-queue
scheduling algorithm, and use randomization with lottery system to
achieve the respective percentage)</li>
</ul></li>
<li>In UNIX, there are 32 queues, 0-7 are system queues, 8-31 are user
queues. User processes can be moved between different user queues by
changing its priority level</li>
<li>Multilevel Feedback Queue (MFQ)
<ul>
<li><p>Processes can move between queues</p></li>
<li><p>Use MFQ to mimic SJF</p>
<ul>
<li>Higher priority queue uses RR with low time quantum (short jobs can
finish here, other jobs are downgraded to lower priority queues after
short time quantum elapses)</li>
<li>Lower priority queue uses RR with high time quantum</li>
<li>Processes downgrade their priorities when they get older</li>
</ul></li>
<li><p>Example:<span class="math inline">\(Q_1\)</span> is RR with <span class="math inline">\(q = 75\)</span>, <span class="math inline">\(Q_2\)</span> is RR with <span class="math inline">\(q = 150\)</span>, <span class="math inline">\(Q_3\)</span> is FCFS( <span class="math inline">\(Q_i\)</span> is served if <span class="math inline">\(Q_1\)</span>, ..., <span class="math inline">\(Q_{i-1}\)</span> is empty). Processes are as
follows</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Process</th>
<th style="text-align: center;">Burst Time</th>
<th style="text-align: center;">Arrival Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">P1</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">P2</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">P3</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">P4</td>
<td style="text-align: center;">175</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">P5</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td style="text-align: center;">P6</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">250</td>
</tr>
</tbody>
</table>
<p>The result is</p>
<table>
<colgroup>
<col style="width: 10%">
<col style="width: 16%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 11%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr>
<th style="text-align: center;">Process</th>
<th style="text-align: center;">Arrival Time</th>
<th style="text-align: center;">Q1</th>
<th style="text-align: center;">Q2</th>
<th style="text-align: center;">Q3</th>
<th style="text-align: center;">Completion Time</th>
<th style="text-align: center;">Turnaround Time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">P1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0-75</td>
<td style="text-align: center;">400-450</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">450</td>
<td style="text-align: center;">450</td>
</tr>
<tr>
<td style="text-align: center;">P2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">75-125</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">125</td>
</tr>
<tr>
<td style="text-align: center;">P3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">125-200</td>
<td style="text-align: center;">450-600</td>
<td style="text-align: center;">825-1100</td>
<td style="text-align: center;">1100</td>
<td style="text-align: center;">1100</td>
</tr>
<tr>
<td style="text-align: center;">P4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">200-275</td>
<td style="text-align: center;">600-700</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">700</td>
<td style="text-align: center;">700</td>
</tr>
<tr>
<td style="text-align: center;">P5</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">275-350</td>
<td style="text-align: center;">700-825</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">825</td>
<td style="text-align: center;">575</td>
</tr>
<tr>
<td style="text-align: center;">P6</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">350-400</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">150</td>
</tr>
</tbody>
</table></li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-11-20220301">Lecture 11 2022/03/01</h2>
<ol type="1">
<li>Real-time scheduling
<ul>
<li>Real-time means process must/should complete by some deadline</li>
<li>Hard real-time
<ul>
<li>Definition: process must complete by some deadline, this requires
dedicated scheduler</li>
<li>Idea:
<ul>
<li>Determine feasible processes(deadline - current time <span class="math inline">\(\ge\)</span> cpu burst), redetermine at every
context switch</li>
<li>Greedy heuristic: choose process with min value of [deadline -
current time - cpu burst]</li>
</ul></li>
</ul></li>
<li>Soft real-time
<ul>
<li>Definition: process should complete by some deadline, this can be
integrated into Multi-level feedback queues</li>
<li>Idea: processes should not be demoted, requires strict
time-slices</li>
</ul></li>
</ul></li>
<li>Scheduling examples
<ul>
<li>Windows
<ul>
<li>Windows uses priority-based, doubly preemptive scheduling,
preemption occurs with both end of time quantum and arrival of
higher-priority thread</li>
<li>Thread priority has different components
<ul>
<li>Statically assigned priority class</li>
<li>Dynamically assigned relative priority (expiration of time quantum,
satisfaction of I/O)</li>
<li>FG processes are given 3 times priority of the BG ones</li>
</ul></li>
<li>Windows 7 add user-mode scheduling(UMS): applications create and
manage threads independent of kernel</li>
</ul></li>
<li>Linux
<ul>
<li>Linux has two classes of processes: real-time processes and
time-shared processes</li>
<li>Real-time processes: have soft deadline, have highest-priority, are
within same priority class, use FCFS or RR as scheduling policy</li>
<li>Timesharing processes: default ones, have credits(similar to aging),
initialize credits based on priority and history, lose credits when
time-sliced out, scheduler chooses process with the most credits</li>
</ul></li>
<li>Java JVM
<ul>
<li>Java uses preemptive, priority-based scheduling. Preemption happens
at the arrival of higher priority thread</li>
<li><code>yield()</code>: running thread yields to another thread of
equal priority</li>
<li>Thread priorities: <code>Thread.MIN_PRIORITY</code>,
<code>Thread.MAX_PRIORITY</code>, <code>Thread.NORM_PRIORITY</code></li>
</ul></li>
</ul></li>
<li>Synchronization overview
<ul>
<li>Idea: processes, threads use shared data access to communicate.
Shared data access can result in data inconsistencies and unpredictable
processes/threads behaviors. So synchronization is needed to prevent
that</li>
<li>Definition
<ul>
<li>Race condition: outcome of thread execution depends on timing of
threads</li>
<li>Synchronization: use of atomic operations to ensure correct
cooperation amongst threads</li>
<li>Mutual exclusion: ensure that only one thread does a particular
thing</li>
<li>Critical section: piece of code that only on thread can execute at
any time (if multiple threads access critical section, there will be
data inconsistencies)</li>
<li>Lock: construct that prevents someone from doing something</li>
<li>Starvation: occurs when one or more threads never gets access to
critical section</li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-12-20220304">Lecture 12 2022/03/04</h2>
<ol type="1">
<li>The milk problem: suppose there is not milk, and we want only one
bottle of milk
<ul>
<li>Solution 1 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Person A                    </span></span><br><span class="line"><span class="keyword">if</span> (noMilk) &#123;</span><br><span class="line">  <span class="keyword">if</span> (noNote) &#123;</span><br><span class="line">    leave note;</span><br><span class="line">    buy milk;</span><br><span class="line">    remove note;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Person B                    </span></span><br><span class="line"><span class="keyword">if</span> (noMilk) &#123;</span><br><span class="line">  <span class="keyword">if</span> (noNote) &#123;</span><br><span class="line">    leave note;</span><br><span class="line">    buy milk;</span><br><span class="line">    remove note;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>This one does not work, think of context switch after the second if,
both will leave a note and buy a bottle of milk, then there will be two
bottles of milk in the end</li>
</ul></li>
<li>Solution 2: <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Person A</span></span><br><span class="line">leave noteA;</span><br><span class="line"><span class="keyword">if</span> (noNoteB) &#123;</span><br><span class="line">  <span class="keyword">if</span> (noMilk) &#123;</span><br><span class="line">    buy milk;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">remove noteA;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Person B</span></span><br><span class="line">leave noteB;</span><br><span class="line"><span class="keyword">if</span> (noNoteA) &#123;</span><br><span class="line">  <span class="keyword">if</span> (noMilk) &#123;</span><br><span class="line">    buy milk;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">remove noteB;</span><br></pre></td></tr></table></figure>
<ul>
<li>This one does not work, think of the situation A starts first, there
is a context switch after the first if to B, and another switch back to
A after executing the first line of B, then neither of A or B will buy a
bottle of milk</li>
</ul></li>
<li>Solution 3: <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Person A</span></span><br><span class="line">leave noteA;</span><br><span class="line"><span class="comment">// Denote this line X</span></span><br><span class="line"><span class="keyword">while</span> (noteB) &#123;  </span><br><span class="line">  skip;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (noMilk) &#123;</span><br><span class="line">  buy milk;</span><br><span class="line">&#125;</span><br><span class="line">remove noteA;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Person B</span></span><br><span class="line">leave noteB;</span><br><span class="line"><span class="comment">// Denote this line Y</span></span><br><span class="line"><span class="keyword">if</span> (noNoteA) &#123;</span><br><span class="line">  <span class="keyword">if</span> (noMilk) &#123;</span><br><span class="line">    buy milk;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">remove noteB;</span><br></pre></td></tr></table></figure>
<ul>
<li>This one works
<ul>
<li>X and Y are lines that after which critical sections are executed,
so we think of situations here</li>
<li>At Y:
<ul>
<li>If noNoteA: A has not start, or A has already bought milk, B can buy
milk if no milk, or does not buy milk if there is milk</li>
<li>If noteA: A is waiting B to quit, or A is buying milk, so B does buy
and quit</li>
</ul></li>
<li>At X:
<ul>
<li>If noNoteB: B has not start, or B has ready bought milk, A can buy
milk if no milk, or does not buy milk if there is milk</li>
<li>If noteB: B is waiting A to quit, or B is buying milk, so A waits B
to remove note, then checks if there is milk to decide whether to buy or
not</li>
</ul></li>
</ul></li>
<li>Disadvantages:
<ul>
<li>This approach is complicated</li>
<li>This is not a symmetric solution, code needs to be different for
each process, can not be generalized to multiple processes situations
easily</li>
<li>A is busy waiting</li>
<li>A is favored: if A and B have different goals, A is more favored
than B</li>
</ul></li>
</ul></li>
<li>Better solutions
<ul>
<li>Have hardware provide better primitives than atomic LOAD, STORE</li>
<li>Build higher-level programming abstraction on above hardware
instructions, for example: lock with atomic <code>acquire()</code> and
<code>release()</code></li>
<li>Milk solution example <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Person A and Person B</span></span><br><span class="line">lock.acquire();</span><br><span class="line"><span class="keyword">if</span> (noMilk) &#123;</span><br><span class="line">  buy milk;</span><br><span class="line">&#125;</span><br><span class="line">lock.release();</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
<li>The critical section problem
<ul>
<li>Idea: only one thread is executing the critical section at a time to
avoid race condition</li>
<li>Examples
<ul>
<li>Milk synchronization: <code>if (noMilk) &#123;buy milk&#125;</code></li>
<li>Bank transfer synchronization:
<code>acc1 -= amount; acc2 += amount;</code></li>
<li>Producer consumer problem:
<ul>
<li>Problems in the producer's method <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insert</span><span class="params">(Object item)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (count == size) &#123;</span><br><span class="line">    skip;</span><br><span class="line">  &#125;</span><br><span class="line">  count ++;</span><br><span class="line">  <span class="comment">// If context switch happens here, then consumer thinks there is one </span></span><br><span class="line">  <span class="comment">// more items than there actually is</span></span><br><span class="line">  buffer[in] = item;</span><br><span class="line">  in = (in + <span class="number">1</span>) % size;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Additional problems: <code>count++</code>/ <code>count--</code>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; There can be context switches between these three instructions</span><br><span class="line">LOAD R1, count</span><br><span class="line">INC R1 &#x2F;&#x2F; DEC R1 for count--</span><br><span class="line">STORE R1, count</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
<li>For efficiency consideration, it's important to identify the exact
lines of code that is the critical section</li>
<li>General form of threads <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  entry code;</span><br><span class="line">  critical section;</span><br><span class="line">  exit code;</span><br><span class="line">  non-critical section;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>General solution to critical section problems
<ul>
<li>Clever algorithms</li>
<li>Hardware-based solutions</li>
<li>Hardware-based solutions + software abstractions</li>
</ul></li>
<li>Goals of solution
<ul>
<li>Mutual exclusion</li>
<li>Progress: no thread outside of its critical section should block
other threads</li>
<li>Bounded waiting: if a thread has already made a request to enter its
critical section, then the amount of times other threads should only
enter their critical sections for a bounded amount of times</li>
</ul></li>
</ul></li>
<li>Software solution to critical section problems
<ul>
<li>Strict alternation
<ul>
<li>Idea: each thread take turns to get into its critical section</li>
<li>Pseudocode <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">shared <span class="keyword">int</span> turn;</span><br><span class="line"><span class="comment">// Thread A</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  <span class="keyword">while</span> (turn != <span class="number">0</span>) &#123;</span><br><span class="line">    skip;</span><br><span class="line">  &#125;</span><br><span class="line">  critical section;</span><br><span class="line">  turn = <span class="number">1</span>;</span><br><span class="line">  non-critical section;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Thread B</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  <span class="keyword">while</span> (turn != <span class="number">1</span>) &#123;</span><br><span class="line">    skip;</span><br><span class="line">  &#125;</span><br><span class="line">  critical section;</span><br><span class="line">  turn = <span class="number">0</span>;</span><br><span class="line">  non-critical section;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Strict alternation satisfies both mutual exclusion, bounded waiting,
but does not satisfy progress</li>
<li>Disadvantage:
<ul>
<li>Does not satisfy progress</li>
<li>Use busy waiting, waste lots of resources</li>
<li>Faster threads can get blocked by slower ones</li>
</ul></li>
</ul></li>
<li>After you
<ul>
<li>Idea: thread specifies interest in entering critical section, and
can only enter if other thread is not interested</li>
<li>Pseudocode <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">shared <span class="keyword">boolean</span> flag0, flag1;</span><br><span class="line"><span class="comment">//Thread A</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  flag0 = <span class="keyword">true</span>;</span><br><span class="line">  <span class="keyword">while</span> (flag1) &#123;</span><br><span class="line">    skip;</span><br><span class="line">  &#125;</span><br><span class="line">  critical section;</span><br><span class="line">  flag0 = <span class="keyword">false</span>;</span><br><span class="line">  non-critical section;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//Thread B</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  flag1 = <span class="keyword">true</span>;</span><br><span class="line">  <span class="keyword">while</span> (flag0) &#123;</span><br><span class="line">    skip;</span><br><span class="line">  &#125;</span><br><span class="line">  critical section;</span><br><span class="line">  flag1 = <span class="keyword">false</span>;</span><br><span class="line">  non-critical section;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>After you satisfies mutual exclusion, but does not satisfy progress
and bounded waiting</li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-13-20220310">Lecture 13 2022/03/10</h2>
<ol type="1">
<li>Software solution to critical section problems
<ul>
<li>Peterson's algorithm
<ul>
<li>Idea: combination of strict alternation and after you, threads take
turns to get into critical section but a thread skips its turn if it's
not interested and another thread is</li>
<li>Pesudocode <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">shared <span class="keyword">int</span> turn;</span><br><span class="line">shared <span class="keyword">boolean</span> flag0, flag1;</span><br><span class="line"><span class="comment">// Thread A</span></span><br><span class="line">flag0 = <span class="keyword">true</span>;</span><br><span class="line">turn = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (turn == <span class="number">1</span> &amp;&amp; flag1) &#123;</span><br><span class="line">  skip;</span><br><span class="line">&#125;</span><br><span class="line">critical section;</span><br><span class="line">flag0 = <span class="keyword">false</span>;</span><br><span class="line">non-critical section;</span><br><span class="line"><span class="comment">// Thread B</span></span><br><span class="line">flag1 = <span class="keyword">true</span>;</span><br><span class="line">turn = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (turn == <span class="number">0</span> &amp;&amp; flag0) &#123;</span><br><span class="line">  skip;</span><br><span class="line">&#125;</span><br><span class="line">critical section;</span><br><span class="line">flag1 = <span class="keyword">false</span>;</span><br><span class="line">non-critical section;</span><br></pre></td></tr></table></figure></li>
<li>Peterson's algorithm satisfies mutual exclusion, progress and
bounded waiting</li>
</ul></li>
</ul></li>
<li>Hardware solutions to critical section problems
<ul>
<li>Disabling interrupts
<ul>
<li>Idea: disable interrupts before entering critical section and enable
interrupts after exiting critical section</li>
<li>Pseudocode <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// All threads</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  disable interrupts;</span><br><span class="line">  critical section;</span><br><span class="line">  enable interrupts;</span><br><span class="line">  non-critical section;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Comments
<ul>
<li>Advantages: this solution is easy to implement</li>
<li>Disadvantages:
<ul>
<li>Overkill: disables all interrupts(I/O), disallows concurrency with
threads in non-critical sections</li>
<li>Does not work for multi-core systems, because each cor has its own
interrupts, so threads on different cores can access their critical
sections at the same time</li>
</ul></li>
</ul></li>
</ul></li>
<li>Test and set
<ul>
<li>An assembly command called TS(test and set), it's an atomic
operation</li>
<li>What does <code>TS(i)</code> do
<ul>
<li>Save the value of <code>Mem[i]</code> (Mem[i] stores a boolean value
of a lock)</li>
<li>Set <code>Mem[i] = True</code></li>
<li>Return the original value of <code>Mem[i]</code></li>
</ul></li>
<li>If TS(i) returns True, then the lock is locked, so we cannot access
the critical section</li>
<li>Solve the critical section problem with TS <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">shared bool lock &#x3D; false;</span><br><span class="line">while (true) &#123;</span><br><span class="line">  while (ts(lock)) &#123;</span><br><span class="line">    skip;</span><br><span class="line">  &#125;</span><br><span class="line">  critical section;</span><br><span class="line">  lock &#x3D; false;</span><br><span class="line">  non-critical section;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Comments
<ul>
<li>Advantages: easy to use for critical sections, not easy to use for
other synchronization problems</li>
<li>Disadvantages: busy waiting is involved, can be expensive on multi
cores</li>
</ul></li>
</ul></li>
</ul></li>
<li>Semaphores: software abstraction of hardware
<ul>
<li>Idea: a semaphore is a data structure consisting of an integer lock
with a waiting queue, it provides thread safe operations
<ul>
<li><code>init(n)</code>: n threads are allowed access to resource at a
time</li>
<li><code>acquire()</code>: thread code calls to gain access</li>
<li><code>release()</code>: thread code calls to relinquish access</li>
</ul></li>
<li>Binary semaphore: n = 1, only one thread can hold lock at a
time</li>
<li>Counting semaphore: n threads can hold lock at a time</li>
<li>A semaphore can be either unlocked or locked by a thread with other
threads waiting in the queue</li>
<li>Critical section solved with binary semaphore <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">shared binary semaphore S = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  acquire(S);</span><br><span class="line">  critical section;</span><br><span class="line">  release(S);</span><br><span class="line">  non-critical section;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol>
<h2 id="lecture-14-20220315">Lecture 14 2022/03/15</h2>
<ol type="1">
<li>Bounded buffer problem
<ul>
<li>Shared buffer between producer thread and consumer thread</li>
<li>Terminology
<ul>
<li>buffer: circular queue, shared by all threads</li>
<li>in: a pointer that points to next spot to insert into buffer, shared
by all producer threads</li>
<li>out: a pointer that points to next spot to remove from buffer,
shared by all consumer threads</li>
<li>Full buffer: ensure producer does not try to insert into a full
buffer (wait until not full)</li>
<li>Empty buffer: ensure consumer does not try to remove from an empty
buffer (wait until not empty)</li>
</ul></li>
<li>Semaphores (buffer size N)
<ul>
<li><code>empty</code>: measure the amount of empty spots in the buffer,
initialized to be N, shared by all producers, locked after N producer
threads acquire the the semaphore (which means the buffer is full)</li>
<li><code>full</code>: measure the amount of full spots in the buffer,
initialized to be 0, shared by all consumers, unlock after 1 producer
thread release the semaphore (which means the buffer is not empty, so
the consumer can start consuming)</li>
<li><code>mutex</code>: binary semaphore, shared by all threads, used to
ensure atomic update of shared variables</li>
</ul></li>
<li>Implementation <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Semaphore mutex = <span class="keyword">new</span> Semaphore(<span class="number">1</span>);</span><br><span class="line">Semaphore empty = <span class="keyword">new</span> Semaphore(N);</span><br><span class="line">Semaphore full = <span class="keyword">new</span> Semaphore(<span class="number">0</span>);</span><br><span class="line"><span class="keyword">int</span> in = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> out = <span class="number">0</span>;</span><br><span class="line">T[] buffer = <span class="keyword">new</span> CircularQueue&lt;T&gt;(N);</span><br><span class="line"><span class="comment">//producer thread</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  empty.acquire();</span><br><span class="line">  <span class="comment">// This line should be after empty.acquire(), because if you acquire mutex first</span></span><br><span class="line">  <span class="comment">// and the buffer is full, then there is no way for consumers to acquire the mutex</span></span><br><span class="line">  <span class="comment">// and increase the empty semaphore</span></span><br><span class="line">  mutex.acquire();    </span><br><span class="line">  buffer[in] = <span class="keyword">new</span> T();</span><br><span class="line">  in = (in + <span class="number">1</span>) % N;</span><br><span class="line">  <span class="comment">// This line can be after the release of the full semaphore, but efficiency is </span></span><br><span class="line">  <span class="comment">// reduced then</span></span><br><span class="line">  mutex.release();</span><br><span class="line">  full.release();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// consumer thread</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  full.acquire();</span><br><span class="line">  mutex.acquire();</span><br><span class="line">  T item = buffer[out];</span><br><span class="line">  out = (out + <span class="number">1</span>) % N;</span><br><span class="line">  mutex.release();</span><br><span class="line">  empty.release();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Bounded buffer with only one binary semaphore and spin lock
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insert</span> <span class="params">(T item)</span> </span>&#123;</span><br><span class="line">  mutex.acquire();</span><br><span class="line">  <span class="keyword">while</span> (count === N) &#123;</span><br><span class="line">    mutex.release();</span><br><span class="line">    <span class="comment">// Context switch can happen here, allowing consumers to remove</span></span><br><span class="line">    mutex.acquire();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Insert goes here</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> T <span class="title">remove</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  T item;</span><br><span class="line">  mutex.acquire();</span><br><span class="line">  <span class="keyword">while</span> (count == <span class="number">0</span>) &#123;</span><br><span class="line">    mutex.release();</span><br><span class="line">    <span class="comment">// Context switch can happen here, allowing producers to insert</span></span><br><span class="line">    mutex.acquire();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Remove goes here</span></span><br><span class="line">  <span class="keyword">return</span> item;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>The single mutex and spin lock approach does eliminate
inconsistencies, but it involves busy waiting and waste of CPU
resources</li>
</ul></li>
</ol>
<h2 id="lecture-15-20220317">Lecture 15 2022/03/17</h2>
<ol type="1">
<li>Readers-writers problem
<ul>
<li>Idea:
<ul>
<li>A buffer of updatable data</li>
<li>Two type of threads: readers and writers</li>
<li>Synchronization rules: can allow access of either any number of
readers at a time, or exactly one writer</li>
</ul></li>
<li>Database example
<ul>
<li>Read is query: queries do not change contents of database, several
can execute concurrently</li>
<li>Write is update: if an update is taking place, it cannot allow
either reads or other writes</li>
</ul></li>
<li>Approach 1: a reader-favored solution
<ul>
<li>Idea
<ul>
<li><code>rcount</code>: the number of readers that is currently reading
the database</li>
<li><code>mutex</code>: a binary semaphore that is used to guard the
modification of rcount among readers</li>
<li><code>rwlock</code>: a binary semaphore shared among readers and
writers
<ul>
<li>It can be acquired by a writer to ensure that no other writer is
updating the databsae concurrently</li>
<li>It can be acquired by the first reader to ensure that no writer is
updating the database when there are readers reading the database</li>
<li>It should be released by the last reader to allow writers to update
the database</li>
</ul></li>
</ul></li>
<li>Implementation <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Shared variables initialization</span></span><br><span class="line"><span class="keyword">int</span> rcount = <span class="number">0</span>;</span><br><span class="line">Semaphore mutex = <span class="keyword">new</span> Semaphore(<span class="number">1</span>);</span><br><span class="line">Semaphore rwlock = <span class="keyword">new</span> Semaphore(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// Writer thread</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  <span class="comment">// If this acquire method does not stuck, it means no other writer is updating </span></span><br><span class="line">  <span class="comment">// the database and no other readers are reading the database</span></span><br><span class="line">  rwlock.acquire();</span><br><span class="line">  <span class="comment">// Update the database</span></span><br><span class="line">  rwlock.release();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reader thread</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  <span class="comment">// Use mutex to ensure exclusive access to rcount</span></span><br><span class="line">  mutex.acquire();</span><br><span class="line">  rcount ++;</span><br><span class="line">  <span class="comment">// If the reader is the first one, then it should not let writers update the</span></span><br><span class="line">  <span class="comment">// database, so it should acquire the rwlock</span></span><br><span class="line">  <span class="keyword">if</span> (rcount == <span class="number">1</span>) &#123;</span><br><span class="line">    rwlock.acquire();</span><br><span class="line">  &#125;</span><br><span class="line">  mutex.release();</span><br><span class="line">  <span class="comment">// Read database</span></span><br><span class="line">  <span class="comment">// Use mutex to ensure exclusive access to rcount</span></span><br><span class="line">  mutex.acquire();</span><br><span class="line">  rcount --;</span><br><span class="line">  <span class="comment">// If the reader is the last one, then it should release the rwlock to let </span></span><br><span class="line">  <span class="comment">// writers run</span></span><br><span class="line">  <span class="keyword">if</span> (rcount == <span class="number">0</span>) &#123;</span><br><span class="line">    rwlock.release();</span><br><span class="line">  &#125;</span><br><span class="line">  mutex.release();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Performance: Give high throughout (there can be many readers active
at a time), but bounded wait not satisfied (the writers may wait
indefinitely)</li>
<li>This is a readers-favored solution: as long as the writer starts
after the 1st reader, it needs to wait for all readers to finish to
run</li>
</ul></li>
<li>Approach 2: a writers-favored solution:
<ul>
<li>Idea
<ul>
<li><code>rcount</code>, <code>wcount</code>, <code>mutex1</code> for
rcount, <code>mutex2</code> for wcount, <code>rlock</code>,
<code>wlock</code></li>
<li>Implementation <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Shared variables initialization</span></span><br><span class="line"><span class="keyword">int</span> rcount = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> wcount = <span class="number">0</span>;</span><br><span class="line">Semaphore mutex1 = <span class="keyword">new</span> Semaphore(<span class="number">1</span>);</span><br><span class="line">Semaphore mutex2 = <span class="keyword">new</span> Semaphore(<span class="number">1</span>);</span><br><span class="line">Semaphore rlock = <span class="keyword">new</span> Semaphore(<span class="number">1</span>);</span><br><span class="line">Semaphore wlock = <span class="keyword">new</span> Semaphore(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Reader thread</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  <span class="comment">// Use wlock to wait writers to finish updating the database</span></span><br><span class="line">  wlock.acquire();</span><br><span class="line">  mutex1.acquire();</span><br><span class="line">  rcount ++;</span><br><span class="line">  <span class="keyword">if</span> (rcount == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="comment">// Lock the database so no update can happen during writing</span></span><br><span class="line">    rlock.acquire();</span><br><span class="line">  &#125;</span><br><span class="line">  mutex1.release();</span><br><span class="line">  wlock.release();</span><br><span class="line">  <span class="comment">//Read database</span></span><br><span class="line">  mutex1.acquire();</span><br><span class="line">  rcount --;</span><br><span class="line">  <span class="keyword">if</span> (rcount == <span class="number">0</span>) &#123;</span><br><span class="line">    rlock.release();</span><br><span class="line">  &#125;</span><br><span class="line">  mutex1.release();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Writer thread</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  mutex2.acquire();</span><br><span class="line">  wcount ++;</span><br><span class="line">  <span class="keyword">if</span> (wcount == <span class="number">1</span>) &#123;</span><br><span class="line">    wlock.acquire();</span><br><span class="line">  &#125;</span><br><span class="line">  mutex2.release();</span><br><span class="line">  <span class="comment">// If there are readers reading the database, then wait; if not, block all </span></span><br><span class="line">  <span class="comment">// subsequent readers</span></span><br><span class="line">  rlock.acquire();</span><br><span class="line">  <span class="comment">// Update database</span></span><br><span class="line">  rlock.release();</span><br><span class="line">  mutex2.acquire();</span><br><span class="line">  wcount --;</span><br><span class="line">  <span class="keyword">if</span> (wcount == <span class="number">0</span>) &#123;</span><br><span class="line">    wlock.release();</span><br><span class="line">  &#125;</span><br><span class="line">  mutex2.release();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Performance: the writers can run ASAP, but readers can wait
indefinitely</li>
</ul></li>
</ul></li>
<li>Dining Philosopher
<ul>
<li>Idea
<ul>
<li>N philosophers sit a table with N chopsticks, each philosopher
shares 2 chopsticks with her left and right neighbor</li>
<li>Each philosopher thinks for a while, gets hungry then tries to grab
both chopsticks next to him, and eats then releases chopsticks</li>
<li>The question is how to serve dishes for the philosophers</li>
</ul></li>
<li>Solution 1: ensure only one philosopher grabs a given chopstick
<ul>
<li>Implementation <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Shared variables initialization</span></span><br><span class="line"><span class="comment">// An array of N binary semaphores</span></span><br><span class="line">Semaphore[] chopsticks = <span class="keyword">new</span> Semaphore[N];</span><br><span class="line"><span class="comment">// For philosopher i thread</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  chopsticks[i].acquire();</span><br><span class="line">  chopsticks[(i + <span class="number">1</span>) % N].acquire();</span><br><span class="line">  <span class="comment">// Eat</span></span><br><span class="line">  chopsticks[i].release();</span><br><span class="line">  chopsticks[(i + <span class="number">1</span>) % N].release();</span><br><span class="line">  <span class="comment">// Think</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Performance: this may result in a deadlock</li>
</ul></li>
<li>Solution 2: ensure only one philosopher grabs both her chopsticks at
a time
<ul>
<li>Implementation <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Shared variables initialization</span></span><br><span class="line">Semaphore mutex = <span class="keyword">new</span> Semaphore(<span class="number">1</span>);</span><br><span class="line">Semaphore[] chopsticks = <span class="keyword">new</span> Semaphore[N];</span><br><span class="line"><span class="comment">// For philosopher i thread</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  mutex.acquire();</span><br><span class="line">  chopsticks[i].acquire();</span><br><span class="line">  chopsticks[(i + <span class="number">1</span>) % N].acquire();</span><br><span class="line">  mutex.release();</span><br><span class="line">  <span class="comment">// Eat</span></span><br><span class="line">  chopsticks[i].release();</span><br><span class="line">  chopsticks[(i + <span class="number">1</span>) % N].release();</span><br><span class="line">  <span class="comment">// Think</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Performance: this does not result in a deadlock, but philosophers
that are not eating (not in the critical section) can block following
philosophers (when she acquires the mutex, but is blocked by acquiring
chopsticks)</li>
</ul></li>
<li>Solution 3: synchronize philosophers states (hungry, eating,
thinking) rather than synchronizing chopsticks
<ul>
<li>Implementation <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Shared variables initialization</span></span><br><span class="line"><span class="comment">// states[i] can be hungry, eating and thinking</span></span><br><span class="line">State[] states = <span class="keyword">new</span> State[N];</span><br><span class="line"><span class="comment">// if phils[i] is locked, she cannot eat, if phils[i] is unlocked, she can eat</span></span><br><span class="line">Semaphore[] phils = <span class="keyword">new</span> Semaphore[N];</span><br><span class="line"><span class="comment">// Use mutex to avoid race condition in states</span></span><br><span class="line">Semaphore mutex = <span class="keyword">new</span> Semaphore(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">seekToEat</span><span class="params">(i)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (states[i] == hungry &amp;&amp; states[(i - <span class="number">1</span>) % N] != eating &amp;&amp; states[(i + <span class="number">1</span>) % N] != eating) &#123;</span><br><span class="line">    states[i] = eating;</span><br><span class="line">    phils[i].release();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// For philosopher i thread</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  mutex.acquire();</span><br><span class="line">  states[i] = hungry;</span><br><span class="line">  seekToEat(i);</span><br><span class="line">  mutex.release();</span><br><span class="line">  phils[i].acquire();</span><br><span class="line">  <span class="comment">// Eat</span></span><br><span class="line">  mutex.acquire();</span><br><span class="line">  states[i] = thinking;</span><br><span class="line">  seekToEat((i + <span class="number">1</span>) % N);</span><br><span class="line">  seekToEat((i - <span class="number">1</span>) % N);</span><br><span class="line">  mutext.release();</span><br><span class="line">  <span class="comment">// Think</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-16-20220322">Lecture 16 2022/03/22</h2>
<ol type="1">
<li>Alternatives to Semaphores: Monitors
<ul>
<li>Performance of semaphores
<ul>
<li>Semaphores accomplishes two tasks:
<ul>
<li>Mutual exclusion of shared memory (mutex)</li>
<li>Scheduling constraints: down and up(counting semaphore)</li>
</ul></li>
<li>If the down and up are scatter among several processes, problems may
occur
<ul>
<li>If the order of down is wrong, deadlocks may occur</li>
<li>If the order of up is wrong, performance may be poor</li>
</ul></li>
<li>We want something that is simple and easy</li>
</ul></li>
<li>Implementation of monitors
<ul>
<li>A package that consists of a collection of procedures, variables and
data structures</li>
<li>Compiler implements the mutual exclusion on monitor entries</li>
</ul></li>
</ul></li>
<li>Monitors
<ul>
<li>Definition: a monitor is an abstract data type that provides a
high-level form of process synchronization</li>
<li><code>Thread.yield()</code>: if a thread calls this method, it
voluntarily relinquishes the CPU
<ul>
<li>If there are threads with equal or higher priorities, then the
thread changes its state from running to ready and gives up the CPU</li>
<li>If there are no threads with equal or higher priorities, then the
thread continues to run</li>
</ul></li>
<li>Bounded buffer problem revisited
<ul>
<li>We can replace the mutex and spin lock solution with
<code>Thread.yield()</code></li>
<li>Since <code>Thread.yield()</code> does not ensure mutual exclusion,
we can change the insert and remove method to
<code>synchronized</code></li>
<li>Such synchronized insert and remove methods can result in
deadlocks</li>
</ul></li>
<li>Synchronized methods
<ul>
<li>All objects in Java have an associated lock</li>
<li>Calling synchronized method requires owning the lock, if the calling
thread does not own the lock, it is placed in the entry set for the
object's lock, the lock is released when the active thread exits the
synchronized method</li>
</ul></li>
<li>Condition variables
<ul>
<li><code>wait()</code> and <code>signal()</code></li>
</ul></li>
<li>Java thread methods
<ul>
<li><code>t.wait()</code>: t release the object's lock, change the state
of t to blocked, t is placed in the wait set</li>
<li><code>t.nofity()</code>: select a thread t' from the wait list of
the object's lock, move t' to the entry set, change the state of t' to
ready</li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-17-20220324">Lecture 17 2022/03/24</h2>
<ol type="1">
<li>Deadlock
<ul>
<li>Definition: deadlock occurs in a set of processes when every process
in the set is blocked and waiting for an even that can only be caused by
another process in the set</li>
<li>Four necessary but not sufficient conditions of deadlock
<ul>
<li>Mutual exclusion: only one process can use a resource at a time</li>
<li>Hold &amp; wait: process holding more than one resources is waiting
to acquire others hold by other processes</li>
<li>No preemption: resources only released by processes voluntarily when
they finish using the resources</li>
<li>Circular wait: there exists a set of waiting processes, <span class="math inline">\(\{P_0,..., P_n\}\)</span>, s.t. <span class="math inline">\(P_0\)</span> waits for <span class="math inline">\(P_1\)</span>, ..., <span class="math inline">\(P_n\)</span> waits for <span class="math inline">\(P_0\)</span></li>
</ul></li>
<li>An deadlock example <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Semaphore m1 = <span class="keyword">new</span> Semaphore(<span class="number">1</span>); <span class="comment">// binary semaphore m1</span></span><br><span class="line">Semaphore m2 = <span class="keyword">new</span> Semaphore(<span class="number">1</span>); <span class="comment">// binary semaphore m2</span></span><br><span class="line"><span class="comment">// Thread A</span></span><br><span class="line">m1.acquire();  <span class="comment">// Context switch after this line to Thread B</span></span><br><span class="line">m2.acquire();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Thread B</span></span><br><span class="line">m2.acquire();</span><br><span class="line">m1.acquire();</span><br></pre></td></tr></table></figure></li>
<li>Starvation is different from deadlock
<ul>
<li>For a deadlock: all threads are suspended</li>
<li>For a starvation: only part of threads are suspended (for example,
threads with lower priorities in the priority scheduling policy)</li>
</ul></li>
<li>Deadlock problem descriptions
<ul>
<li>Models
<ul>
<li>Resource types: <span class="math inline">\(R_1\)</span>, ..., <span class="math inline">\(R_m\)</span></li>
<li>Each resource type <span class="math inline">\(R_i\)</span> has
<span class="math inline">\(W_i\)</span> instances</li>
<li>Each process requests a resource (wait until the resource is
granted), use the resource, and release the resource</li>
<li>System table records if a resource is free or not, process it
allocated, and queue for processes waiting for each resource</li>
</ul></li>
</ul></li>
<li>Resource allocation graph
<ul>
<li>V: set of vertices
<ul>
<li>P = <span class="math inline">\(\{P_1, ..., P_n\}\)</span>:
processes in the system</li>
<li>R = <span class="math inline">\(\{R_1, ..., R_n\}\)</span>:
resources in the system</li>
</ul></li>
<li>E: set of edges
<ul>
<li>Request edge: <span class="math inline">\(P_i \rightarrow
R_j\)</span></li>
<li>Assignment edge: <span class="math inline">\(R_j \rightarrow
P_i\)</span></li>
</ul></li>
<li>Some interpretations
<ul>
<li>No cycle, then no deadlock</li>
<li>Cycle and n instances per resource type, then possible deadlock</li>
<li>Cycle and one instance per resource type, then deadlock for
sure</li>
<li>Processes that are not in a cycle can also be in deadlock</li>
</ul></li>
</ul></li>
</ul></li>
<li>Dealing with deadlock
<ul>
<li>Prevention: breaking one of the three conditions with
deadlock(mutual exclusion cannot be broken), so deadlocks will not occur
<ul>
<li>Break hold &amp; wait: before execution, processes try to acquire
all resources they need, if they cannot do so, they are allocated no
resources</li>
<li>Break preemption: if a process holds some resources and requires
more but cannot get them, then the process releases all resources it
currently holds</li>
<li>Break circular wait: processes must request resources in an
increasing order</li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-18-20220329">Lecture 18 2022/03/29</h2>
<ol type="1">
<li>Deadlock Avoidance
<ul>
<li>Does not break the necessary conditions for deadlock, but monitor
the system, and if a request will lead the system to unsafe state, such
request is rejected</li>
<li>System state
<ul>
<li>In a safe state, there is no possibility of deadlock, we can find a
safe sequence to grant the requests in a safe state</li>
<li>In an unsafe state, there is possibility of deadlock</li>
<li>Avoidance: never enter an unsafe state</li>
</ul></li>
<li>Banker's algorithm: deadlock avoidance
<ul>
<li>Terminology
<ul>
<li><code>Available</code>: array with length m, available resources for
each resource type</li>
<li><code>Max</code>: n * m matrix, process i request max[i][j]
instances of resource j</li>
<li><code>Allocation</code>: n * m matrix, process i is allocated
allocation[i][j] instances of resource j</li>
<li><code>Need</code>: n * m matrix, need[i][j] = max[i][j] -
allocation[i][j]</li>
</ul></li>
<li>Something to remember
<ul>
<li>Available can be calculated from total available resources and
allocated resources</li>
<li>If a request is granted, then the allocated resources should be
added to available resources</li>
<li>Check if a request should be granted: add the request to allocation,
if we can find a safe sequence, then the result is a safe state, so the
request should be granted, but if somewhere in the middle we cannot
grant any more process for execution, the the system enters an unsafe
state, so the request should not be granted</li>
</ul></li>
</ul></li>
</ul></li>
<li>Deadlock detection + recovery
<ul>
<li>Idea: allow system to enter deadlocked states, periodically run
deadlock detection algorithm. If deadlock is detected, then run recovery
scheme to break the deadlock</li>
<li>Detection situation 1: each resource type has one instance
<ul>
<li>Idea: we just need to check if there is a cycle in the resource
allocation graph</li>
<li>Algorithm: use DFS to check if a directed graph has a backward edge,
then the system is in a deadlocked state, the time complexity is O(V +
E)</li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-19-20220331">Lecture 19 2022/03/31</h2>
<ol type="1">
<li>Deadlock detection + recovery (ctd)
<ul>
<li>Detection situation 2: each resource types have multiple instances,
we use a detection algorithm that is similar to the banker's algorithm
<ul>
<li>If some requests can be granted, then they are not deadlocked.</li>
<li>The requests that cannot be granted are deadlocked</li>
<li>Time complexity of the deadlock detection algorithm: <span class="math inline">\(O(N^2 * M)\)</span>, where n is the number of
processes, and m is the number of resource types</li>
</ul></li>
<li>The deadlock detection algorithm is expensive, therefore we do not
run this deadlock detection algorithm after resource allocation. The
frequency of the detection algorithm depends on how often a deadlock is
likely to occur and how many processes will need to be rolled back</li>
<li>Deadlock recovery
<ul>
<li>Recovery approach 1: process termination
<ul>
<li>Kill all deadlocked processes</li>
<li>Kill one deadlocked process at a time until the deadlock cycle is
eliminated (select the process to be killed based on priority,
completion percentage, resources held, resources requested, etc.)</li>
</ul></li>
<li>Recovery approach 2: preempt resources from deadlocked processes
<ul>
<li>Preemption cost should be minimal</li>
<li>If a process is preempted, it should be rolled back to a previous
safe state and restart from that state</li>
<li>We need to ensure wo do not always preempt resources from the same
process, which can avoid starvation of that process</li>
</ul></li>
</ul></li>
</ul></li>
<li>The Ostrich method
<ul>
<li>As the deadlock can be very infrequently and the cost of handling
can be very high, the OS can just pretend there are no deadlocks</li>
<li>Most OSs use this method and let the programmers to handle
deadlocks</li>
</ul></li>
<li>Memory Manager
<ul>
<li>Memory manager is another resource manager component of the OS</li>
<li>Responsibilities
<ul>
<li>Memory allocation: responsible of allocating memory to
processes</li>
<li>Address mapping
<ul>
<li>Address mapping allows to run the same code in different physical
memory in different processes, and to support process address space
abstraction</li>
<li>Memory management unit (MMU) is responsible for mapping logical
addresses generated by CPU instructions to physical addresses seen by
memory controller</li>
</ul></li>
</ul></li>
<li>MMU is usually on the same chip as the CPU</li>
</ul></li>
<li>Address mapping
<ul>
<li>Logical addresses
<ul>
<li>Generated by CPU instructions, is an abstraction of memory</li>
<li>Logical addresses start at 0, and are contiguous</li>
</ul></li>
<li>Generating logical addresses
<ul>
<li>At compile time</li>
<li>At link/load time</li>
<li>At execution time</li>
</ul></li>
<li>Logical address space is defined by base and limit registers. If the
address is not in the range, there are traps to the OS</li>
</ul></li>
<li>Memory allocation
<ul>
<li>Memory allocation goals
<ul>
<li>High memory utilization: ensure as much memory is used as
possible</li>
<li>High concurrency: support as may processes as possible</li>
<li>Memory allocation should be fast</li>
</ul></li>
<li>Memory allocation strategies
<ul>
<li>Contiguous memory allocation
<ul>
<li>Fixed partition allocation
<ul>
<li>Idea: divide the physical address space into fixed partitions and
allocate processes into the partitions. One partition can hold at most
one process</li>
<li>Implementation: requires the support of relocation (base) and limit
register</li>
<li>Issues:
<ul>
<li>Internal fragmentation: when a process is allocated to a partition
and parts of the partition is not used, such part is called the internal
fragmentation</li>
<li>We need to know in advance the memory needs of processes</li>
<li>The memory needs of processes should not change during their
lifetime</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-20-20220405">Lecture 20 2022/04/05</h2>
<ol type="1">
<li>Memory allocation (ctd)
<ul>
<li>Memory allocation strategies (ctd)
<ul>
<li>Contiguous memory allocation (ctd)
<ul>
<li>Variable partition allocation
<ul>
<li>Idea: physical memory is not divide into fixed partitions, but has a
set of holes from which memory can be assigned</li>
<li>Allocation policies
<ul>
<li>Best fit: choose the smallest feasible hole</li>
<li>Worst fit: choose the largest feasible hole</li>
<li>First fit: choose the first feasible hole (do not need to search the
list)</li>
<li>Next fit: choose the next feasible hole (do not need to search the
list, do not need to start from the beginning)</li>
</ul></li>
<li>Issues:
<ul>
<li>External fragmentation: the size of a hole may be larger than the
needs of a process, the difference is called the external
fragmentation</li>
<li>External fragmentation can be periodically eliminated via
compaction</li>
</ul></li>
</ul></li>
<li>Comments
<ul>
<li>Contiguous memory allocation is easy to implement and
conceptualize</li>
<li>Contiguous memory allocations is faster than non-contiguous memory
allocation</li>
<li>The memory utilization is poor</li>
<li>Used by some early batch systems</li>
</ul></li>
</ul></li>
<li>Non-contiguous memory allocation
<ul>
<li>Idea: logical address space is partitioned, each partition is mapped
to a contiguous chunk of physical memory</li>
<li>Approach 1: paging, all chunks are the same size, chunks are
physically determined</li>
<li>Approach 2: segmentation, chunks are variably sized, chunks are
logically determined</li>
<li>Approach 3: paged segmentation, combination of paging and
segmentation</li>
<li>Comments:
<ul>
<li>Better memory utilization: by dividing address space into smaller
chunks, more memory fragments can be used</li>
<li>More complex implementation</li>
<li>The standard memory allocation system used by all OS today</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>Paging
<ul>
<li>Basic idea
<ul>
<li>Divide physical memory into fixed-size blocks called frames</li>
<li>Divide logical memory into same-sized blocks called pages</li>
<li>Within a page, memory is contiguous, but pages need not to be
contiguous or in order</li>
</ul></li>
<li>Address translation
<ul>
<li>Logical address = (p, d), physical address = (f, d)
<ul>
<li>p: page number (index into page table which contains base address of
corresponding frame f)</li>
<li>d: page offset: added to base address to find location within
page/frame</li>
</ul></li>
<li>Logical and physical addresses are binary, so the translation is
actually in binary form</li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-21-20220407">Lecture 21 2022/04/07</h2>
<ol type="1">
<li>Paging (ctd)
<ul>
<li>Page table implementation
<ul>
<li>Page table is kept in the main memory</li>
<li>Each process use process special status registers:
<ul>
<li>Page table base register (PTBR): location of page table for
process</li>
<li>Page table limit register(PTLR): size of process page table</li>
</ul></li>
</ul></li>
<li>Issues with paging
<ul>
<li>Two memory access problem: use TLB</li>
<li>Page table is too large:
<ul>
<li>Use two level page table</li>
<li>Use inverted page table</li>
<li>Use hashed page table</li>
</ul></li>
</ul></li>
<li>Improve memory access with TLB
<ul>
<li>The implementation of page table requires 2 memory accesses for each
operation
<ul>
<li>Look up the frame location for this process in page table</li>
<li>Loop up the offset in the frame to find the location of the
data</li>
</ul></li>
<li>Use translation loop-aside buffer to solve the two memory access
problem
<ul>
<li>TLB is a fast lookup hardware cache containing some page table
entries</li>
<li>Why TLB is fast
<ul>
<li>TLB is cache, and cache is faster than memory</li>
<li>Associative memory permits parallel search</li>
</ul></li>
<li>When doing to page table lookup, first look in the cache TLB, if no
in cache (called TLB miss), look in memory page table</li>
</ul></li>
<li>Effective access time (EAT) with TLB
<ul>
<li>If TLB misses, then the performance is even worse than the page
table lookup with two memory accesses (additional access in TLB)</li>
<li>Assumption
<ul>
<li>Each memory access takes time of 1</li>
<li>Each TLB access takes <span class="math inline">\(\epsilon\)</span>
time</li>
<li>TLB hit ratio <span class="math inline">\(\alpha\)</span> = TBL hits
/ (TLB hits + TLB misses)</li>
</ul></li>
<li>EAT = <span class="math inline">\((1 + 1 + \epsilon) * \alpha + (1 +
\epsilon) * (1 - \alpha) = 2 + \epsilon - \alpha\)</span></li>
<li>If the hit ratio <span class="math inline">\(\alpha\)</span> is
high, then the EAT is better</li>
</ul></li>
<li>If there is a TLB miss, then we need to update the TLB entries</li>
</ul></li>
<li>Two level page table
<ul>
<li>Page table is stored contiguously in the physical memory</li>
<li>In 32-bit virtual memory, 12 bits are used for offset, 20 bits are
used for page table, page table entry size is 4 bytes, then page table
size is 4 * 2 ^ (32 - 12) = 4MB</li>
<li>In 32-bit virtual memory, 12 bits are used for offset, 10 bits are
used for outer (primary) page table, 10 bits are used for inner
(secondary) page table, page table entry size is 4 bytes, then outer
page table size is 4 * 2 ^ 10 = 4KB, inner page table size is 4 * 2 ^ 10
= 4KB, there is one outer page table and 2 ^ 10 inner page tables</li>
<li>In the above two level page table, one virtual address translation
involves the outer page table and one inner page tables</li>
</ul></li>
<li>Inverted page table
<ul>
<li>For ordinary page table, each process has a page table</li>
<li>For inverted page table, there is only one page table for all
processes</li>
<li>Comments
<ul>
<li>Reduced memory space</li>
<li>Longer lookup time</li>
<li>Difficult shared memory implementation</li>
</ul></li>
</ul></li>
<li>Hashed page table
<ul>
<li>Page number is hashed to hashed page table</li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-22-20220414">Lecture 22 2022/04/14</h2>
<ol type="1">
<li>Segmentation
<ul>
<li>Idea: similar to paging, but partitions of logical address
space/physical memory are variable, not fixed. Partition the logical
address into text segment, stack segment, etc. Each segment itself is
continuous, but the page table of a process as a whole is not.</li>
<li>Architecture
<ul>
<li>Logical addresses = (s, d)</li>
<li>One process has one segment table</li>
<li>Each process PCB holds segment table base register (STBR) and
segment table length register(STLR). If a logical address is invalid,
trap to the OS</li>
</ul></li>
<li>Issues with segmentation
<ul>
<li>Two memory accesses problem: use TLB</li>
<li>Segment table is too large: use paged segmentation: segmentation
based allocation, but each segment is paged</li>
</ul></li>
<li>Comments on segmentation
<ul>
<li>Easier to implement protection: text segment should be read-only,
other segments may be non-executable. Page in the paging scheme can
contain both code and data</li>
<li>Easier to implement sharing: same text segment can be used by
multiple processes concurrently</li>
</ul></li>
<li>Summary <img src="/images/Courses/COSI131A/memory_management.png"></li>
</ul></li>
<li>Virtual memory
<ul>
<li>Motivation: increase the degree of multiprogramming in a running
system</li>
<li>Means
<ul>
<li>High memory utilization, flexible memory allocation: allow allocated
memory to be non-contiguous</li>
<li>Swapping and virtual memory: allow some process to have address
space dynamically mapped to physical memory</li>
</ul></li>
<li>Swapping
<ul>
<li>Idea: ready or blocked processes have no physical memory allocation,
instead, their address spaces are mapped to disk</li>
<li>Processes can be swapped out to disk and swapped into memory
dynamically</li>
<li>Comments:
<ul>
<li>Multiprogramming is limited not to memory but disk</li>
<li>Easy to implement</li>
<li>Expensive: involves a lot of disk I/O</li>
<li>Time variable: depends on process size</li>
</ul></li>
</ul></li>
<li>Virtual memory
<ul>
<li>Idea: ready, blocked, or running process have some (but not all) of
their allocation of physical memory</li>
<li>Implementation: demand paging/demand segmentation</li>
<li>Demand paging
<ul>
<li>Idea: bring a page into memory only when it's referenced (read or
written)</li>
<li>Implementation
<ul>
<li>Add valid/invalid bit to each page table entry: 1 is valid, which
means the page is in memory, 0 is invalid, which means the page is not
in memory</li>
<li>Page mapping algorithm
<ul>
<li>Logical address = (p, d)</li>
<li>Search p in page table, if valid, convert p to f and get physical
address = (f, d). If invalid, trap to page fault routine in OS to bring
p into a free frame and update page table, then restart translation
(during page fault, control is passed to OS, and when the handling
process is done, control is returned to the process, the process does
not know where to start from during execution, and it cannot start in
the middle of an instruction, so it needs to restart)</li>
<li>Submit memory request with (f, d)</li>
</ul></li>
</ul></li>
</ul></li>
<li>Pre-paging: load working set pages into memory before the process
starts</li>
</ul></li>
</ul></li>
</ol>
<h2 id="lecture-23-20220426">Lecture 23 2022/04/26</h2>
<ol type="1">
<li>Virtual memory
<ul>
<li>Page replacement
<ul>
<li>If there is no free frame, then we need to replace a existing page
in the memory using replacement algorithm</li>
<li>Page replacement algorithms
<ul>
<li>Goal: minimize number of page faults</li>
<li>If page fault rate is <span class="math inline">\(\rho\)</span>,
then EAT = (1 - <span class="math inline">\(\rho\)</span>) * memory
access time + <span class="math inline">\(\rho\)</span> * (page fault
overhead + [swap out time] + swap in time + restart time)</li>
<li>Algorithms
<ul>
<li>FIFO:
<ul>
<li>Replace pages that has the oldest time when it was brought into the
memory is replaced</li>
<li>Belady' anomaly: when the number of allocated frames increases, page
fault rate increases</li>
</ul></li>
<li>Optimal: replace the page that will not be used for the longest
period time</li>
<li>LRU: replace the page that hasn't been used for the longest period
time</li>
<li>NRU: not recently used</li>
<li>LFU: replace page referenced fewest times</li>
</ul></li>
</ul></li>
</ul></li>
<li>Frame allocation
<ul>
<li>Idea: we have one set of frames but multiple processes, the problem
is how to allocate the processes</li>
<li>Goal:
<ul>
<li>Minimize page fault rates</li>
<li>Avoid any process thrashing</li>
</ul></li>
<li>Allocation policy
<ul>
<li>Equitable: each process get even share of frames</li>
<li>Proportional allocation: each process gets proportional share of
frames</li>
<li>Priority-based allocation: process with higher priority gets more
frames</li>
</ul></li>
<li>Local allocation
<ul>
<li>Idea: assign a process some number of frames when it is created,
when page fault requires page replacement, replace a frame the process
itself owns</li>
<li>Issue: thrashing</li>
<li>Comment:
<ul>
<li>Does not use local allocation</li>
<li>We need to know the set of current processes</li>
<li>We need to assume the process' needs do not change over its
lifetime</li>
<li>We need to assume frame availability does not change over a process'
lifetime</li>
</ul></li>
</ul></li>
<li>Global allocation
<ul>
<li>Idea: all processes have access to all frames, when replacement is
needed, a process can replace a page owned by anyone</li>
<li>Issue: thrashing is still possible</li>
<li>Working set: a set of pages that a process is currently using
<ul>
<li>If entire working set is in memory, there are no page faults at
all</li>
<li>If there is insufficient space for working set, thrashing may
occur</li>
</ul></li>
<li>Use working set algorithm and adaptive algorithm (page fault
frequency scheme) to deal with thrashing
<ul>
<li>Locality model: in order to avoid thrashing, we need to provide the
pages that a process needs. We can say the pages that a process
currently uses is the pages it needs, the view is called the locality
model</li>
<li>Working set model
<ul>
<li>Use parameter <span class="math inline">\(\Delta\)</span> to
determine the working set window size at time <span class="math inline">\(T\)</span>.</li>
<li>Working set of a process at time <span class="math inline">\(T\)</span>: the set of last pages in the most
recent <span class="math inline">\(\Delta\)</span> page references</li>
<li>Allocation algorithm
<ul>
<li>Find the working set for each page</li>
<li>If there are more pages in the working sets of all processes than
the available frames, some processes must be suspended</li>
<li>Otherwise, allocate processes frames according to their working
sets</li>
<li>If there are available frames after allocation, we can increase the
number of multiprogramming and allow more processes to run</li>
</ul></li>
<li>Issues:
<ul>
<li>Working set size <span class="math inline">\(\Delta\)</span> is
important: if size is too small, do not include enough locality, if size
is too large, include too many localities</li>
<li>Maintain the working set of each process is too expensive</li>
</ul></li>
</ul></li>
<li>Page fault frequency (PFF) scheme
<ul>
<li>We can avoid thrashing by controlling the frequency of page
faults</li>
<li>If the page fault rate is high, we give the process more frames</li>
<li>If the page fault rate is low, we know the process has too many
frames and can remove some frames</li>
<li>If the page fault rate is high, but there are no available frames,
we need to swap out some processes</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ol>
<h2 id="book-contents">Book contents</h2>
<ol type="1">
<li>Introduction
<ul>
<li>What does os do Ch 1.1
<ul>
<li>Computer system: hardware, operating system, application program,
user</li>
<li>Programs
<ul>
<li>OS: one program running at all time on the computer, it's the
kernel</li>
<li>System program: associated with the os, but not necessarily part of
the kernel</li>
<li>Application programs: all programs not associated with the os</li>
</ul></li>
</ul></li>
<li>Computer system organization Ch 1.2
<ul>
<li>A modern general-purpose computer system consists of one or more
CPUs and a number of device controllers connected through a common bus
that provides access between components and shared memory</li>
<li>OS has a device driver (software) for each device controller
(hardware)</li>
<li>CPU and device controller can execute in parallel, competing for
memory, a memory controller synchronizes access to the memory</li>
<li>Interrupt
<ul>
<li>Interrupts are used throughout modern operating system to handle
asynchronous events</li>
<li>I/O operation read:
<ol type="1">
<li>Device driver loads appropriate registers into the device
controller</li>
<li>Device controller examines the registers and determines what action
should take</li>
<li>The controller transfers data from device to the a buffer in
controller</li>
<li>When data transfer is done, device controller informs device driver
via interrupt</li>
<li>The device driver then gives control to other parts of os <img src="/images/Courses/COSI131A/interrupt_driven_io_cycle.png"></li>
</ol></li>
<li>Hardware may trigger an interrupt at any time by sending a signal to
the cpu, usually via system bus</li>
<li>When interrupted, the cpu immediately switches to the interrupting
program, when that program is done, the cpu switches back to the
interrupted program</li>
<li>Interrupt mechanism are different in different computers, but
interrupt must transfer control to the appropriate interrupt service
routine
<ul>
<li>One way is to use a generic routine to examine the interrupt
information, this routine calls the interrupt-specific handler, but this
approach does not satisfy the speed requirement</li>
<li>Another way is to use a table of pointers (called interrupt vector)
stored in low memory to hold the addresses of the interrupt service
routines for various devices</li>
</ul></li>
<li>Implementation via interrupt vector
<ul>
<li>CPU hardware has a wire called the interrupt-request line that the
cpu senses after executing every instruction</li>
<li>When cpu detects a controller has asserted a signal on the
interrupt-request line, it jumps to the interrupt-handler routine by
using the interrupt number as an index into the interrupt vector (device
controller/hardware faults raises an interrupt, the cpu catches the
interrupt and dispatches it to the interrupt handler)</li>
<li>The interrupt handler performs all related work and return the cpu
to the execution state prior to the interrupt (interrupt handler clears
the interrupt)</li>
</ul></li>
<li>The interrupt mechanism also implements a system of interrupt
priority level</li>
</ul></li>
<li>Storage structure
<ul>
<li>Memory
<ul>
<li>Main memory (Random access memory, RAM), commonly ram is implemented
using a semiconductor technology called dynamic random access memory
(DRAM). RAM is volatile, it loses all its content when power is turned
off</li>
<li>Other types of memory: EEPROM (electrically erasable programmable
read-only memory), ROM. Bootstrap is resides in EEPROM, because EEPROM
is non-volatile</li>
<li>All memory provides an array of bytes, each byte has its own
address, cpu uses <code>laod</code>/<code>store</code> to interact with
memory</li>
</ul></li>
<li>Secondary storage
<ul>
<li>Be able to hold large quantities of data permanently</li>
<li>Common secondary storage devices: hard-disk drives(HDDs),
nonvolatile memory (NVM) devices</li>
</ul></li>
<li>Tertiary storage: magnetic tape, optical disk, blue-ray</li>
<li>Nonvolatile storage: mechanical or electrical storage <img src="/images/Courses/COSI131A/storage_hierarchy.png"></li>
</ul></li>
<li>I/O structure
<ul>
<li>Interrupt-driven I/O is fine for moving small amounts of data, but
can produce high overhead when used for bulk data movement</li>
<li>Direct memory access(DMA) is used to transfer entire block of data
between device and main memory</li>
</ul></li>
</ul></li>
<li>Computer system architecture Ch 1.3
<ul>
<li>Single processor (a processor is a physical chip that contains one
ore more cpu) system: one CPU with a single processing core. Core is the
component that executes instructions and registers for storing data
locally</li>
<li>There are also special-purpose processors: device-specific
processors, run a limited instruction set, do not run processes</li>
<li>Multiprocessor system
<ul>
<li>Advantages
<ul>
<li>Increased throughput: if N processors, the speed-up ration is less
than N</li>
<li>Economy of scale: average cost is reduced, multiprocessor system can
share power supplies, peripherals, mass storage</li>
<li>Increased reliability: if properly distributed, then failure of one
processor will not halt the system</li>
</ul></li>
<li>The most common multiprocessor systems use symmetric multiprocessing
(SMP), each peer cpu processor performs all tasks. Each processor has
its own cpu, set of registers and local cache, all processors share
physical memory over the system bus</li>
<li>Multicore system: multiple computing cores reside on a single chip,
each core has its own cpu and L1 cache, but L2 cache is shared among
cores. This is faster than traditional multiprocessor system, because
on-chip communication is faster than between-chip communication</li>
</ul></li>
<li>Clustered system
<ul>
<li>A clustered system is composed of two or more individual
systems</li>
<li>Clustering is usually used to provide high-availability service,
high availability provides increased reliability (graceful degradation/
fault tolerant)</li>
<li>Asymmetric clustering: one machine is in hot-standby mode, the other
is running the applications</li>
<li>Symmetric clustering: two or more hosts are running applications and
are monitoring each other</li>
</ul></li>
</ul></li>
<li>Operating system operations Ch 1.4
<ul>
<li>Bootstrap program is in the computer firmware, it loads the os into
memory</li>
<li>Some os services are provided outside the kernel, they are provided
by system programs that are loaded into the memory at boot time, thee
system programs become system daemons</li>
<li>Interrupts signal events
<ul>
<li>Introduction
<ul>
<li>Hardware device controller can raises interrupts</li>
<li>Trap(exception) is a software-generated interrupt, trap is either
caused by an error or by a specific request from a user program that an
os service be performed by executing a special operation called a system
call</li>
</ul></li>
<li>Multiprogramming and multitasking
<ul>
<li>A program in execution is called a process. In multiprogramming,
there are multiple processes loaded into the memory</li>
<li>Multitasking is a logical extension of multiprogramming</li>
<li>CPU scheduling: determines which process is executed next</li>
<li>Virtual allows the logical memory be separated from the physical
memory</li>
</ul></li>
<li>Dual-mode and multi-mode operation
<ul>
<li>We must be able to distinguish between the execution of
operating-system code and user-defined code</li>
<li>User mode, kernel mode (supervisor mode, system mode, privileged
mode). Transition between the two modes is made by system call</li>
<li>A mode bit is added to the hardware of the computer to indicate the
current mode: kernel(0) or user(1)</li>
<li>The concept of modes can be extended beyond two modes</li>
</ul></li>
<li>Timer
<ul>
<li>Timer is used to ensure that the os maintains the control over the
cpu, and a user program always returns control to the os (not stuck in
an infinite loop, or fail to invoke system call)</li>
<li>A timer can be set to interrupt the computer after a specified
period, the period can be fixed, or variable (achieved by a fixed-rate
clock and a counter, when counter decreases to 0, an interrupt
occurs)</li>
</ul></li>
</ul></li>
</ul></li>
<li>Resource management Ch 1.5
<ul>
<li>Process management
<ul>
<li>Creating and deleting both user and system processes</li>
<li>Scheduling processes and threads on the cpus</li>
<li>Suspending and resuming processes</li>
<li>Providing mechanisms for process synchronization</li>
<li>Providing mechanisms for process communication</li>
</ul></li>
<li>Memory management
<ul>
<li>Keeping track of which parts of memory are being used and which
process is using them</li>
<li>Allocating and deallocating memory space as needed</li>
<li>Deciding which processes and data to move into and out of
memory</li>
</ul></li>
<li>File system management
<ul>
<li>Creating and deleting files</li>
<li>Creating and deleting directories to organize files</li>
<li>Supporting primitives for manipulating files and directories</li>
<li>Mapping files onto mass storage</li>
<li>Backing up files on stable storage media</li>
</ul></li>
<li>Mass storage management
<ul>
<li>Mounting and unmounting</li>
<li>Free space management</li>
<li>Storage allocation</li>
<li>Disk scheduling</li>
<li>Partitioning</li>
<li>Protection</li>
</ul></li>
</ul></li>
</ul></li>
<li>Operating system structures
<ul>
<li>Operating system services Ch 2.1 <img src="/images/Courses/COSI131A/os_services.png">
<ul>
<li>User interface (UI): graphical user interface(GUI), touch-screen
interface, command-line interface(CLI)</li>
<li>I/O operations: users usually cannot control I/O devices
directly</li>
<li>File-system manipulation</li>
<li>Communication: inter-process communication implemented via shared
memory or message passing</li>
<li>Error detection</li>
<li>Resource allocation: allocate resources among different
processes</li>
<li>Logging: record events</li>
<li>Protection and security</li>
</ul></li>
<li>User and os interface Ch 2.2
<ul>
<li>Command-line interface/ command interpreter (shell)
<ul>
<li>Main functionality: get and execute the next user-specified
command</li>
<li>Two ways to interpret the commands:
<ul>
<li>The command interpreter contains the code to execute the
command</li>
<li>The commands are implemented through system programs, the command
interpreter loads commands into memory and execute them</li>
</ul></li>
</ul></li>
</ul></li>
<li>System calls Ch 2.3
<ul>
<li>System calls provide an interface to the services made available by
an operating system</li>
<li>Application programming interface (API)
<ul>
<li>API specifies a set of functions that are available to an
application programmer, including the parameters that are passed to each
function and the return values the programmer can expect</li>
<li>Three most common APIs: Windows API for Windows systems, the POSIX
API for POSIX-based systems (virtually all versions of UNIX, Linux, and
macOS), Java API for Java virtual machines</li>
<li>The function that makes up an API typically invoke the actual system
calls on behalf of the application programmer</li>
<li>Why use API instead of system calls directly:
<ul>
<li>Portability: program can run on any machine that supports the same
set of API</li>
<li>Easy to use: APIs are much simpler and easier to use than system
calls</li>
</ul></li>
</ul></li>
<li>Types of system calls
<ul>
<li>Process control
<ul>
<li>Operations: create/terminate process, load/execute program, get/set
process attribute, wait/signal event, allocate/free memory</li>
<li>Windows/Unix system calls:
<code>CreateProcess()</code>/<code>fork()</code>,
<code>ExitProcess()</code>/<code>exit()</code>,
<code>WaitForSingleObject()</code>/<code>wait()</code></li>
</ul></li>
<li>File management
<ul>
<li>Operations: create/delete file, open/close file,
read/write/reposition file, get/set file attribute</li>
<li>Windows/Unix system calls:
<code>CreateFile()</code>/<code>open()</code>,
<code>ReadFile()</code>/<code>read()</code>,
<code>WriteFile()</code>/<code>write()</code>,
<code>CloseHandle()</code>/<code>close()</code></li>
</ul></li>
<li>Device management
<ul>
<li>Operations: request/release device, read/write/reposition device,
get/set device attribute, logically attach/detach device</li>
<li>Windows/Unix system calls:
<code>SetConsoleMode()</code>/<code>ioctl()</code>,
<code>ReadConsole()</code>/<code>read()</code>,
<code>WriteConsole()</code>/<code>write()</code></li>
</ul></li>
<li>Information maintenance
<ul>
<li>Operations: get/set time or date, get/set system data, get/set
process file, set/set process, file or device attributes</li>
<li>Windows/Unix system calls:
<code>GetCurrentProcessID()</code>/<code>getpid()</code>,
<code>SetTimer()</code>/<code>alarm()</code>,
<code>Sleep()</code>/<code>sleep()</code></li>
</ul></li>
<li>Communications
<ul>
<li>Message-passing model: server and client communicates through
established connection(use hostname and process name to establish
connection)</li>
<li>Shared-memory model: processes use shared memory (which is not
allowed by os in common situations) to exchange data, the type and
format of data is determined by processes themselves, not by the os</li>
<li>Operations: create/delete communication connection, send/receive
messages, transfer status information, attach/detach remove devices</li>
<li>Windows/Unix system calls:
<code>CreatePipe()</code>/<code>pipe()</code>,
<code>CreateFileMapping()</code>/<code>shm_open()</code>,
<code>MapViewOfFile()</code>/<code>mmap()</code></li>
</ul></li>
<li>Protection
<ul>
<li>Operations: get/set file permissions</li>
<li>Windows/Unix system calls:
<code>SetFileSecurity()</code>/<code>chmod()</code>,
<code>InitializeSecurityDescriptor()</code>/<code>umask()</code>,
<code>SetSecurityDescriptorGroup()</code>/<code>shown()</code></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>Processes
<ul>
<li>Process concept Ch 3.1
<ul>
<li>Contemporary computer systems allow multiple programs to be loaded
into memory and executed concurrently, this requires firm control and
compartmentalization of the various programs, these needs results the
notation of a process, which is a program in execution.</li>
<li>A process is the unit of work in modern computer systems</li>
<li>The status of a process is represented by the value of the program
counter and the contents of the processor's registers</li>
<li>The memory layout of a process is divided into:
<ul>
<li>Text section: the executable code, fixed size</li>
<li>Data section: global variables, fixed size</li>
<li>Heap section: memory that is dynamically allocated during program
run time, variable size</li>
<li>Stack section: temporary data storage when invoking functions,
variable size, contains activation records (when a function is called,
an activation record containing function parameters, local variables and
the return address is pushed onto the stack)</li>
</ul></li>
<li>Process state
<ul>
<li>New: the process is being created</li>
<li>Running: instructions are being executed</li>
<li>Waiting: the process is waiting for some event to occur</li>
<li>Ready: the process is waiting to be assigned to a processor</li>
<li>Terminated: the process has finished execution <img src="/images/Courses/COSI131A/process_state_book.png"></li>
</ul></li>
<li>Process control block
<ul>
<li>Each process is represented in the os by a process control block
(PCB, also called task control block)</li>
<li>Contained information
<ul>
<li>Process state</li>
<li>Process id</li>
<li>Program counter</li>
<li>CPU registers</li>
<li>CPU scheduling information</li>
<li>Memory management information</li>
<li>Accounting information</li>
<li>I/O status information</li>
</ul></li>
</ul></li>
<li>Threads: PCB can be extended to be compatible with multiple
threads</li>
</ul></li>
<li>Process scheduling Ch 3.2
<ul>
<li>Goals
<ul>
<li>Goal of multiprogramming: maximize cpu utilization</li>
<li>Goal of time sharing: switch cpu among processes fo that users can
interact with each program while it is running</li>
</ul></li>
<li>Each cpu core can run one process at a time, the process scheduler
selects an available process for program execution on a core</li>
<li>Degree of multiprogramming: the number of processes currently in
memory</li>
<li>Bounds
<ul>
<li>CPU-bound process: a process that spends more of its time doing
computations than doing I/O</li>
<li>I/O-bound process: a process that spends more of its time doing I/O
than doing computations</li>
</ul></li>
<li>Scheduling queues
<ul>
<li>Ready queue: processes that are ready and waiting to execute on a
core</li>
<li>Wait queue: process that are waiting for a certain event to occur,
there are different kinds of wait queues <img src="/images/Courses/COSI131A/process_scheduling_queues.png"></li>
</ul></li>
<li>Context switch:
<ul>
<li>Switching the cpu core to another process requires saving the
current state of the cpu core to process PCB and restoring the state of
a different process from its PCB</li>
<li>Context switch time is pure overhead, because the system does no
useful work while switching <img src="/images/Courses/COSI131A/context_switch.png"></li>
</ul></li>
</ul></li>
<li>Operations on processes Ch 3.3
<ul>
<li>Process creation
<ul>
<li>Parent processes create child processes, the result is a tree of
processes</li>
<li>Most operating systems identify processes according to a unique
process identifier (pid)</li>
<li>When a process creates a child process, that child process will need
certain resources (cpu time, memory files, I/O devices) to accomplish
its task</li>
<li>Execution possibilities:
<ul>
<li>The parent continues to execute concurrently with its children</li>
<li>The parent waits until some or all of its children have
terminated</li>
</ul></li>
<li>Address space possibilities:
<ul>
<li>The child process is a duplicate of the parent process</li>
<li>The child process has a new program loaded into it</li>
</ul></li>
</ul></li>
<li>Process termination
<ul>
<li>When a process terminates, it asks the os to delete it by using the
<code>exit()</code> (exit with a status number:
<code>exit(exit_status_num)</code>) system call</li>
<li>The process may return a status value to its waiting parent
process</li>
<li>The resources (memory, files, I/O buffers) of the process are
de-allocated and reclaimed by the os</li>
<li>A process can cause the termination of another process via an
appropriate system call. Usually, such a system call cal be invoked only
by the parent of the process that is to be terminated. Reasons for
parent to terminate a child process:
<ul>
<li>The child has exceeded its usage of some of the resources that it
has been allocated</li>
<li>The task assigned to the child is no longer required</li>
<li>The parent is exiting and the operating system does not allow a
child to continue if its parent terminates</li>
</ul></li>
<li>Cascading termination: the phenomenon that all child processes are
terminated when the parent process terminates</li>
<li>A parent process can use the <code>wait()</code> system call to wait
for the termination of a child process</li>
<li>When a process is terminated, is resources are de-allocated by its
entry is still on the process table (the process table is a data
structure maintained by the os to facilitate context switching and
scheduling, each entry in the process table is a pointer to a specific
process control block).</li>
<li>Zombie process: if a process is terminated by its parent has not yet
called wait, then the process is called a zombie process. Only when the
parent process calls <code>wait()</code>, the record of a zombie process
is removed from the process table</li>
<li>Orphan process: if a process is running but its parent process has
terminated, then the process is called an orphan process. Traditional
Unix system assigns a new parent process to the orphan process</li>
</ul></li>
</ul></li>
<li>Inter-process communication (IPC) Ch 3.4
<ul>
<li>If a process doe not share data with any other processes executing
in the system, then it's independent. Otherwise a process is
cooperating</li>
<li>Why we need inter-process communication
<ul>
<li>Information sharing</li>
<li>Computation speedup</li>
<li>Modularity</li>
</ul></li>
<li>IPC models:
<ul>
<li>Shared memory: a region of memory is shared by cooperating
processes, processes can exchange information by reading and writing
data to the shared region
<ul>
<li>Faster: system call only occurs when creating the shared memory
region</li>
<li>If two processes write in the same location, there may be
conflicts</li>
</ul></li>
<li>Message passing: communication is achieved by message exchanged
between the cooperating processes
<ul>
<li>Easier to implement in a distributed system</li>
<li>Useful for exchanging smaller amounts of data, because no conflicts
need to be avoided</li>
</ul></li>
</ul></li>
<li>IPC in shared memory systems
<ul>
<li>Typically, os does not allow one process to access the memory of
another process, in order to create a shared memory, this restriction
needs to be removed</li>
<li>Producer and consumer with shared memory
<ul>
<li>Unbounded buffer: the buffer capacity is infinite</li>
<li>Bounded buffer: the producer must wait when the buffer is full, the
consumer must wait when the buffer is empty</li>
</ul></li>
</ul></li>
<li>IPC in message passing systems
<ul>
<li>Message passing facility allows processes to communicate and to
synchronize their actions</li>
<li>Operations
<ul>
<li><code>send(message)</code>: message size can be fixed or
variable</li>
<li><code>receive(message)</code>: message size can be fixed or
variable</li>
</ul></li>
<li>A link is needed for message passing</li>
<li>Direct or indirect communication
<ul>
<li>Direct communication
<ul>
<li><code>send(P, message)</code>: send a message to process P</li>
<li><code>receive(Q, message)</code>: receive a message from process Q,
this is symmetry addressing</li>
<li>A link is automatically created between two processes</li>
<li><code>receive(id, message)</code>: receive a message from any
process, the id is the process_id, this is asymmetry addressing</li>
</ul></li>
<li>Indirect communication
<ul>
<li><code>send(A, message)</code>: send a message to mailbox/port A</li>
<li><code>receive(A, message)</code>: receive a message from
mailbox/port A</li>
<li>A link is established if processes have a shared mailbox/port, then
the link may be associated with more than two processes</li>
</ul></li>
</ul></li>
<li>Synchronous or asynchronous communication
<ul>
<li>Message sending can be blocking/synchronous or
non-blocking/asynchronous</li>
<li>Blocking send, non-blocking send, blocking receive, non-blocking
receive</li>
</ul></li>
<li>Automatic or explicit buffering
<ul>
<li>The messages exchanged by communicating processes reside in a
temporary queue</li>
<li>Queue capacity
<ul>
<li>Zero capacity: the sender must block until the recipient receives
the message</li>
<li>Bounded capacity: the sender must block when the queue is full</li>
<li>Unbounded capacity: the sender is never blocked</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>Communication in client server system
<ul>
<li>Apart from the shared memory and message passing, the client-server
systems can also use two other strategies for communication: sockets and
remote procedure calls (RPCs)</li>
<li>Sockets
<ul>
<li>A socket is defined as an endpoint for communication, a pair of
processes communicating over a network employs a pair of sockets</li>
<li>A socket is identified by an IP address concatenated with a port
number</li>
<li>Generally, sockets use a client-serve architecture: the server waits
for incoming client requests by listening to a specified port, once a
request is received, the server accepts a connection from the client
socket to complete the connection. App ports below 1024 are considered
well known and are used to implement standard services</li>
<li>When a client initiates a request for a connection, it's assigned a
port by its host computer, the port has some arbitrary number greater
than 1024</li>
<li>All connections are unique</li>
<li>Java sockets
<ul>
<li>Connection-oriented sockets (TCP) implemented with the
<code>Socket</code> class</li>
<li>Connectionless sockets (UDP) use the <code>DatagramSocket</code>
class</li>
<li>The <code>MulticastSocket</code> class is a subclass of the
<code>DatagramSocket</code> class: a multicast socket allows data to be
set to multiple recipients</li>
</ul></li>
</ul></li>
<li>Remote procedure calls
<ul>
<li>RPC paradigm is one of the most common forms of remote service and
it's usually based upon the IPC mechanism with message-based
communication scheme</li>
<li>A port in RPC is simply a number included at the start of a message
packet. A system normally has one network address, but it can have many
ports</li>
<li>RPC allows a client to invoke a procedure on a remote host as it
would invoke a procedure locally, the RPC system hides the details</li>
<li>RPCs can fail due to common network errors, so RPC uses the exactly
once semantics for calls</li>
</ul></li>
</ul></li>
</ul></li>
<li>Threads &amp; Concurrency
<ul>
<li>Overview Ch 4.1
<ul>
<li>Benefits of multi-threaded programming
<ul>
<li>Responsiveness: a program can continue running even if part of it is
blocked or is performing a lengthy operation</li>
<li>Resource sharing: threads share the memory and the resources of the
process to which they belong by default, sharing resources among
processes is costly</li>
<li>Economy: it is more economical to create and context-switch
threads</li>
<li>Scalability: for multiprocessor architecture, threads may be running
in parallel on different processing cores</li>
</ul></li>
</ul></li>
<li>Multi-core programming Ch 4.2
<ul>
<li>Concurrency and parallelism
<ul>
<li>A concurrent system supports more than one task by allowing all the
tasks to make progress</li>
<li>A parallel system can perform more than one task simultaneously. It
is possible to have concurrency without parallelism</li>
</ul></li>
<li>Multi-core programming challenges
<ul>
<li>Identifying tasks: tasks are independent</li>
<li>Balance: tasks perform equal work of equal value</li>
<li>Data splitting: data should be divided to run on separate cores</li>
<li>Data dependency: data should be examined for dependencies between
two or more tasks</li>
<li>Testing and debugging: testing and debugging concurrent programs is
more difficult</li>
</ul></li>
<li>Types of parallelism
<ul>
<li>Data parallelism: distribute subsets of the same data across
multiple computing cores and performing the same operation on each
core</li>
<li>Task parallelism: distribute tasks across multiple computing
cores</li>
</ul></li>
</ul></li>
<li>Multi-threading models Ch 4.3
<ul>
<li>User threads and kernel threads
<ul>
<li>User threads are supported above the kernel and managed without
kernel support</li>
<li>Kernel threads are supported and managed directly by the os</li>
</ul></li>
<li>Multi-threading models measure the relationship between user threads
and kernel threads</li>
<li>Many-to-one model
<ul>
<li>Maps many user level threads to one kernel level thread</li>
<li>Efficient: thread management is done by thread library in user
space</li>
<li>The entire process will block if a thread makes a blocking system
call</li>
<li>Only one thread can access the kernel at a time, multiple threads
are unable to run in parallel on multi-core systems</li>
<li>Very few systems continue to use this model now</li>
</ul></li>
<li>One-t-one model
<ul>
<li>Maps each user thread to a kernel thread</li>
<li>Threads can run in parallel, one blocked thread does not affect
other threads</li>
<li>Each user thread corresponds to a kernel thread, this may burden the
performance of a system</li>
<li>Linux and Windows implements one-to-one model</li>
</ul></li>
<li>Many-to-many model
<ul>
<li>Multiplexes many user level threads to a smaller or equal number of
kernel threads</li>
<li>Allows parallelism, the system is efficient</li>
<li>Difficult to implement in practice</li>
</ul></li>
</ul></li>
<li>Thread library Ch 4.4
<ul>
<li>A thread library provides the programmer with an API for creating
and managing threads</li>
<li>Implementation approaches
<ul>
<li>Provide a library in user space with no kernel support: invoking a
function in the library results in a local function call in user space
and not a system call</li>
<li>Implement a kernel level library supported directly by the os:
invoking a function in the library results in a system call to the
kernel</li>
</ul></li>
<li>Main thread libraries
<ul>
<li>POSIX Pthreads: can be either a user level library or a kernel level
library</li>
<li>Windows thread library: kernel level library</li>
<li>Java thread API: allows threads to be created and managed directly
in java programs, because in most cases, the JVM is running on top of a
host os, the Java thread API is generally implemented using a thread
library available on the host system</li>
</ul></li>
<li>Ptrheads
<ul>
<li>Pthreads refer to the POSIX standard defining an API for thread
creation an synchronization</li>
<li>It is a specification for thread behavior, not an
implementation</li>
</ul></li>
<li>Windows threads
<ul>
<li>Similar to the Pthreads</li>
</ul></li>
<li>Java threads
<ul>
<li>Thread creation in Java
<ul>
<li>Extends the <code>Thread</code> class and overwrites the
<code>run()</code> method</li>
<li>Implements the <code>Runnable</code> interface and overwrites the
<code>run()</code> method</li>
</ul></li>
<li>Interrupt
<ul>
<li>Call the <code>join()</code> method on a child thread to that the
parent thread waits until the child thread terminates
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  Thread t = <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">        System.out.println(i);</span><br><span class="line">        Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">  t.start();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    t.join();</span><br><span class="line">  &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
</ul></li>
<li>Implicit threading Ch 4.5
<ul>
<li>Implicit threading: inorder to address the difficulties of handling
large amounts of threads and better support the design of concurrent and
parallel applications, the creation and management of threading should
be transferred from developer to compiler and run-time libraries</li>
<li>Tread pools
<ul>
<li>Idea: create a pool of threads, when there is a request, find one
idle thread in the pool and let it service the request. If there is no
idle thread, the task is queued until one thread becomes available. Once
a thread finishes its task, it returns to the pool and awaits next
task</li>
<li>Benefits
<ul>
<li>Servicing a request with an existing thread is faster than creating
a new thread</li>
<li>A pool can limit the number of threads that exist at any give time,
this alleviates the burden of os</li>
<li>Separating the task from the mechanics of creating the task allows
user to use different strategies for running the task</li>
</ul></li>
</ul></li>
<li>Fork join
<ul>
<li>The parent thread creates (forks) one ore more child threads and
then waits for the children to terminate and join with it, at which
point it can retrieve and combine their results</li>
</ul></li>
<li>OpenMP
<ul>
<li>OpenMP is a set of compiler directives as well as an API for
programs written in C, C++, or FORTRAN that provides support for
parallel programming in shared memory environments</li>
<li>OpenMP identifies parallel regions as blocks of code that may run in
parallel</li>
<li>OpenMP also allows developer s to choose among several level of
parallelism</li>
</ul></li>
<li>Grand central dispatch (GCD)
<ul>
<li>GCD is developed by Apple</li>
<li>It is a combination of a run time library, an API, and language
extension that allow developers to identify sections of code to run in
parallel</li>
<li>GCD schedules tasks for run time execution by placing them on a
dispatch queue</li>
</ul></li>
</ul></li>
</ul></li>
<li>CPU Scheduling
<ul>
<li>Basic concepts Ch 5.1
<ul>
<li>CPU-I/O burst cycle
<ul>
<li>Process execution consists of a cycle of cpu execution and I/O wait,
processes alternate between these two states</li>
<li>Process execution begins with a cpu burst, then an I/O burst, then
..., the final cpu burst ends with a system request to terminate
execution</li>
<li>CPU bursts tend to have a large number of short cpu bursts, and a
small number of long cpu bursts. An I/O bound program tends to have many
short cpu bursts, a cpu bound program might have a few long cpu
bursts</li>
</ul></li>
<li>CPU scheduler
<ul>
<li>When a cpu is idle, the cpu scheduler selects a process from the
processes in memory that are ready to execute and allocates the cpu to
that process</li>
<li>The ready queue is not necessarily a FIFO queue, its structure
varies depending on the scheduling algorithm</li>
</ul></li>
<li>Preemptive and non-preemptive scheduling
<ul>
<li>Four circumstances where cpu scheduling decision may take place
<ul>
<li>A process switches from the running state to the waiting state</li>
<li>A process switches from the running state to the ready state</li>
<li>A process switches from the waiting state to the ready state</li>
<li>A process terminates</li>
</ul></li>
<li>A non-preemptive(cooperative) scheduling algorithm picks a process
to turn and then just lets it run until it blocks or voluntarily
releases the cpu</li>
<li>A preemptive scheduling algorithm picks a process and lets it run
for a maximum of some fixed time. If it is still running at the end of
the time interval, it is suspended and the scheduler picks another
process to run</li>
<li>Preemptive scheduling can result in race condition</li>
<li>Preemption also affects the design of the os kernel. If a process is
preempted during a system call in which some state of the kernel is
modified, then chaos ensues. A non-preemptive will wait for a system
call to complete</li>
</ul></li>
</ul></li>
<li>Scheduling criteria Ch 5.2
<ul>
<li>CPU utilization
<ul>
<li>We should maximize the cpu utilization</li>
<li>In a real system, the cpu utilization should range from 40% (lightly
loaded system) to 90% (heavily loaded system)</li>
</ul></li>
</ul></li>
<li>Throughput
<ul>
<li>We should maximize the throughput</li>
<li>Throughput is the amount of processes that are completed per time
unit</li>
</ul></li>
<li>Turnaround time
<ul>
<li>We should minimize the turnaround time</li>
<li>Turnaround time: the interval from the time of submission of a
process to the time of completion of the process</li>
<li>Turnaround time is the sum of the periods spend waiting in the ready
queue, executing on the cpu and doing I/O</li>
</ul></li>
<li>Waiting time
<ul>
<li>We should minimize the waiting time</li>
<li>The scheduling algorithm does not affect the amount of time during
which a process executes or does I/O, it affects only the amount of time
that a process spends waiting in the ready queue</li>
<li>Waiting time is the sum of the periods spend waiting in the ready
queue</li>
</ul></li>
<li>Response time
<ul>
<li>We should minimize the response time</li>
<li>Response time is the time from the submission of a request until the
first response is produced (the time it takes to start responding, not
the time it takes to output the response)</li>
<li>This measure is more appropriate for an interactive system</li>
</ul></li>
<li>Scheduling algorithms Ch 5.3
<ul>
<li>We assume there is a single cpu with a single processing core, thus
the system is capable of only running one process at a time</li>
<li>Use Gantt chart to visualize the scheduling algorithm</li>
<li>First-come first-serve (FCFS)
<ul>
<li>Idea: the process that requests the cpu first is allocated the cpu
first</li>
<li>FCFS is a non-preemptive scheduling algorithm</li>
<li>FCFS can be implemented using a FIFO queue (actually the scheduling
queue can just be the ready queue)</li>
<li>Convoy effect: the whole system is slow down due to a few slower
processes (if the process with the longest cpu burst arrives at first,
then the average turnaround time will be very long)</li>
<li>FCFS is bad for troublesome for interactive systems</li>
</ul></li>
<li>Shortest-job-first scheduling (SJF)
<ul>
<li>Idea: when the cpu is available, the process that has the smallest
next cpu burst is allocated to the cpu, if the next cpu burst of
multiple processes are the same, FCFS scheduling is used to break the
tie</li>
<li>The SFJ scheduling algorithm is provably optimal, because it gives
the minimum average waiting time for a given set of processes</li>
<li>SJF cannot be implemented at the level of cpu scheduling, because
there is no way to know the length of the next cpu burst, instead we can
pick the process with the shortest predicted cpu burst</li>
<li>The next cpu burst is generally predicted using an exponential
average: <span class="math inline">\(\tau_{n + 1} = \alpha t_n +
(1-\alpha) \tau_{n}\)</span>, where <span class="math inline">\(t_n\)</span> is the nth cpu burst, and <span class="math inline">\(\tau_{n+1}\)</span> is the predicted value of the
next cpu burst. Actually <span class="math inline">\(\tau_{n+1} = \alpha
t_n + \sum\limits_{i=0}^{n-1}\alpha (1-\alpha)^{n-i}t_{i}\)</span></li>
<li>Preemptive SJF: shortest-remaining-time-first scheduling: when a new
process comes and the older one is still running, if the next cpu burst
of the new process is shorter than the remaining cpu burst of the older
one, then the older process is preempted and the new process is
allocated to the cpu</li>
</ul></li>
<li>Round-robin scheduling (RR)
<ul>
<li>Idea: similar to FCFS with preemption via time quantum (time slice).
The ready queue is treated as a circular queue, each process can be
executed for an interval of up to 1 time quantum</li>
<li>The average waiting time for RR is often long</li>
<li>If the time quantum is very large, then RR is just FCFS. If the time
quantum is very small, then there are a large number of context
switches. The time quantum should be set so that context switch time is
a relative small fraction of the time quantum</li>
<li>If time quantum increases, the turnaround time does not necessarily
decreases</li>
</ul></li>
<li>Priority scheduling
<ul>
<li>Idea: a priority is associated with each process, and the cpu is
allocated to the process with the highest priority, equal-priority
processes are scheduled in FCFS order</li>
<li>A SJF is simply a priority scheduling algorithm where the priority
is the inverse of the next cpu burst</li>
<li>Priorities can be defined either internally or externally</li>
<li>Priority scheduling can be either non-preemptive or preemptive</li>
<li>A major problem with priority scheduling is indefinite blocking, or
starvation: a low priority process can wait indefinitely.</li>
<li>A solution to the problem of indefinite blockage of low-priority
processes is aging, which meas gradually increases the priority of
processes that wait in the system for a long time</li>
<li>Another solution is to combine RR and priority scheduling (same
priority processes are scheduled in RR)</li>
</ul></li>
<li>Multilevel queue scheduling
<ul>
<li>Idea: there are separate queues for each distinct priority,
scheduling between queues are priority scheduling, scheduling inside one
queue is round-robin</li>
<li>Processes have a static priority, so they reside in the same queue
forever</li>
</ul></li>
<li>Multilevel feedback queue scheduling
<ul>
<li>Idea: multilevel queue scheduling + processes can move between
queues</li>
</ul></li>
</ul></li>
</ul></li>
<li>Synchronization Tools
<ul>
<li>Background Ch 6.1
<ul>
<li>Concurrent or parallel execution can contribute to issues involving
the integrity of data shared by several processes</li>
<li>Race condition: a situation where several processes access and
manipulate the same data concurrently and the outcome of the execution
depends on the particular order in which the access take place is called
race condition</li>
<li>We use process synchronization to prevent race condition</li>
</ul></li>
<li>The critical section problem Ch 6.2
<ul>
<li>Situation: a system consists of n processes <span class="math inline">\(\{P_0, ..., P_n\}\)</span>, each process has a
critical section part in which process my access and update data that is
shared with at least one other process</li>
<li>The critical-section problem is to design a protocol that the
processes can use to synchronize their activity so as to cooperatively
share data</li>
<li>Procedure of each process: entry section -&gt; critical section
-&gt; remainder section</li>
<li>Requirements for critical section problems
<ul>
<li>Mutual exclusion: if process <span class="math inline">\(P_i\)</span> is executing its critical section,
then no other process can execute its critical section</li>
<li>Progress: if no process is executing in its critical section and
some processes wish to enter their critical section, then only the
processes that are not int the exit section can decide which will enter
the critical section next, and this selection cannot be postponed
indefinitely</li>
<li>Bounded waiting: if a processes has made a request to enter its
critical section, the its waiting time should be bounded before the
request is granted</li>
</ul></li>
<li>Two general approaches to handle critical sections in os:
<ul>
<li>Preemptive kernels
<ul>
<li>Processes in the kernel mode cannot be preempted, they will run
until it exits the kernel mode, blocks, or voluntarily yields control of
the cpu</li>
<li>Free from race conditions</li>
</ul></li>
<li>Non-preemptive kernels
<ul>
<li>More responsive</li>
<li>More suitable for real-time programming</li>
</ul></li>
</ul></li>
</ul></li>
<li>Peterson's Solution Ch 6.3
<ul>
<li>Peterson's solution is a software-based solution to the critical
section problem</li>
<li>Given the way that modern computer architectures perform
machine-language instructions (such as <code>LOAD</code>,
<code>STORE</code>), there is no guarantee that Peterson's solution
works on such architectures</li>
<li>Idea: two processes alternate execution between critical sections
and other sections, if one is ready (wishes) to enter its critical, the
other process will wait (if one is not ready to, or wishes to enter its
critical section, it will skips its turn to enter the critical
section)</li>
<li>Algorithm
<ul>
<li>Shared variables
<ul>
<li><code>int turn</code>: if <code>turn == 0</code>, then <span class="math inline">\(P_0\)</span> can enter its critical section, if
<code>turn == 1</code>, then <span class="math inline">\(P_1\)</span>
can enter its critical section</li>
<li><code>boolean flag[2]</code>: if <code>flag[0] == true</code>, then
<span class="math inline">\(P_0\)</span> is ready (wishes) to enter its
critical section, if <code>flag[1] == true</code>, then <span class="math inline">\(P_1\)</span> is ready (wishes) to enter its
critical sections</li>
</ul></li>
<li>Java implementation <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  <span class="comment">// Entry section</span></span><br><span class="line">  flag[i] = <span class="keyword">true</span>;</span><br><span class="line">  turn = j; <span class="comment">// turn = 1 - i</span></span><br><span class="line">  <span class="keyword">while</span> (flag[j] &amp;&amp; turn == j) &#123;</span><br><span class="line">    ; <span class="comment">// Wait</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Critical section</span></span><br><span class="line">  flag[i] = <span class="keyword">false</span>;</span><br><span class="line">  <span class="comment">// Remainder section</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Proof of mutual exclusion, progress and bounded waiting
<ul>
<li>Mutual exclusion
<ul>
<li>If <span class="math inline">\(P_i\)</span> enters critical section:
<code>flag[i] == true and (turn == i or flag[j] == false)</code></li>
<li>If <span class="math inline">\(P_j\)</span> enters critical section:
<code>flag[j] == true and (turn == j or flag[i] == false)</code></li>
<li>These two conditions cannot happen simultaneously</li>
</ul></li>
<li>Progress: <span class="math inline">\(P_i\)</span> is prevented from
enter critical section if <code>flag[j] == true and turn == j</code>,
then <span class="math inline">\(P_j\)</span> must be in critical
section so it cannot be in non-critical section</li>
<li>Bounded waiting: after <span class="math inline">\(P_j\)</span>
leaves its critical section, it sets <code>flag[j] = false</code>, so
<span class="math inline">\(P_i\)</span> waits at most one round</li>
</ul></li>
<li>Why Peterson's solution may not work: to improve performance, modern
computers may reorder read/write instructions for independent variables
<ul>
<li>Example <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Thread 1</span></span><br><span class="line"><span class="keyword">boolean</span> flag = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">int</span> x = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (!flag) &#123;</span><br><span class="line">  ;</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(x);</span><br><span class="line"><span class="comment">// Thread 2</span></span><br><span class="line"><span class="keyword">int</span> x = <span class="number">100</span>;</span><br><span class="line">flag = <span class="keyword">true</span>;</span><br></pre></td></tr></table></figure> We want Thread to print 100, but if computer
reorders the instructions in Thread 2, then Thread 1 will print 1</li>
</ul></li>
</ul></li>
<li>Hardware support for synchronization Ch 6.4
<ul>
<li>Memory Barriers
<ul>
<li>Memory model: how a computer determines what memory guarantees it
will provide to an application program is called the memory mode</li>
<li>Memory model categories:
<ul>
<li>Strongly ordered model: a memory modification on one processor is
immediately visible to all other processes</li>
<li>Weakly ordered model: modifications on one processor may not be
immediately visible to other processes</li>
</ul></li>
<li>Memory barriers/ fences: computer instructions that can force any
changes in memory to be propagated to all other processors, thereby
ensuring that memory modifications are visible to threads running on
other processors
<ul>
<li>An example <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Thread 1</span></span><br><span class="line"><span class="keyword">boolean</span> flag = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">int</span> x = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (!flag) &#123;</span><br><span class="line">  memory_barrier();</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(x);</span><br><span class="line"><span class="comment">// Thread 2</span></span><br><span class="line"><span class="keyword">int</span> x = <span class="number">100</span>;</span><br><span class="line">memory_barrier();</span><br><span class="line">flag = <span class="keyword">true</span>;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
<li>Hardware instructions
<ul>
<li>Modern computer architectures usually provide atomic instructions to
test and modify the contents of a variable or swap the contents of
variables</li>
<li><code>test_and_set()</code> and mutual exclusion
<ul>
<li><code>test_and_set()</code> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">testAndSet</span><span class="params">(<span class="keyword">boolean</span> flag)</span> </span>&#123;</span><br><span class="line">  oldVal = flag;</span><br><span class="line">  flag = <span class="keyword">true</span>;</span><br><span class="line">  <span class="keyword">return</span> oldVal;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Mutual exclusion with test_and_set <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">boolean</span> lock = <span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  <span class="comment">// Entry section</span></span><br><span class="line">  testAndSet(lock) &#123;</span><br><span class="line">    ; <span class="comment">// Wait here</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Critical section</span></span><br><span class="line">  lock = <span class="keyword">false</span>;</span><br><span class="line">  <span class="comment">// Remainder section</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><code>compare_and_swap()</code> and mutual exclusion
<ul>
<li><code>compare_and_swap()</code> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">compareAndSwap</span><span class="params">(<span class="keyword">int</span> val, <span class="keyword">int</span> exp, <span class="keyword">int</span> newVal)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> tmp = val;</span><br><span class="line">  <span class="keyword">if</span> (val == exp) &#123;</span><br><span class="line">    val = newVal;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> tmp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Mutual exclusion with compare_and_swap <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> lock = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  <span class="comment">// Entry section</span></span><br><span class="line">  <span class="keyword">while</span> (compareAndSwap(val, <span class="number">0</span>, <span class="number">1</span>)) &#123;</span><br><span class="line">    ; <span class="comment">// Wait here</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Critical section</span></span><br><span class="line">  lock = <span class="number">0</span>;</span><br><span class="line">  <span class="comment">// Remainder section</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
<li>Atomic variables
<ul>
<li>Atomic variables: a tool that is based on compare_and_swap and is
used to solve the critical section problem</li>
<li>Most systems that support atomic variables provide special atomic
data structures and functions for accessing and manipulating atomic
variables
<ul>
<li>Example <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">increment</span><span class="params">(<span class="keyword">int</span> atomicVar)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> tmp;</span><br><span class="line">  <span class="keyword">do</span> &#123;</span><br><span class="line">    tmp = atomicVar;</span><br><span class="line">  &#125; <span class="keyword">while</span> (tmp != compareAndSwap(v, tmp, tmp + <span class="number">1</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
</ul></li>
<li>Mutex locks Ch 6.5
<ul>
<li>Hardware solutions are generally complicated and inaccessible to
application programmers, so os usually provides higher-level software
solutions</li>
<li>The simplest solution is mutex lock (mutex is short for mutual
exclusion). A process must acquire the lock before entering the critical
section and releasing the lock after critical section is completed</li>
<li>Mutex lock
<ul>
<li><code>acquire()</code> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">acquire</span><span class="params">(<span class="keyword">boolean</span> available)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (!available) &#123;</span><br><span class="line">    ; <span class="comment">// Wait</span></span><br><span class="line">  &#125;</span><br><span class="line">  available = <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><code>release()</code> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">release</span><span class="params">(<span class="keyword">boolean</span> available)</span> </span>&#123;</span><br><span class="line">  available = <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Mutual exclusion and mutex lock <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">boolean</span> available = <span class="keyword">true</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  <span class="comment">// Entry section</span></span><br><span class="line">  acquire(available);</span><br><span class="line">  <span class="comment">// Critical section</span></span><br><span class="line">  release(available);</span><br><span class="line">  <span class="comment">// Remainder section</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>The above lock is called spin-lock, because the process spins while
waiting for the lock to become available</li>
<li>Advantages: no context switch is required when a process must wait
on a lock, context switch may take a considerable time</li>
<li>Disadvantages: busy waiting, cpu utilization is not optimized</li>
</ul></li>
</ul></li>
</ul></li>
<li>Semaphores Ch 6.6
<ul>
<li>Compared with mutex locks, semaphores can provide more sophisticated
ways for processes to synchronize their activities</li>
<li>Semaphore: a semaphore is an integer variable that, apart from
initialization, is accessed only by two standard atomic operations,
<code>wait()</code>, <code>signal()</code></li>
<li><code>wait()</code> and <code>signal()</code> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">wait</span><span class="params">(<span class="keyword">int</span> semaphore)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (semaphore &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">    ; <span class="comment">// Wait</span></span><br><span class="line">  &#125;</span><br><span class="line">  semaphore--;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">signal</span><span class="params">(<span class="keyword">int</span> semaphore)</span> </span>&#123;</span><br><span class="line">  semaphore++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Counting semaphore and binary semaphore
<ul>
<li>Counting semaphore: semaphores that can range over an unrestricted
domain</li>
<li>Binary semaphores: semaphores that can be either 0 or 1</li>
</ul></li>
<li>Semaphore implementation
<ul>
<li>The previous <code>wait</code> and <code>signal</code> still use
busy waiting</li>
<li>Modified waiting: if wait is called, and the value of semaphore is
not positive, then instead of busy waiting, we add the process to a
waiting queue associated with the semaphore, and the state of the
process is switched from running to waiting, and then the cpu scheduler
can pick another process to run</li>
<li>Implementation <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Semaphore</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> value;</span><br><span class="line">  Queue&lt;Process&gt; waitingQueue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wait(Semaphore s) &#123;</span><br><span class="line">  s.value --;</span><br><span class="line">  <span class="keyword">if</span> (s.value &lt; <span class="number">0</span>) &#123;</span><br><span class="line">    s.waitingQueue(thisProcess);</span><br><span class="line">    <span class="keyword">this</span>.Process.sleep();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">signal(Semaphore s) &#123;</span><br><span class="line">  s.value++;</span><br><span class="line">  <span class="keyword">if</span> (s.value &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">    Process p = s.waitingQueue.remove();</span><br><span class="line">    p.wakeUp();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>When the value of semaphore is negative, its magnitude is the number
of processes waiting in the waiting queue of this semaphore</li>
<li>Semaphore operations should be atomic</li>
</ul></li>
</ul></li>
<li>Monitors Ch 6.7
<ul>
<li>Various errors can happen when programmers misuse semaphores or
mutex locks to solve critical section problems, one strategy for
handling these errors is to incorporate simple synchronization tools as
high-level language constructs, for example, the monitor type.</li>
<li>A monitor type is an ADT that includes a set of programmer-defined
operations that are provided with mutual exclusion within the monitor,
the monitor type also declares the variables whose values define the
state of an instance of that type, along with the bodies of functions
that operate on those variables</li>
<li>Only one process at a time is active within the monitor</li>
<li>The monitor type itself is not sufficiently powerful for modeling
some synchronization schemes, so we need to define additional
synchronization mechanisms provided by the condition type. The only
operations that can be invoked on a condition variable are
<code>wait()</code> and <code>signal()</code>
<ul>
<li><code>x.wait()</code> with monitor <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// condition x: binary semaphore x_sem and integer x_count, both initialized to 0</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">F</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  x_count ++;</span><br><span class="line">  <span class="keyword">if</span> (next_count &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    signal(next);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    signal(mutex);</span><br><span class="line">  &#125;</span><br><span class="line">  wait(s_sem);</span><br><span class="line">  x_count --;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><code>x.signal()</code> with monitor <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (x_count &gt; <span class="number">0</span>) &#123;</span><br><span class="line">  next_count ++;</span><br><span class="line">  signal(x_sem);</span><br><span class="line">  wait(next);</span><br><span class="line">  next_count--;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Resume processes with a monitor
<ul>
<li>When <code>x.signal()</code> is called on a condition x and there
are multiple suspended processes, there are different wats for the
monitor to determine which process to wake up</li>
<li>The simplest way is to use a FCFS policy, then the process that has
waited for the longest part should be waken up</li>
<li>In circumstances that FCFS does not work, the monitor can use
conditional wait: <code>x.signal(cond)</code>, where cond is an integer
expression that is evaluated when the wait operation is executed. The
value of cond, which is called priority number, is stored in the
suspended process, and the process with the lowest priority number is
waken up</li>
</ul></li>
</ul></li>
</ul></li>
<li>Synchronization examples
<ul>
<li>Classic problems of synchronization
<ul>
<li>The bounded buffer problem
<ul>
<li>The producers and consumers share these data: <code>int n</code>,
<code>Semaphore mutex = 1</code>, <code>Semaphore empty = n</code>,
<code>Semaphore full = 0</code></li>
<li>The pool consists of n buffers, each capable of holding one item,
the mutex binary semaphore provides mutual exclusion for accesses to the
buffer pool, the empty and full semaphores count the number of empty and
full buffers</li>
<li>Producer code <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  <span class="comment">// Produce an item</span></span><br><span class="line">  wait(empty);</span><br><span class="line">  wait(mutex);</span><br><span class="line">  <span class="comment">// Add item to the buffer</span></span><br><span class="line">  signal(mutext);</span><br><span class="line">  signal(full);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Consumer code <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">  wait(full);</span><br><span class="line">  wait(mutex);</span><br><span class="line">  <span class="comment">// Remove an item from buffer</span></span><br><span class="line">  signal(mutex);</span><br><span class="line">  signal(empty);</span><br><span class="line">  <span class="comment">// Consume the item</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Readers-writers problem
<ul>
<li>If readers access the database simultaneously, no adverse effects
will result. However, if a writer and some other process access the
database simultaneously, chaos may ensue</li>
<li>We require that the writers have exclusive access to the shared
database whill writing to the database</li>
<li>Variations of readers-writers problem
<ul>
<li>The first readers-writers problem
<ul>
<li>No reader shall be kept waiting if the shared data is opened for
reading</li>
<li>In this variation, writers may starve</li>
</ul></li>
<li>The second readers-writers problem
<ul>
<li>No writer in the waiting queue shall be kept waiting longer than
absolutely necessary</li>
<li>In this variation, readers may starve</li>
</ul></li>
<li>The third readers-writers problem
<ul>
<li>No thread shall be allowed to be starve</li>
<li>The operation of obtaining a lock on a shared data will always
terminate in a bounded amount of time</li>
</ul></li>
</ul></li>
<li>Solution to the first readers-writers problem
<ul>
<li>Shared variables:
<ul>
<li><code>Semaphore rw_mutex = 1</code>: binary semaphore, used to
provide mutual exclusion to writers, also used for the first reader to
enter and the last reader to exit the critical section</li>
<li><code>Semaphore mutex = 1</code>: binary semaphore, ensure mutual
exclusion for updating the read_count variable</li>
<li><code>int read_count = 0</code>: count how many readers are
accessing the shared data</li>
</ul></li>
<li>Code of a writer process <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  wait(rw_mutex);</span><br><span class="line">  <span class="comment">// Perform writing</span></span><br><span class="line">  signal(rw_mutex);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Code of a reader process <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">  wait(mutex);</span><br><span class="line">  read_count ++;</span><br><span class="line">  <span class="keyword">if</span> (read_count == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="comment">// If this is the first reader</span></span><br><span class="line">    wait(rw_mutex);</span><br><span class="line">  &#125;</span><br><span class="line">  signal(mutex);</span><br><span class="line">  <span class="comment">// Perform reading</span></span><br><span class="line">  wait(mutex);</span><br><span class="line">  read_count--;</span><br><span class="line">  <span class="keyword">if</span> (read_count == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// If this is the last reader</span></span><br><span class="line">    signal(rw_mutex);</span><br><span class="line">  &#125;</span><br><span class="line">  signal(mutex;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
<li>The dinning-philosophers problem
<ul>
<li>A circular table with n philosophers and n chopsticks. Philosophers
think individually, when a philosopher is hungry, she picks two nearest
chopsticks and eat, when she finishes eating, she releases the two
chopsticks</li>
<li>Semaphore solution
<ul>
<li>Represent each chopstick with a binary semaphore, a philosopher
grabs a chopstick by executing the wait() operation on that semaphore,
and release the chopstick by executing the signal() operation on the
semaphore</li>
<li>This solution can result in a deadlock: all philosophers require the
wait operation on the chopsticks on their left side, then require the
wait operation on the chopsticks on their right side</li>
</ul></li>
<li>Monitor solution
<ul>
<li>Denote the state of a philosopher be eating, thinking, hungry, a
philosopher can only be eating if the two neighbors of her are not
eating</li>
</ul></li>
</ul></li>
</ul></li>
<li>Synchronization in Java Ch 7.4
<ul>
<li>Every object in Java has associated with it a single lock</li>
<li>You can use synchronized keyword in the method signature to indicate
the method is synchronized, calling synchronized method requires owning
the lock for the object</li>
<li>If one synchronized method requires the lock, but the lock is owned
by another thread, then calling method blocks and is placed in the entry
set for the object's lock. The entry set represents the set of threads
waiting for the lock to become available</li>
<li>If the lock is released, but the entry set of the object is not
empty, then JVM arbitrarily picks one thread from the entry set to own
the lock</li>
<li>Each object also has a wait set, if a thread enters a synchronized
method but is unable to continue because some condition has not been
met, then the thread releases the lock, the state of the thread is
changed to blocked, and the thread is placed in the wait set of the
object</li>
<li>Reentrant lock
<ul>
<li>A ReentrantLock is owned by a single thread and is used to provide
mutually exclusive access to a shared resource, and it can provide
additional features, such as setting a fairness parameter, which favors
granting the lock to the longest-waiting thread</li>
<li>A thread acquires a ReentrantLock lock by invoking its
<code>lock()</code> method. If the lock is available or the thread
already owns the lock, lock method assigns the invoking thread lock
ownership and returns control, if the lock is not available, the
invoking thread blocks until it is ultimately assigned the lock when the
lock owner invokes <code>unlock()</code> method</li>
<li>ReentrantLock and critical section <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Lock key = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">key.lock();</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="comment">// Critical section</span></span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  key.unlock();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Semaphore
<ul>
<li>Java also provides a counting semaphore</li>
<li>Constructor: <code>Semaphore(int value)</code></li>
<li>Semaphore and critical section <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Semaphores sem = <span class="keyword">new</span> Semaphore(<span class="number">1</span>);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  sem.acquire();</span><br><span class="line">  <span class="comment">// Critical section</span></span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">  e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  sem.release();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Condition variables
<ul>
<li>Condition variables provide functionally similar to the
<code>wait()</code> and <code>notify()</code> methods</li>
<li>Create a condition <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Lock key = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">Condition condVar = key.newCondition();</span><br></pre></td></tr></table></figure></li>
<li>Condition example <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Lock key = <span class="keyword">new</span> ReentrantLock();</span><br><span class="line">Condition condVar = key.newCondition();</span><br><span class="line">key.lock();</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (conditionNotMet) &#123; <span class="comment">// User-defined boolean condition  </span></span><br><span class="line">    condVar.await();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Do some work</span></span><br><span class="line">  condVar.signal();</span><br><span class="line">&#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">  e.printStackTrace();</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  lock.unlock();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
</ul></li>
<li>Deadlocks
<ul>
<li>Introduction
<ul>
<li>In a multiprogramming environment, several threads may compete for a
finite number of resources. A thread request resources, if the resources
are not available, the thread enters a waiting state</li>
<li>Sometimes, a waiting thread can never change state, because the
resources it has requested are held by other waiting threads. This
situation is called a deadlock</li>
<li>Definition of deadlock: every process in a set of processes is
waiting for an even that can be caused only by another process in the
set</li>
</ul></li>
<li>System model Ch 8.1
<ul>
<li>A system consists of a finite number of resources (CPU cycles,
files, I/O devices, etc.) to be distributed among a number of competing
threads</li>
<li>A thread may utilize a resource in only the following sequence:
request, use, release</li>
</ul></li>
<li>Deadlock in multithreaded applications Ch 8.2
<ul>
<li>An deadlock example <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Thread A</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">work1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  lock1.lock();  </span><br><span class="line">  <span class="comment">// If there is context switch after this line, then deadlock will occur</span></span><br><span class="line">  lock2.lock();</span><br><span class="line">  <span class="comment">// Do some work</span></span><br><span class="line">  lock2.unlock();</span><br><span class="line">  lock1.unlock();</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Thread B</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">work1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  lock2.lock();</span><br><span class="line">  lock1.lock();</span><br><span class="line">  <span class="comment">// Do some work</span></span><br><span class="line">  lock1.unlock();</span><br><span class="line">  lock2.unlock();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>Livelock
<ul>
<li>Livelock is another kind of liveness failure, similar to
deadlock</li>
<li>Deadlock prevents threads from proceeding because when each thread
in a set is blocked waiting for an event that can be caused only by
another thread in the set</li>
<li>Live lock prevents threads from proceeding because when threads
continuously attempt an action that fails</li>
</ul></li>
</ul></li>
<li>Deadlock characterization Ch 8.3
<ul>
<li>Necessary conditions (no sufficient conditions)
<ul>
<li>Mutual exclusion: at least one resource must be held in a
non-sharable node, if another thread requests that resource, the
requesting thread must be delayed until the resource has been
released</li>
<li>Hold and wait: a thread must be holding at least one resource and
waiting to acquire additional resources that are currently being held by
other threads</li>
<li>No preemption: resources cannot be preempted</li>
<li>Circular wait: a set <span class="math inline">\(\{T_0, T_1, ...,
T_n \}\)</span> of waiting threads must exist such that <span class="math inline">\(T_0\)</span> is waiting for a resource held by
<span class="math inline">\(T_1\)</span>, ..., <span class="math inline">\(T_n\)</span> is waiting for a resource held by
<span class="math inline">\(T_1\)</span></li>
</ul></li>
<li>Resource allocation graph
<ul>
<li>A system resource allocation graph can describe more precisely
deadlocks</li>
<li>Vertices: <span class="math inline">\(T = \{T_1, ..., T_n
\}\)</span> represent threads, <span class="math inline">\(R = \{R_1,
..., R_m \}\)</span> represent resource types, each thread can request
multiple multiple instances from multiple resource types, each resource
type can have multiple instances, each instance can be requested by
different threads</li>
<li>Edges: directed edge <span class="math inline">\(T_i \rightarrow
R_j\)</span> means <span class="math inline">\(T_i\)</span> requests an
instance of resource type <span class="math inline">\(R_j\)</span>, this
is called a request edge, directed edge <span class="math inline">\(R_j
\rightarrow T_i\)</span> means an instance of resource type <span class="math inline">\(R_j\)</span> is held by <span class="math inline">\(T_i\)</span>, this is called an assignment
edge</li>
<li>Example <img src="/images/Courses/COSI131A/resource_allocation_graph.png"></li>
<li>Observations
<ul>
<li>If there is no cycle in the resource allocation graph, then there is
no deadlock</li>
<li>If there is a cycle, then the system may be in the deadlock
state</li>
<li>If there is a cycle (circular waiting), each resource type has one
instance, then there must be a deadlock</li>
</ul></li>
</ul></li>
</ul></li>
<li>Methods for handling deadlocks Ch 8.4
<ul>
<li>Three ways to deal with the deadlock problem
<ul>
<li>Ignore the problem, pretend the deadlocks do not exist
<ul>
<li>This is adopted by most OS (including Linux and Windows), then it's
up to the developers to handle deadlocks, typically using approaches
outlined in the second solution</li>
<li>Dealing with deadlocks are costly, and deadlocks generally happen
infrequently</li>
</ul></li>
<li>Use a protocol to prevent or avoid deadlocks, then the system never
enters a deadlock state
<ul>
<li>Deadlock Prevention: provides a set of methods to ensure that at
least one of the necessary conditions cannot hold, this approach
constrains how requests for some resources can be made</li>
<li>Avoidance: require that the os be given additional information in
advance concerning which resources a thread will request and use during
its lifetime. With the additional information, the os can decide for
each request whether or not the thread should wait</li>
</ul></li>
<li>Allow the system to enter a deadlock state, detect it, and recover.
Some databases adopt this approach
<ul>
<li>Use deadlock detection algorithm to check if deadlocks occur, use
recovery algorithm to recover from deadlocks</li>
</ul></li>
</ul></li>
</ul></li>
<li>Deadlock Prevention Ch 8.5
<ul>
<li>Mutual exclusion
<ul>
<li>This condition must be hold</li>
<li>Shareable resources do not require mutually exclusive access and
cannot be involved in a deadlock</li>
<li>Shareable resource example: read-only files</li>
<li>Non-shareable resources: mutex lock</li>
</ul></li>
<li>Hold and wait
<ul>
<li>To break this condition, we require that whenever a thread requests
a resource, it does not hold any other resources</li>
<li>Approach 1: require each thread to request and be allocated all its
resources before it begins execution</li>
<li>Approach 2: a thread can request resources only when it has none (it
must release all the resources that it is currently allocated before it
can request additional resources)</li>
<li>Disadvantages of both the approaches
<ul>
<li>Resource utilization may be low: resources may be allocated but
unused for a long period, for example, a long process with a a very
short critical session requesting a mutex lock</li>
<li>Starvation is possible, a thread that requests several popular
resources may have to wait indefinitely</li>
</ul></li>
</ul></li>
<li>No preemption
<ul>
<li>Approach 1: if a thread is holding some resources and request
another resources that cannot be immediately allocated to it, then all
resources the thread is currently holding are preempted</li>
<li>Approach 2: if a thread requests some resources, if there is
available resources, we allocate them to the requesting thread. If there
is no available resources, but there are some waiting threads holding
the desired resources, we preempt the waiting threads and allocate the
desired resources to the requesting thread. If both conditions do not
hold, we let the thread wait, and all its current resources can be
preempted</li>
</ul></li>
<li>Circular wait
<ul>
<li>The above approaches of breaking the deadlock conditions are
impractical, but there is a practical solution to break the circular
wait condition</li>
<li>Approach: impose a total ordering of resource types and require each
thread requests resources in an increasing order of enumeration</li>
<li>Implementation of this approach
<ul>
<li>Resource set <span class="math inline">\(R = \{R_1, ..., R_m
\}\)</span>, ordering function <span class="math inline">\(f: R
\rightarrow N\)</span></li>
<li>Requirement 1: initially each thread can request an instance of
resource, <span class="math inline">\(R_i\)</span>, after that, it can
request an instance of <span class="math inline">\(R_j\)</span> if and
only if <span class="math inline">\(f(R_j) &gt; f(R_i)\)</span></li>
<li>Requirement 2: if a thread requests an instance of resource <span class="math inline">\(R_i\)</span>, it must have released all resources
<span class="math inline">\(R_j\)</span> where <span class="math inline">\(f(R_j) &gt; f(R_i)\)</span></li>
</ul></li>
<li>Proof of correctness: suppose there is circular wait: <span class="math inline">\(T_i\)</span> is requesting <span class="math inline">\(R_i\)</span>, and <span class="math inline">\(R_i\)</span> is held by <span class="math inline">\(T_{i + 1}\)</span> (use modularity for indices:
<span class="math inline">\(T_n\)</span> is requesting <span class="math inline">\(R_0\)</span>, <span class="math inline">\(R_0\)</span> is held by <span class="math inline">\(T_0\)</span>). Since <span class="math inline">\(T_{i + 1}\)</span> holds <span class="math inline">\(R_i\)</span> but requests <span class="math inline">\(R_{i+1}\)</span>, we must have <span class="math inline">\(f(R_i) &lt; f(R_{i+1})\)</span>. Then <span class="math inline">\(f(R_0) &lt; ... &lt; f(R_n) &lt; f(R_0)\)</span>,
which is not possible. We conclude that there is no circular wait</li>
<li>Developing an ordering itself does not prevent deadlock, it's up to
developers to write programs that follow the ordering</li>
</ul></li>
</ul></li>
<li>Deadlock avoidance Ch 8.6
<ul>
<li>The simplest and most useful model requires that each thread declare
the maximum number of resources of each type that it may need</li>
<li>Resource allocation state: the number of available and allocated
resources and the maximum demands of the threads. A deadlock avoidance
algorithm dynamically examines the resource allocation state to ensure
that a circular wait condition can never exist</li>
<li>Safe state
<ul>
<li>A state is safe if the system can allocate resources to each thread
in some order (construct a safe sequence) and still avoid a
deadlock</li>
<li>In a safe state, there are no deadlocks. In an unsafe state, there
may be deadlocks</li>
<li>The avoidance algorithm is constructed so that the system is always
in the safe state</li>
</ul></li>
<li>Resource allocation graph algorithm
<ul>
<li>If there is only one instance of each resource type, then we can use
the resource allocation graph algorithm in deadlock avoidance</li>
<li>If granting resources to a thread results in a cycle in the resource
allocation graph, then the resources should no be allocated.</li>
<li>Checking whether there is a cycle in a graph takes <span class="math inline">\(O(n^2)\)</span> time, where n is the number of
threads in the system (use DFS to check cycle in a directed graph)</li>
</ul></li>
<li>Banker's algorithm
<ul>
<li>It can handle resources with multiple instances, although it's less
efficient than the resource allocation graph algorithm
<ul>
<li>Terminology</li>
<li>Available vector: a vector of length m,
<code>available[i] = k</code> indicates there are k instances of
resources available for type i</li>
<li>Max matrix: an n * m matrix defines the maximum demand of each
thread, <code>max[i][j] = k</code> indicates thread i may request at
most k instances of resource type j</li>
<li>Allocation matrix: an n * m matrix defines the number of resources
of each type currently allocated to each thread,
<code>allocation[i][j] = k</code> indicates thread i has k instances of
resource type j</li>
<li>Need matrix: an n * m matrix indicates the remaining resource need
of each thread, <code>need[i][j] = k</code> indicates thread i needs
additional k instances of resource type j. Notice
<code>need[i][j] = max[i][j] - allocation[i][j]</code></li>
</ul></li>
<li>Safety algorithm
<ol type="1">
<li>Let <code>work</code> and <code>finish</code> be vectors of length m
and n. Initialize <code>work = available</code>, and
<code>finish[i] = false</code> for all i</li>
<li>Find an index i such that <code>finish[i] = false</code> and
<code>need[i] &lt;= work</code>. If no such i exists, go to step 4</li>
<li><code>work = work + alloc[i]</code>, <code>finish[i] = true</code>.
Go to step 2</li>
<li>If <code>finish[i] = true</code> for all i, then the system is in a
safe state</li>
</ol></li>
<li>Resource request algorithm
<ol type="1">
<li>If <code>request[i] &lt;= need[i]</code>, go to step 2. Otherwise,
the thread has exceeded its maximum claim, raises an error</li>
<li>If <code>request[i] &lt;= available</code>, go to step 3. Otherwise,
<span class="math inline">\(T_i\)</span> must wait</li>
<li><code>available = available - request[i]</code>,
<code>alloc[i] = alloc[i] + request[i]</code>,
<code>need[i] = need[i] = request[i]</code></li>
</ol></li>
</ul></li>
</ul></li>
<li>Deadlock detection Ch 8.7
<ul>
<li>Single instance of each resource type
<ul>
<li>Use a wait-for graph, which is a variant of the resource allocation
graph, to detect deadlocks</li>
<li>An edge <span class="math inline">\(T_i \rightarrow T_j\)</span>
exists in the wait-for graph if and only if the corresponding resource
allocation graph contains two edges: <span class="math inline">\(T_i
\rightarrow R_q\)</span>, and <span class="math inline">\(R_q
\rightarrow T_j\)</span> for some <span class="math inline">\(R_q\)</span></li>
<li>The system needs to maintain the wait-for graph, and periodically
check whether there is a cycle in the wait-for graph, this is an <span class="math inline">\(O(n^2)\)</span> algorithm</li>
</ul></li>
<li>Several instances of a resource type
<ul>
<li>The detection algorithm is similar to the banker's algorithm</li>
<li>Terminology
<ul>
<li>Available: a vector of length m indicates the number of available
resources of each type</li>
<li>Allocation: an n * m matrix defines the number of resources of each
type currently allocated to each thread</li>
<li>Request: an n * m matrix indicates the current request of each
thread</li>
</ul></li>
<li>Detection algorithm
<ol type="1">
<li>Let <code>word</code> and <code>finish</code> be vectors of length m
and n. Initialize <code>work = available</code>, and
<code>finish[i] = false</code> if <code>allocation[i] != 0</code>,
otherwise <code>finish[i] = true</code></li>
<li>Find an index i such that <code>finish[i] = false</code> and
<code>request[i] &lt;= work</code>. If not such i exists, go to step
4</li>
<li><code>work = work + allocation[i]</code>,
<code>finish[i] = true</code>. Go to step 2</li>
<li>If <code>finish[i] = false</code> for some i, then the system is in
a deadlocked state. Moreover, if <code>finish[i] = false</code>, then
thread <span class="math inline">\(T_i\)</span> is deadlocked</li>
</ol></li>
</ul></li>
<li>Detection algorithm usage
<ul>
<li>If deadlock occurs frequently, then the detection algorithm should
be invoked frequently</li>
<li>Ideally, we can invoke a deadlock detection algorithm whenever the
request of a thread cannot be granted immediately, but such detection
will incur considerable overhead in computation time. A less expensive
alternative is simply to invoke the algorithm at defined intervals</li>
</ul></li>
</ul></li>
<li>Recovery from deadlock Ch 8.8
<ul>
<li>If a deadlock is detected, several alternatives are available:
<ul>
<li>Inform the operator that a deadlock has occurred and let the
operator to deal with the deadlock manually</li>
<li>Let the system recover from the deadlock automatically
<ul>
<li>Abort one or more threads to break the circular wait condition</li>
<li>Preempt some resources from one or more of the deadlocked
threads</li>
</ul></li>
</ul></li>
<li>Process and thread termination
<ul>
<li>Approach 1: abort all deadlocked processes, the system reclaims all
resources allocated to the terminated processes
<ul>
<li>This method is expensive</li>
<li>Aborting a process may not be easy (the process may be updating a
file, the process may be holding a mutex lock and updating a shared
data, etc.)</li>
</ul></li>
<li>Approach 2: abort one process at a time until the deadlock cycle is
eliminated, resources of the aborted processes are reclaimed by the
system
<ul>
<li>This method incurs considerable overhead, since a deadlock detection
algorithm should be invoked to determine whether any processes are still
deadlocked</li>
<li>We must determine which deadlocked process should be terminated, the
determination policy should be such that the termination incurs minimal
cost</li>
</ul></li>
</ul></li>
<li>Resource preemption
<ul>
<li>We successively preempt some resources from processes and give these
resources to other processes until the deadlock cycle is broken</li>
<li>Issues to be addressed
<ul>
<li>Select a victim: select which resources and which processes should
be preempted, minimal cost should be incurred</li>
<li>Rollback: if some needed resources are preempted from a process,
obviously it cannot proceed with its normal execution, we need to
rollback the process to some safe state and restart it from there. Since
it is hard to determine what a safe state is, we can simply abort the
process and restart it</li>
<li>Starvation: we must ensure the resources will not always be
preempted from the same process, which means a process can be picked as
a victim only a finite number of times to prevent starvation</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>Main memory
<ul>
<li>Background Ch 9.1
<ul>
<li>Main memory and the registers built into each processing core are
the only general purpose storage that the CPU can access directly</li>
<li>Basic hardware
<ul>
<li>We need to make sure that each process has a separate memory space.
Two registers are used to limit the range of the process memory
<ul>
<li>The basic register: holds the smallest legal physical memory
address</li>
<li>The limit register: holds the size of range</li>
<li>The process memory range is [basic, basic + limit] (both
inclusive)</li>
</ul></li>
<li>Protection of memory is accomplished by having the CPU hardware
compare every address generated in user mode with the registers. Any
attempt by a program executing in user mode to access OS memory or other
users' memory results in a trap to the OS, which treats the attempt as a
fatal error</li>
<li>Only the OS can use privileged instruction to load the basic and
limit registers</li>
</ul></li>
<li>Address binding
<ul>
<li>A program needs to be loaded into memory to be executed, the process
of converting symbolic addresses in the source program into relocatable
physical addresses is called address binding</li>
<li>A user program goes through several steps, some of which may be
optional, before being executed.
<ul>
<li>A compiler binds symbolic addresses in the source program to
relocatable addresses</li>
<li>The linker or loader binds the relocatable addresses to absolute
addresses</li>
</ul></li>
<li>The binding of instructions and data to memory addresses can be done
at any step
<ul>
<li>Compile time: if you know at compile time where the process will
reside in memory, then absolute code can be used. If, in later time, the
address changes, then it will be necessary to recompile the program</li>
<li>Load time: If it's not know at compile time where the process will
reside in memory, the the compiler must generate relocatable addresses.
Final binding is delayed until load time. If the address changes, we
need only reload the code</li>
<li>Execution time: if the process can be moved during execution from
one memory segment to another, then the binding must be delayed until
run time. Special hardware must be available for this scheme to work.
Most operating systems use this method</li>
</ul></li>
</ul></li>
<li>Logical and physical address space
<ul>
<li>An address generated by the CPU is commonly referred to as a logical
address, an address seen by the memory unit (the one that is loaded into
the memory address register of the memory), is commonly referred to as a
physical address</li>
<li>Binding at compile time or load time results in identical logical
and physical addresses. Binding at execution time results in differing
logical and physical addresses</li>
<li>Logical address space: the set of logical addresses. Physical
address space: the set of physical addresses</li>
<li>The user program can never access the real physical addresses, it
deals with logical addresses only. The mapping of logical address to
physical address is done by the memory mapping hardware, this is only
used in run time binding <img src="/images/Courses/COSI131A/memory_address.png"></li>
</ul></li>
<li>Dynamic loading
<ul>
<li>The entire program does not need to be loaded into the memory</li>
<li>To achieve better memory-space utilization, we can use dynamic
loading, a routine is not loaded until it is called. All routines are
kept on disk in a relocatable load format</li>
<li>The advantage of dynamic loading is that a routine is loaded only
when it is needed</li>
<li>Dynamic loading does not require special support from the OS</li>
</ul></li>
<li>Dynamic linking and shared libraries
<ul>
<li>Dynamically linked libraries(DLLs, aka shared libraries) are system
libraries that are linked to user programs when the programs are
run</li>
<li>Dynamic linking requires help from the OS</li>
</ul></li>
</ul></li>
<li>Contiguous memory allocation Ch 9.2
<ul>
<li>The main memory must accommodate both the operating system and the
various user processes. Many OSs (including Linux and Windows) place the
OS in high memory</li>
<li>In contiguous memory allocation, each process is contained in a
single section of memory that is contiguous to the section containing
the next process</li>
<li>Memory protection
<ul>
<li>Use base register (relocation register) and limit register to
specify the range of a process</li>
<li>During context switch, the correct values of the relocation register
and limit register are loaded into the CPU</li>
</ul></li>
<li>Memory allocation
<ul>
<li>A simple approach
<ul>
<li>Assign processes to variably sized partition in memory, each
partition contains one process, the OS keeps a table indicating which
parts of memory are available and which are occupied</li>
<li>If there is no available space, the OS can either provide an error
message, or put the process into a waiting queue</li>
</ul></li>
<li>Dynamic storage allocation problem
<ul>
<li>Problems: how to satisfy a request of size n from a list of free
holes</li>
<li>First-fit: allocate the first hold that is big enough, search can
start from either the beginning or the location where previous first-fit
search ended</li>
<li>Best-fit: allocate the smallest hold that is big enough, must search
the entire list</li>
<li>Worst-fit: allocate the largest hole, must search the entire
list</li>
<li>There is no clear advantage in terms of storage utilization, but
generally first fit is faster</li>
</ul></li>
</ul></li>
<li>Fragmentation
<ul>
<li>As processes are loaded and removed from memory, the free memory
space is broken into little pieces</li>
<li>External fragmentation
<ul>
<li>Definition: there is enough total memory space to satisfy a request
but the available space are not contiguous</li>
<li>Solution 1 to external fragmentation: compaction, so all free memory
is placed together in one large block</li>
<li>Solution 2 to external fragmentation: allow the logical address
space to be noncontiguous, thus allowing a process to be allocated
physical memory wherever such memory is available</li>
</ul></li>
<li>50-percent rule: given N allocated blocks, another 0.5N will be lost
to fragmentation, that is, one-third of memory may be unusable</li>
<li>Internal fragmentation
<ul>
<li>If there is a hole of size n, and a process request a hole of size m
(m &lt; n), the the cost to track n - m (especially when n - m &lt;&lt;
n) is very high</li>
<li>We break the physical memory into fixed-size blocks, and each block
is assigned to a process</li>
<li>Internal fragmentation: the difference between the size of the block
and the size of a process</li>
</ul></li>
</ul></li>
</ul></li>
<li>Paging Ch 9.3
<ul>
<li>Paging is a memory management scheme that permits a process's
physical address space to be noncontiguous</li>
<li>Paging avoids external fragmentation and is used in most OS</li>
<li>Basic method
<ul>
<li>Frames and pages
<ul>
<li>Frames are fixed-sized blocks in physical memory</li>
<li>Pages are fixed-sized blocks in logical memory, a page has the same
size as a frame</li>
</ul></li>
<li>During execution, the pages of a process are loaded into available
memory frames</li>
<li>Every address generated by the CPU is divided into two parts: a page
number (p) and a page offset (d) <img src="/images/Courses/COSI131A/paging_model.png"></li>
<li>Each process has a page table, which contains mapping from page
number to base address of each frame in physical memory</li>
<li>MMU address translation procedure
<ul>
<li>Extract the page number p and use it as an index into the page
table</li>
<li>Extract the corresponding frame number f from the page table</li>
<li>Replace the page number p in the logical address with the frame
number f</li>
<li>The offset d does not change, so it is not replaced</li>
</ul></li>
<li>With paging, we have no external fragmentation, but we can have
internal fragmentation</li>
<li>Size of a page
<ul>
<li>If process size is independent of page size, then on average the
size of internal fragmentation is one-half page size per process (For a
page of size n, waste can be in [0, n -1] with equal probability, then
on average is n /2)</li>
<li>Logically, smaller page size is desirable. But overhead is involved
in each page table entry, and this overhead is reduced as the size of
the pages increases. Also, disk I/O is more efficient when the amount of
data being transferred is larger</li>
<li>Typical page size is 4KB or 8KB, some OSs support larger pages</li>
</ul></li>
<li>Frame table: the OS need to maintain the information about which
frames are available and which frames are occupied, this information is
in the frame table. The frame table is a single, system-wide data
structure</li>
</ul></li>
<li>Hardware support
<ul>
<li>Each process has its own page table, a pointer to the page table is
stored in the PCB of each process. During context switch, the page table
should be loaded into CPU</li>
<li>Hardware implementation of page table
<ul>
<li>Simplest approach: page table is implemented as a set of high-speed
hardware registers, which makes the page-address translation very
efficient. This increases the context switch time</li>
<li>When the page table is large, it is kept in the main memory, and a
page-table base register(PTBR) points to the page table. Changing page
tables requires changing only this register during context switch
time</li>
<li>TLB
<ul>
<li>Storing page table in main memory can yield faster context switch
time, but it also results in slower memory access time (use p + PTBR to
find f, then use f + d to find the physical address). So we use TLB to
speed up</li>
<li>Translation look-aside buffer(TLB) is a special, small, fast-lookup
hardware cache. It contains a few page table entries</li>
<li>Use TLB with page table
<ul>
<li>When CPU generate the logical address, MMU searches in the TLB
first</li>
<li>If the page number is in the TLB, the frame number is immediately
available</li>
<li>If the page number is not in the TLB (called TLB miss), the MMU then
searches the page table. When search in the page table is done, the
entry is added to the TLB</li>
<li>If the TLB is full, an existing entry is replaced with the latest
one. Replacement policy can use LRU, round-robin, or random</li>
</ul></li>
<li>Some TLB stores ASIDs (address-space identifier) in each TLB entry.
An ASID can uniquely identifies each process and is used to provide
address-space protection for that process</li>
<li>Hit ratio: the percentage of times that the page number of interest
is found in the TLB</li>
<li>Effective access time = hit ratio * access time in TLB + (1 - hit
ration) * (access time in page table + access time in main memory)</li>
</ul></li>
</ul></li>
</ul></li>
<li>Protection
<ul>
<li>Memory protection in a paged environment is accomplished by
protection bits associated with each frame. Normally, theses bits are
kept in the page table</li>
<li>One additional bit is generally attached to each entry in the page
table: a valid-invalid bit. This bit is valid if the page is in the
process's logical address space</li>
</ul></li>
<li>Shared pages
<ul>
<li>When part of the code of a process is reentrant code, then it can be
shared</li>
<li>Shared code has the same physical address then thus the same
frame</li>
</ul></li>
</ul></li>
<li>Structure of the page table Ch 9.4
<ul>
<li>Most common techniques for structuring the page table: hierarchical
paging, hashed page tables, inverted page tables</li>
<li>Hierarchical paging
<ul>
<li>Page table is paged to become smaller ones</li>
<li>Two-level paging algorithm
<ul>
<li>Logical address = page number for outer page table <span class="math inline">\(p_1\)</span>, page number for inner page table
(also offset in the outer page table) <span class="math inline">\(p_2\)</span>, and page offset <span class="math inline">\(d\)</span></li>
<li>Because address translation works from the outer page table inward,
this scheme is aka a forward-mapped page table</li>
</ul></li>
<li>If the paged table is still too large, the table can be further
paged, so we can have a three-level page table...</li>
</ul></li>
<li>Hashed page table
<ul>
<li>If the address space is larger than 32 bits, then we can use hashed
page table.</li>
<li>Hash with chaining collision resolution is used, key is the hash
value of logical address, value is a linked list of objects. Each object
contains the logical page number and value of the mapped page
frame.</li>
<li>One variation of hashed page table is clustered page table, where
each object contains multiple physical page frames</li>
</ul></li>
<li>Inverted page tables
<ul>
<li>An inverted page table has one entry for each real page of memory.
Each entry consists of the virtual address of the page stored in that
real memory location, with information about the process that owns the
page.</li>
<li>Only one page table is needed in the system, and it has only one
entry for each page of physical memory</li>
<li>This approach decreases the amount of memory needed to store each
page table, it increases the amount of time needed to search the page
table when a page reference occurs</li>
<li>This approach also cannot use shared memory: in the standard page
table with shared memory, multiple logical addresses map to the same
physical address, but in the inverted page table, one physical address
cannot be mapped to multiple logical addresses</li>
</ul></li>
</ul></li>
<li>Swapping Ch 9.5
<ul>
<li>Process instructions and the data they operate on must be in memory
to be executed. However, a process, or a portion of a process, can be
swapped temporarily out of memory to a backing store and then brought
back into memory for continued execution</li>
<li>Swapping makes it possible for the total physical address space of
all processes to exceed the real physical memory of the system, thus
increasing the degree of multiprogramming in a system</li>
<li>Standard swapping
<ul>
<li>Move entire processes between main memory and a backing store, the
backing store is commonly fast secondary storage</li>
<li>It allows physical memory to be oversubscribed, so that the system
can accommodate more processes thant there is actual physical memory to
store them</li>
</ul></li>
<li>Swapping with paging
<ul>
<li>Standard swapping is generally no longer used in contemporary OSs,
because the amount of time required to move entire processes is
prohibitive</li>
<li>Most OSs use a variation of swapping in which pages of a process can
be swapped</li>
<li>Generally, swapping refers to standard swapping, paging refers to
swapping with paging</li>
<li>Page out moves a page from memory to the backing store, page in is
the reverse operation</li>
</ul></li>
<li>Swapping on mobile systems
<ul>
<li>Mobile systems typically do not support swapping in any form</li>
<li>Reasons for not support swapping
<ul>
<li>Mobile devices generally use flash memory rather than more spacious
hard disk for nonvolatile storage</li>
<li>Flash memory limits the number writes it can tolerate before it
becomes unreliable</li>
<li>The throughput between main memory and flash memory is also
limited</li>
</ul></li>
<li>When free memory falls below a certain threshold, IOS asks
applications to voluntarily relinquish allocated memory</li>
<li>Android may terminate a process if insufficient free memory is
available</li>
</ul></li>
</ul></li>
<li>Example: Intel 32- and 64-bit architecture
<ul>
<li>IA-32 architecture
<ul>
<li>Memory management in IA-32 systems is divided into two components:
segmentation and paging <img src="/images/Courses/COSI131A/ia_32_translation.png"></li>
<li>IA-32 segmentation
<ul>
<li>IA-32 allows a segment to be as large as 4 GB, and the maximum
number of segments per process is 16K</li>
<li>The logical address space of a process is divided into two
partitions
<ul>
<li>The first partition is up to 8K segments that are private to the
process. Information about the first partition is kept in the local
descriptor table (LDT)</li>
<li>The second partition is up to 8K segments that are shared among all
processes. Information about the second partition is kept in the global
descriptor table (GDT)</li>
<li>Each entry in the LDT and GDT contains an 8-byte segment descriptor
with detailed information about a particular segment, including the base
location and limit of that segment</li>
</ul></li>
<li>The logical address is a pair (selector, offset)
<ul>
<li>The selector is a 16-bit number, the first 13 bits designate the
segment number s, the 14th bit indicates whether the segment is in
LDT/GDT, and the last 2 bits deal with protection</li>
<li>The offset is a 32-bit number specifying the offset within the
segment</li>
</ul></li>
<li>The machine has 6 segment registers, allowing 6 segments to be
addressed at any one time by a process</li>
<li>The linear address is 32 bits long, it is the just the offset in the
logical address plus the base address of the segment (acquired from a
specific entry in LDT/GDT)</li>
</ul></li>
<li>IA-32 paging
<ul>
<li>The IA-32 allows a page size of either 4KB or 4MB</li>
<li>For 4KB pages, IA-32 uses a two-level paging scheme: for the 32-bit
linear address, outer page number uses 10 bits, inner page number uses
10 bits, offset uses 12 bits</li>
<li>For 4MB pages: for the 32-bit linear address, page number uses 10
bits, offset uses 22 bits</li>
<li>Intel adopted a page address extension (PAE), which allows 32-bit
processors to access a physical address space larger than 4GB. PAE uses
a three-level paging scheme instead of a two-level paging scheme</li>
</ul></li>
</ul></li>
<li>X86-64 architecture
<ul>
<li>Given the limitation of amount of bits in physical memory, the
x86-64 architecture currently uses a 48-bit logical address with support
for page sizes of 4KB, 2MB, or 1GB using four levels of paging
hierarchy</li>
</ul></li>
</ul></li>
</ul></li>
<li>Virtual memory
<ul>
<li>Background Ch 10.1
<ul>
<li>Virtual memory is a technique that allows the execution of processes
that are not completely in memory</li>
<li>Advantages
<ul>
<li>Programs can be larger than physical memory</li>
<li>Abstracts main memory into an extremely large, uniform array of
storage, frees programmers from the concerns of memory-storage
limitations</li>
<li>Allows processes to share files and libraries, and to implement
shared memory</li>
<li>Provides an efficient mechanism for process creation</li>
</ul></li>
<li>Disadvantages
<ul>
<li>Hard to implement</li>
<li>May decrease performance if it is used carelessly</li>
</ul></li>
</ul></li>
<li>Demand paging Ch 10.2
<ul>
<li>How an executable program might be loaded from secondary storage
into memory
<ul>
<li>Load the entire program in physical memory at program execution
time</li>
<li>Demand paging: load pages only as they are needed. Demand paging is
commonly used in virtual memory systems</li>
</ul></li>
<li>Basic concepts
<ul>
<li>Use valid-invalid bit scheme to distinguish between pages in memory
and pages in secondary storage
<ul>
<li>Valid pages: pages that are in memory and inside the range of
process address space</li>
<li>Invalid pages: pages that are not in memory, or pages that are not
inside the range of process address space</li>
</ul></li>
<li>Page fault: when a process tries to access a page that is not in
memory (an invalid page). The paging hardware will notice the invalid
bit and cause a trap to the OS.</li>
<li>Procedures to handle page fault
<ul>
<li>Check an internal table for the process to determine whether the
address is valid or not</li>
<li>If the reference was invalid, terminate the process. If the
reference was valid but the page was not in memory, we now page it
in</li>
<li>Find an available frame, read the secondary storage to load the page
into the newly allocated frame</li>
<li>When the read operation is done, modify the internal table to
indicate the page is not in memory</li>
<li>Restart the instruction that was interrupted by the page fault
trap</li>
</ul></li>
<li>The hardware to support demand paging is page table and secondary
memory</li>
</ul></li>
<li>Free frame list
<ul>
<li>When a page fault occurs, the operating system must bring the
desired page from secondary storage into main memory. To resolve page
faults, most operating systems maintain a free-frame list, a pool of
free frames for satisfying such requests</li>
<li>OS typically allocate free frames using a technique known as
zero-fill-on-demand: previous contents of a frame are eliminated before
being allocated</li>
</ul></li>
<li>Performance of demand paging
<ul>
<li>Demand paging can significantly affect the performance of a computer
system</li>
<li>Effective access time with demand paging: EAT = (1 - page fault
prob) * memory access time + p * page fault time</li>
<li>Typically, page fault time = time to service the page fault
interrupt + time to read the page into memory + time to restart the
process</li>
<li>Generally, EAT <span class="math inline">\(\propto\)</span> page
fault rate</li>
</ul></li>
</ul></li>
<li>Copy on write Ch 10.3
<ul>
<li>Process creation with <code>fork()</code> system call can be
achieved efficiently with copy-on-write</li>
<li>Traditionally, <code>fork()</code> copies the parent's address space
for the child, duplicating the page belonging to the parent</li>
<li>Copy-on-write: allow the parent and child process initially to share
the same pages, the shared pages are marked as copy-on-write pages, if
either process writes to a shared page, a copy of the shared page is
created, modification will be on the copied page</li>
<li>Several versions of UNIX provide a variation of the
<code>fork()</code> system call, <code>vfork()</code> (for virtual
memory fork), that uses copy-on-write</li>
</ul></li>
<li>Page replacement Ch 10.4
<ul>
<li>If a page fault occurs but there is not available free frame to load
the page, there are several choices
<ul>
<li>Choice 1: terminate the process</li>
<li>Choice 2: use standard swapping and swap out a process, freeze all
its frames and reduce the level of multiprogramming</li>
<li>Choice 3: page replacement</li>
</ul></li>
<li>Basic page replacement
<ul>
<li>Find a frame that is not currently being used and free it</li>
<li>How to free a frame
<ul>
<li>Write the contents of the frame to secondary storage</li>
<li>Modify the page table to indicate the page is no longer in
memory</li>
</ul></li>
<li>This approach involves two page transfers: page out one replaced
page and page in the target page</li>
<li>Use modify bit (dirty bit) to speed page replacement
<ul>
<li>If the page is written into at any time, the modify bit is set to
true</li>
<li>When we want to replace a page, we check the modify bit, if it is
true, we know it's modified since the last time it was read from
secondary storage, then we need to page it out to secondary storage. If
the modify bit is false, we do not need to page it out, because it's the
same as the one in secondary storage</li>
</ul></li>
<li>Page replacement is basic to demand paging, it completes the
separation between logical memory and physical memory</li>
<li>We must solve two major problems to implement demand paging
<ul>
<li>Develop a frame-allocation algorithm: if we have multiple processes
in memory, we must decide how many frames to allocate to each
process</li>
<li>Develop a page-replacement algorithm: when page replacement is
required, we must select the frames that are to be replaced</li>
</ul></li>
<li>In terms of page replacement algorithm, we want the one with the
lowest page-fault rate</li>
</ul></li>
<li>FIFO page replacement
<ul>
<li>A FIFO replacement algorithm associates with each page the time when
that page was brought into memory</li>
<li>When a page must be replaced, the oldest page is chosen</li>
<li>We don't have to record the timestamp with each page, we can simply
use a FIFO queue to hold the pages in memory</li>
<li>The FIFO page replacement algorithm is easy to understand and
program. But it's performance is not always good</li>
<li>Belady's anomaly: for some page replacement algorithms, the page
fault rate may increase as the number of allocated frames increases</li>
</ul></li>
<li>Optimal page replacement
<ul>
<li>Optimal page replacement algorithm is the one that has the lowest
page fault rate of all algorithms and will never suffer from Belaydy's
anomaly</li>
<li>The optimal algorithm does exist and has been called OPT or MIN,
it's simply this: replace the page that will not be used for the longest
period of time</li>
<li>The optimal page replacement algorithm is difficult to implement,
because it requires future knowledge of the reference string. So it is
used mainly for comparison studies</li>
</ul></li>
<li>LRU page replacement
<ul>
<li>We can sue the recent past as an approximation of the near future
then we can replace the page that has not been used for the longest
period of time. This approach is the least recently used (LRU)
algorithm</li>
<li>LRU replacement associates with each page the time of that page's
last use. We can think of this strategy as the optimal page replacement
algorithm looking backward in time, rather than forward</li>
<li>The LRU policy is often used as a page replacement algorithm and is
considered to be good</li>
<li>LRU implementation
<ul>
<li>Approach 1: each page table entry has a timestamp and there is a
clock/counter in the CPU. The clock is incremented each time a page is
referenced. When a page is referenced, the value of the clock is copied
to the timestamp field of its page table entry. When we want to replace
a page, we replace the one with the smallest timestamp</li>
<li>Approach 2: a hash map to contain pages that are in the memory and a
doubly linked list to keep track of the order of pages (use a doubly
linked list to mimic a stack, the LRU page is always at the bottom of
the stack)</li>
</ul></li>
<li>LRU page replacement does not suffer from Belady's anomaly, it
belongs to a class of page replacement algorithms, called stack
algorithms, that can never exhibit Belady's anomaly</li>
<li>Both implementations of LRU need hardware assistance</li>
</ul></li>
<li>LRU approximation page replacement
<ul>
<li>If the hardware does not provide sufficient support, then we cannot
use LRu page replacement. If there is a reference bit in each page table
entry, then we can use the LRU approximation page replacement
algorithm</li>
<li>Reference bit
<ul>
<li>Initially, all bits are set to 0 by the OS</li>
<li>When a page is referenced (read or write), the bit is set to 1</li>
<li>We do not know the order ot use</li>
</ul></li>
<li>Additional reference bits algorithm
<ul>
<li>Gain additional ordering information by recording the reference bits
at regular intervals</li>
<li>Keep 8 bits to represent the history of usage of the page</li>
<li>Each bit corresponds to one time interval, with the leftmost bit
indicating the most recent time interval and the right most bit
indicating the earliest time interval</li>
<li>Shift bits to one right position at the end of each time
interval</li>
<li>Clock ticks determine intervals</li>
<li>Convert the bits into unsigned integer, the page with the lowest
value is replaced</li>
</ul></li>
<li>Second chance algorithm/clock algorithm
<ul>
<li>The basic algorithm of second change replacement is a FIFO
replacement algorithm</li>
<li>Idea
<ul>
<li>Check the reference bit if a page is selected</li>
<li>If the reference bit is 0, replace this page</li>
<li>If the reference bit is 1, we give it a second chance: reset the
reference bit to 0 and move to the next FIFO page (this page will not be
replaced until all other pages are checked)</li>
</ul></li>
<li>Implementation: a circular queue</li>
</ul></li>
</ul></li>
<li>Counting based page replacement
<ul>
<li>Least frequently used (LFU) page replacement algorithm: the page
with the smallest count be replaced</li>
<li>The most frequently used(MFU) page replacement algorithm</li>
</ul></li>
<li>Page buffering algorithms
<ul>
<li>Used in addition to specific page replacement algorithm.</li>
<li>OS maintains a pool of free frames. When a page fault occurs, before
a page is written out, the desired page is read into a free frame in the
pool, so the process can restart as soon as possible, without waiting
for the victim page to be written out. When the victim is later written
out, its frame is added to the free frame pool</li>
</ul></li>
</ul></li>
</ul></li>
</ol>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Yanxuanshaozhu
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://yanxuanshaozhu.github.io/2022/01/22/COSI-131A-Operating-System/" title="COSI 131A Operating System">https://yanxuanshaozhu.github.io/2022/01/22/COSI-131A-Operating-System/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Operating-System/" rel="tag"># Operating System</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/01/21/COSI-127B-Database-Management-Systems/" rel="prev" title="COSI 127B Database Management Systems">
      <i class="fa fa-chevron-left"></i> COSI 127B Database Management Systems
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/01/22/COSI-21A-Data-Structures-and-Algorithms/" rel="next" title="COSI 21A Data Structures and Algorithms">
      COSI 21A Data Structures and Algorithms <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-01-20220118"><span class="nav-number">1.</span> <span class="nav-text">Lecture 01 2022&#x2F;01&#x2F;18</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-02-20220120"><span class="nav-number">2.</span> <span class="nav-text">Lecture 02 2022&#x2F;01&#x2F;20</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-03-20220125"><span class="nav-number">3.</span> <span class="nav-text">Lecture 03 2022&#x2F;01&#x2F;25</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-05-20220201"><span class="nav-number">4.</span> <span class="nav-text">Lecture 05 2022&#x2F;02&#x2F;01</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-06-20220204"><span class="nav-number">5.</span> <span class="nav-text">Lecture 06 2022&#x2F;02&#x2F;04</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-07-20220207"><span class="nav-number">6.</span> <span class="nav-text">Lecture 07 2022&#x2F;02&#x2F;07</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-08-20220210"><span class="nav-number">7.</span> <span class="nav-text">Lecture 08 2022&#x2F;02&#x2F;10</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-09-20220215"><span class="nav-number">8.</span> <span class="nav-text">Lecture 09 2022&#x2F;02&#x2F;15</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-10-20220217"><span class="nav-number">9.</span> <span class="nav-text">Lecture 10 2022&#x2F;02&#x2F;17</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-11-20220301"><span class="nav-number">10.</span> <span class="nav-text">Lecture 11 2022&#x2F;03&#x2F;01</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-12-20220304"><span class="nav-number">11.</span> <span class="nav-text">Lecture 12 2022&#x2F;03&#x2F;04</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-13-20220310"><span class="nav-number">12.</span> <span class="nav-text">Lecture 13 2022&#x2F;03&#x2F;10</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-14-20220315"><span class="nav-number">13.</span> <span class="nav-text">Lecture 14 2022&#x2F;03&#x2F;15</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-15-20220317"><span class="nav-number">14.</span> <span class="nav-text">Lecture 15 2022&#x2F;03&#x2F;17</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-16-20220322"><span class="nav-number">15.</span> <span class="nav-text">Lecture 16 2022&#x2F;03&#x2F;22</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-17-20220324"><span class="nav-number">16.</span> <span class="nav-text">Lecture 17 2022&#x2F;03&#x2F;24</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-18-20220329"><span class="nav-number">17.</span> <span class="nav-text">Lecture 18 2022&#x2F;03&#x2F;29</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-19-20220331"><span class="nav-number">18.</span> <span class="nav-text">Lecture 19 2022&#x2F;03&#x2F;31</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-20-20220405"><span class="nav-number">19.</span> <span class="nav-text">Lecture 20 2022&#x2F;04&#x2F;05</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-21-20220407"><span class="nav-number">20.</span> <span class="nav-text">Lecture 21 2022&#x2F;04&#x2F;07</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-22-20220414"><span class="nav-number">21.</span> <span class="nav-text">Lecture 22 2022&#x2F;04&#x2F;14</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#lecture-23-20220426"><span class="nav-number">22.</span> <span class="nav-text">Lecture 23 2022&#x2F;04&#x2F;26</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#book-contents"><span class="nav-number">23.</span> <span class="nav-text">Book contents</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yanxuanshaozhu"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Yanxuanshaozhu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">59</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yanxuanshaozhu" title="GitHub  https:&#x2F;&#x2F;github.com&#x2F;yanxuanshaozhu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:mrlixm.cn@gmail.com" title="E-Mail  mailto:mrlixm.cn@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yanxuanshaozhu</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">1.8m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">26:43</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
