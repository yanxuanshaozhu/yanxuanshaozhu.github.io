<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yanxuanshaozhu.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Course Name: Introduction to Machine Learning   Course Website: Coursera  Instructor: Professor Lawrence Carin, Professor David Carlson, Timothy Dunn, Kevin Liang">
<meta property="og:type" content="article">
<meta property="og:title" content="Introduction to Machine Learning Notes">
<meta property="og:url" content="https://yanxuanshaozhu.github.io/2021/04/19/Introduction%20to%20Machine%20Learning%20Notes/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:description" content="Course Name: Introduction to Machine Learning   Course Website: Coursera  Instructor: Professor Lawrence Carin, Professor David Carlson, Timothy Dunn, Kevin Liang">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/IntroML/SigmoidFunction.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/IntroML/LogisticRegression.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/IntroML/MultilayerPerceptron.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/IntroML/DeepLearning.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/IntroML/MultilayerPerceptronInterception.png">
<meta property="og:image" content="https://yanxuanshaozhu.github.io/images/IntroML/CNNLayers.png">
<meta property="article:published_time" content="2021-04-18T16:37:07.000Z">
<meta property="article:modified_time" content="2021-05-03T07:37:59.328Z">
<meta property="article:author" content="Yanxuanshaozhu">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yanxuanshaozhu.github.io/images/IntroML/SigmoidFunction.png">

<link rel="canonical" href="https://yanxuanshaozhu.github.io/2021/04/19/Introduction%20to%20Machine%20Learning%20Notes/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Introduction to Machine Learning Notes | My Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">My Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">17</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">17</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://yanxuanshaozhu.github.io/2021/04/19/Introduction%20to%20Machine%20Learning%20Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Yanxuanshaozhu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Introduction to Machine Learning Notes
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-19 00:37:07" itemprop="dateCreated datePublished" datetime="2021-04-19T00:37:07+08:00">2021-04-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-05-03 15:37:59" itemprop="dateModified" datetime="2021-05-03T15:37:59+08:00">2021-05-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Science/" itemprop="url" rel="index"><span itemprop="name">Computer Science</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>12k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>11 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p style="text-align:left; font-weight:bold;">
Course Name: Introduction to Machine Learning
</p>
<p><a style="text-align:left; font-weight:bold;" target="_blank" rel="noopener" href="https://www.coursera.org/learn/machine-learning-duke"> Course Website: Coursera</a></p>
<p style="text-align:left; font-weight:bold;">
Instructor: Professor Lawrence Carin, Professor David Carlson, Timothy Dunn, Kevin Liang
</p>
<a id="more"></a>
<h1 id="week-1">Week 1</h1>
<ol type="1">
<li>Logistic Regression
<ul>
<li>Why Machine Learning is Exciting
<ul>
<li>DL in the analysis of images: the ImageNet Challenge: evaluates algorithms for object detection and image classification at large scale</li>
<li>DL in games:solve a complex sequential problem</li>
</ul></li>
<li>What is Machine Learning
<ul>
<li>In ML we give the machine data, and teach them to build models and make predictions</li>
<li>Terminology: <code>x</code>: data/feature, <code>y</code>:outcome, training data</li>
</ul></li>
<li>Logistic Regression
<ul>
<li>Process: training set =&gt; mathematical model =&gt; learned parameters =&gt; make predictions on new data</li>
<li>Linear predictive model
<ul>
<li>Data: <span class="math inline">\(x_{i1}, x_{i2},..., x_{iM}\)</span>, <span class="math inline">\(y_1, y_2, ..., y_M\)</span></li>
<li>Model: <span class="math inline">\(z_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + ... + b_M x_{iM}\)</span>, the parameters tell how important the data variables are to the prediction</li>
<li>Sigmoid function: <span class="math inline">\(p(y_i = 1|x_i) = \sigma(z_i)\)</span>, the sigmoid function convert predictions to a probabilistic perspective <img src="/images/IntroML/SigmoidFunction.png"></li>
</ul></li>
</ul></li>
<li>Interpretation of Logistic Regression
<ul>
<li>Digit recognition problem on the MNIST Data Set: decompose the handwritten number figures into pixels and convert the color into a number for each pixel, then regard the training set as the set of numbers for each figure, run a logistic regression on the training set and use the result to distinguish which number is written</li>
</ul></li>
<li>Motivation for Multilayer Perceptron
<ul>
<li>Linear classifiers can only represent limited relationships, we often want to use a classifier thant can handle non-linearities</li>
</ul></li>
</ul></li>
<li>Multilayer Perceptron
<ul>
<li><p>Concepts</p>
<ul>
<li>Logistic regression: M features of data =&gt; a single filter =&gt; probability of a particular outcome <img src="/images/IntroML/LogisticRegression.png"></li>
<li>Multilayer perceptron: M features of data =&gt; K filters =&gt; probability of K latent processes/features =&gt; probability of a particular outcome <img src="/images/IntroML/MultilayerPerceptron.png"></li>
<li>The multilayer perceptron can be viewed as logistic regression on K latent features, rather than directly on the M components of raw data</li>
</ul></li>
<li><p>Math Model</p>
<p><span class="math inline">\(z_{i1} = b_{01} + b_{11} x_{i1} + b_{21} x_{i2} + ... + b_{M1} x_{iM}\)</span>, <span class="math inline">\(\; p(y_i = 1|x_i, b_1) = \sigma(z_{i1})\)</span></p>
<p><span class="math inline">\(z_{i2} = b_{02} + b_{12} x_{i1} + b_{22} x_{i2} + ... + b_{M2} x_{iM}\)</span>, <span class="math inline">\(\; p(y_i = 1|x_i, b_2) = \sigma(z_{i2})\)</span></p>
<p><span class="math inline">\(\qquad \qquad \qquad \qquad \qquad \vdots\)</span></p>
<p><span class="math inline">\(z_{iK} = b_{0K} + b_{1K} x_{i1} + b_{2K} x_{i2} + ... + b_{MK} x_{iM}\)</span>, <span class="math inline">\(\; p(y_i = 1|x_i, b_K) = \sigma(z_{iK})\)</span></p>
<p><span class="math inline">\(\zeta_is = c_0 + c_1 \sigma(z_{i1}) + c_2 \sigma(z_{i2}) + ... + c_K \sigma(z_{iK})\)</span></p></li>
<li><p>Deep Learning <img src="/images/IntroML/DeepLearning.png"></p>
<ul>
<li>Deep learning is a form of machine learning where a model has multiple layers of latent processes</li>
</ul></li>
<li><p>Multilayer Perceptron: Neural Network <img src="/images/IntroML/MultilayerPerceptronInterception.png"></p></li>
<li><p>Transfer Learning</p>
<ul>
<li>Considering multiple likes and dislikes
<ul>
<li>The first-two layers look for topics and meta-topics, and thus can be used in models of multiple people, parameters "transferred" across all data, documents, and people</li>
<li>The top layer characterizes specific people, parameters are different for each people</li>
</ul></li>
</ul></li>
<li><p>Model Selection</p>
<ul>
<li>Bias-Variance trade-off
<ul>
<li>Variance: the more complex the model is, the bigger variance it has. The variation of outputs for different inputs of a model is the variance of the model</li>
<li>Bias: the simpler the model is, the more biased it is</li>
<li>Logistic regression: bigger bias, smaller variance</li>
<li>Multilayer perceptron: smaller bias, bigger variance</li>
</ul></li>
<li>Logistic regression results in a linear classifier, while multilayer perceptron results in a non-linear classifier</li>
</ul></li>
<li><p>History of Neural Networks</p>
<ul>
<li>Seasons of Neural Networks:
<ul>
<li>1960 Multilayer Perceptron(MLP) 多层感知机</li>
<li>1986 Back Propagation 反向传播(BP算法)</li>
<li>1989 Convolutional Neural Network(CNN) 卷积神经网络</li>
<li>1990 - 1994 Neural Nets in the Wild: insufficient training data</li>
<li>1995 Long Short-Term Memory(LSTM) 长短期记忆网络</li>
<li>1998 - 2005 More Neural Nets in the Wild: not good performance</li>
<li>2005 - 2010 Banishment: no neural network at all because of bad performance</li>
<li>2010 Rename: Deep Learning</li>
<li>2013 CNN + GPU(parallel computation) + ImageNet(image dataset)</li>
<li>2015 AlphaGo: based on CNN and Reinforcement Learning</li>
</ul></li>
<li>Occam's razor: all things being equal, the simplest solution tends to be the best one</li>
</ul></li>
</ul></li>
<li>Convolutional Neural Networks
<ul>
<li><p>Hierarchical Structure of Images</p>
<ul>
<li>Picture(most complex) =&gt; high-level motifs =&gt; repeated sub-structures called sub-motifs =&gt; atomic elements(simplest) <img src="/images/IntroML/CNNLayers.png"></li>
<li>Structures: edges, corners, textures, shapes, ...</li>
</ul></li>
<li><p>Convolution Filters</p>
<ul>
<li>Layer 1 Convolution: shift an atomic element to every possible location in an image, check the correlation between the atomic element and the local region, the process is the called the convolution, the correlations make a feature map. We do so for each atomic element to construct multiple feature maps</li>
<li>Layer 2 Convolution: shift combination of atomic elements to every possible location in the feature maps. Construct layer 2 feature maps in such a way.</li>
<li>Layer 3 Convolution: shift sub-motifs with layer 2 feature maps to construct layer 3 feature maps</li>
</ul></li>
<li><p>Convolutional Neural Network</p>
<ul>
<li>CNN classifier is based on layer 3 feature maps</li>
</ul></li>
<li><p>CNN Math Model</p>
<ul>
<li><p>Layer 1:</p>
<p><span class="math inline">\(\quad M_n = f(I_n;\phi_1, ..., \phi_K)\)</span> where <span class="math inline">\(I_n\)</span> represents the ith image, and <span class="math inline">\(\phi_1,..., \phi_K\)</span> represents layer 1 filters</p></li>
<li><p>Layer 2:</p>
<p><span class="math inline">\(\quad L_n = f(M_n;\Psi_1, ..., \Psi_K)\)</span> where <span class="math inline">\(M_n\)</span> represents the ith layer 1 feature map, and <span class="math inline">\(\Psi_1,..., \Psi_K\)</span> represents layer 2 filters</p></li>
<li><p>Layer 3:</p>
<p><span class="math inline">\(\quad G_n = f(L_n;\omega_1, ..., \omega_K)\)</span> where <span class="math inline">\(L_n\)</span> represents the ith layer 2 feature map, and <span class="math inline">\(\omega_1,..., \omega_K\)</span> represents layer 3 filters</p></li>
<li><p>Classifier:</p>
<p><span class="math inline">\(\quad l_n = l(G_n;W)\)</span></p></li>
</ul></li>
<li><p>How the Model Learns</p>
<ul>
<li>We have labeled data <span class="math inline">\(\{I_n,\; y_n\}\)</span>, where <span class="math inline">\(y_n \in \{+1,\; -1\}\)</span></li>
<li>Risk function of model parameters <span class="math inline">\(E(\Phi,\; \Psi,\; \Omega,\; W) = 1/N \sum {loss(y_n,\;l_n)}\)</span></li>
<li>Find model parameters <span class="math inline">\(\hat{\Phi},\;\hat{\Psi},\;\hat{\Omega},\;\hat{W}\)</span> that minimize <span class="math inline">\(E(\Phi,\;\Psi,\;\Omega,\;W)\)</span></li>
</ul></li>
<li><p>Advantages of Hierarchical Model</p>
<ul>
<li>Top level motifs would be learned independently if we do not use hierarchy</li>
<li>Sharing similarities allows more effective data use</li>
<li>Facts
<ul>
<li>Learning is manifested by taking large quantities of labeled data</li>
<li>Learning is the concept of estimating the model parameters so the predictions are consistent with true labels</li>
</ul></li>
</ul></li>
</ul></li>
<li>Applications in the Real World
<ul>
<li>CNN on Real Images</li>
<li>Application in Use and Practice
<ul>
<li>Image Processing</li>
<li>DL in Games</li>
<li>Digit Recognition</li>
</ul></li>
<li>Deep Learning and Transfer Learning
<ul>
<li>Image analysis in radiology, ophthalmology, dermatology in medicine industry</li>
<li>DL can access massive quantities of labeled data, but this not possible or way too expensive in medicine industry, so we need transfer learning here</li>
<li>And we can sometimes even transfer parameters from models implemented in vast different areas</li>
</ul></li>
</ul></li>
<li>PyTorch Basics
<ul>
<li>Conda commands
<ul>
<li><code>conda clean</code></li>
<li><code>conda config</code></li>
<li><code>conda create</code></li>
<li><code>conda info</code></li>
<li><code>conda install</code></li>
<li><code>conda list</code></li>
<li><code>conda remove</code></li>
<li><code>conda update</code></li>
<li><code>conda search</code></li>
</ul></li>
<li>For users from mainland China, you can add tsinghua channel to speed up</li>
<li>Installation
<ul>
<li>Install some supporting dependencies: <code>conda install h5py imageio jupyter matplotlib numpy tqdm</code></li>
<li>Install PyTorch: <code>conda install pytorch torchvision cpuonly -c pytorch</code></li>
</ul></li>
<li>Advantages of using PyTorch
<ul>
<li>Library functions</li>
<li>Computational efficiency + GPU support</li>
<li>Auto-differentiation</li>
<li>Online community</li>
</ul></li>
</ul></li>
</ol>
<h1 id="week-2">Week 2</h1>
<ol type="1">
<li>Logistic Regression as Running Example
<ul>
<li>How Do We Define Learning?
<ul>
<li>Define what is performance. Given data, find the best parameters that give us the best performance with least resources used</li>
<li>Empirical risk minimization
<ul>
<li><p>A loss function defines a penalty for poor predictions</p></li>
<li><p>Want to minimize average loss: <span class="math inline">\(b^{*} = \underset{b}{\operatorname{argmin}} \frac{1}{N}\sum\limits_{i = 1}^{N} l(y_{i},\sigma(z_{i}))\)</span></p>
<p>where <span class="math inline">\(b^{*}\)</span> is the optimal parameters, <span class="math inline">\(\sigma(z_{i})\)</span> is the predicted probability, <span class="math inline">\(y_i\)</span> is the true label, we can use the negative log-likelihood function as the loss function, <span class="math inline">\(l(y_i, \sigma(z_i)) = -log(p(y_i|\sigma(z_i)))\)</span>.</p>
<p>In binary classification problem, we have</p>
<p><span class="math display">\[l(y,\sigma(z)) = -ylog(\sigma(z)) - (1 - y)log(1 - \sigma(z))\]</span></p></li>
</ul></li>
</ul></li>
<li>How Do We Evaluate Our Networks?
<ul>
<li>We can use DL to calculate complex relationships, but models need to be validated</li>
<li>Overfitting: when the learned model increases complexity to fit the observed training data too well but not predicts well in real world
<ul>
<li>Increase parameters, increase error rate</li>
<li>The learned relationship is too complex for reality, so models and analyses are not generalized</li>
</ul></li>
<li>Validating process
<ul>
<li>Ideal way: collecting new real-world data is useful, but it costs way too much</li>
<li>We can split existing data into separate groups, training data set, validation data set and testing data set.
<ul>
<li>Test set: never used to learn or fit parameter, can evaluate performance ot network, is should be used only once, reusing a test set will lead to bias</li>
<li>Validation set: used to compare which approach is best, not used to learn parameters, use repeatedly to estimate the performance, can be used to pick the best performance model</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>Learning via Gradient Descent
<ul>
<li>How Do We Learn Our Network?
<ul>
<li>Minimize <span class="math inline">\(b^* = agrmin f(b)\)</span> using gradient descent
<ul>
<li>Start with initial value <span class="math inline">\(b^0\)</span></li>
<li>Run series of updates to move from <span class="math inline">\(b^k\)</span> to <span class="math inline">\(b^{k + 1}\)</span>, <span class="math inline">\(b^{k + 1} = b^{k} - a^{k} \nabla f(b^{k})\)</span>, where <span class="math inline">\(a^{k}\)</span> is the step size</li>
<li>Repeat step 2 and step 3 until solution is good enough</li>
</ul></li>
</ul></li>
<li>How Do We Handle Big Data?
<ul>
<li><p>Calculate the gradient requires looking at every single data point, which is unbearable</p>
<p><span class="math display">\[\nabla\frac{1}{N}\sum\limits_{i = 1}^{N} l(y_{i},\sigma(z_{i})) = \frac{1}{N}\sum\limits_{i = 1}^{N} \nabla l(y_{i},\sigma(z_{i}))\]</span></p></li>
<li><p>We use approximations to improve calculation speed vastly <span class="math display">\[\nabla l(y_{j},\sigma(z_{j})) \approx \frac{1}{N}\sum\limits_{i = 1}^{N} \nabla l(y_{i},\sigma(z_{i}))\]</span></p>
<p>where j is a randomly picked point, this is called stochastic gradient descent. This can work because there is often redundant data</p></li>
<li><p>Minimize <span class="math inline">\(b^* = agrmin f(b)\)</span> using stochastic gradient descent</p>
<ul>
<li>Start with initial value <span class="math inline">\(b^0\)</span></li>
<li>Choose a random data entry j</li>
<li>Estimate gradient <span class="math inline">\(\widehat{\nabla f}(b^{k})\)</span> by data point j</li>
<li>Iteratively update: <span class="math inline">\(b^{k + 1} = b^{k} - a^{k} \widehat{\nabla f}(b^{k})\)</span></li>
<li>Repeat step 2 to step 4 until solution is good enough</li>
</ul></li>
<li><p>In practice, we're often going to use a mini-batch. Which means that instead of using a single data example, we're going to run a few data examples to estimate the gradient and this will reduce variance.</p></li>
</ul></li>
<li>Early Stopping
<ul>
<li>Maximizing generalization of network is mismatched with our optimization goal, because the goal of optimization is to do as well as possible on our training set. Taken overfitting into account, it may be better to stop earlier.</li>
<li>Early stopping:
<ul>
<li>Can check validation loss as we go</li>
<li>Instead of optimizing to convergence, optimize until validation loss stops improving(during the optimization loop, check the validation loss, stop when loss stops improving)</li>
<li>Helps save computational cost</li>
<li>Will perform better in the real world</li>
</ul></li>
</ul></li>
</ul></li>
<li>Model Learning with PyTorch
<ul>
<li>Logistic Regression
<ul>
<li>MNIST Dataset
<ul>
<li>Prepare the data using <code>torchvision</code> package <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line">mnist_train = datasets.MNIST(root=<span class="string">&quot;./datasets&quot;</span>, train=<span class="literal">True</span>, transform=transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line">mnist_test = datasets.MNIST(root=<span class="string">&quot;./datasets&quot;</span>, train=<span class="literal">False</span>, transform=transforms.ToTensor(), download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li>
<li>Use a DataLoader to take care of shuffling and batching instead of working directly with the dataset <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=<span class="number">100</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=<span class="number">100</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Logistic Regression Model
<ul>
<li><p>The model <span class="math inline">\(Y = XW + b\)</span>, where <span class="math inline">\(Y \in \boldsymbol{R}^{m*c}\)</span>, <span class="math inline">\(X \in \boldsymbol{R}^{m*d}\)</span>, <span class="math inline">\(W \in \boldsymbol{R}^{d*c}\)</span>, <span class="math inline">\(b \in \boldsymbol{R}^{c}\)</span></p></li>
<li><p>Initialization <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = images.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights W</span></span><br><span class="line">W = torch.randn(<span class="number">784</span>, <span class="number">10</span>)/np.sqrt(<span class="number">784</span>)</span><br><span class="line">W.requires_grad_()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize bias b as 0s</span></span><br><span class="line">b = torch.zeros(<span class="number">10</span>, requires_grad=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Linear transformation with W and b</span></span><br><span class="line">y = torch.matmul(x, W) + b</span><br></pre></td></tr></table></figure></p></li>
<li><p>Calculate probabilities <span class="math inline">\(p(y_{i}) = \frac{exp(y_{i})}{\sum_{j} exp(y_{j})}\)</span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Option 1: Softmax to probabilities from equation</span></span><br><span class="line">py_eq = torch.exp(y) / torch.<span class="built_in">sum</span>(torch.exp(y), dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Option 2: Softmax to probabilities with torch.nn.functional</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">py = F.softmax(y, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p></li>
<li><p>Cross-Entropy Loss: <span class="math inline">\(H_{y^{&#39;}}(y) = \sum_{i}y^{&#39;}_{i}log(y_{i})\)</span>, where <span class="math inline">\(y_{i}\)</span> is the model predicted value, and <span class="math inline">\(y^{&#39;}_{i}\)</span> is the true label <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Option 1: Cross-entropy loss from equation</span></span><br><span class="line">cross_entropy_eq = torch.mean(-torch.log(py_eq)[<span class="built_in">range</span>(labels.shape[<span class="number">0</span>]),labels])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Option 2: cross-entropy loss with torch.nn.functional</span></span><br><span class="line">cross_entropy = F.cross_entropy(y, labels)</span><br></pre></td></tr></table></figure></p></li>
<li><p>The Backward Pass: update the model by changing the parameters in order to minimize the loss function</p>
<p><span class="math display">\[\theta_{t + 1} = \theta_{t} - \alpha \nabla_{\theta}   \mathcal{L} \]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the parameter(here is W and b), <span class="math inline">\(\alpha\)</span> is the learning rate(step size), and <span class="math inline">\(\nabla_{\theta}\mathcal{L}\)</span> is the gradient of our loss with respect to <span class="math inline">\(\theta\)</span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Optimizer</span></span><br><span class="line">optimizer = torch.optim.SGD([W,b], lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></p></li>
<li><p>Model Training</p>
<p>To train the model, we just need repeat what we just did for more minibatches from the training set. As a recap, the steps were:</p>
<ol type="1">
<li>Draw a minibatch</li>
<li>Zero the gradients in the buffers for W and b</li>
<li>Perform the forward pass (compute prediction, calculate loss)</li>
<li>Perform the backward pass (compute gradients, perform SGD step) <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Iterate through train set minibatchs </span></span><br><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> tqdm(train_loader):</span><br><span class="line">    <span class="comment"># Zero out the gradients</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Forward pass</span></span><br><span class="line">    x = images.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">    y = torch.matmul(x, W) + b</span><br><span class="line">    cross_entropy = F.cross_entropy(y, labels)</span><br><span class="line">    <span class="comment"># Backward pass</span></span><br><span class="line">    cross_entropy.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure></li>
</ol></li>
<li><p>Testing <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Testing</span></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="built_in">len</span>(mnist_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># Iterate through test set minibatchs </span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> tqdm(test_loader):</span><br><span class="line">        <span class="comment"># Forward pass</span></span><br><span class="line">        x = images.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        y = torch.matmul(x, W) + b</span><br><span class="line">        </span><br><span class="line">        predictions = torch.argmax(y, dim=<span class="number">1</span>)</span><br><span class="line">        correct += torch.<span class="built_in">sum</span>((predictions == labels).<span class="built_in">float</span>())</span><br><span class="line">    </span><br><span class="line">print(<span class="string">&#x27;Test accuracy: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(correct/total))</span><br></pre></td></tr></table></figure></p></li>
</ul></li>
</ul></li>
</ul></li>
</ol>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Yanxuanshaozhu
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://yanxuanshaozhu.github.io/2021/04/19/Introduction%20to%20Machine%20Learning%20Notes/" title="Introduction to Machine Learning Notes">https://yanxuanshaozhu.github.io/2021/04/19/Introduction to Machine Learning Notes/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/04/16/Princeton%20Algorithms%20Part%20One%20Notes/" rel="prev" title="Princeton Algorithms Part One Notes">
      <i class="fa fa-chevron-left"></i> Princeton Algorithms Part One Notes
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/05/03/Some-LaTex/" rel="next" title="Some LaTex">
      Some LaTex <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#week-1"><span class="nav-number">1.</span> <span class="nav-text">Week 1</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#week-2"><span class="nav-number">2.</span> <span class="nav-text">Week 2</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yanxuanshaozhu"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Yanxuanshaozhu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yanxuanshaozhu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yanxuanshaozhu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:mrlixm.cn@gmail.com" title="E-Mail → mailto:mrlixm.cn@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yanxuanshaozhu</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">210k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">3:11</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
