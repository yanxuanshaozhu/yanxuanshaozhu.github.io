<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yanxuanshaozhu.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Chapter 1 开发环境配置">
<meta property="og:type" content="article">
<meta property="og:title" content="Learn Python Web Scraper">
<meta property="og:url" content="https://yanxuanshaozhu.github.io/2021/07/05/Learn%20Python%20Web%20Scraper/index.html">
<meta property="og:site_name" content="My Blog">
<meta property="og:description" content="Chapter 1 开发环境配置">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-07-05T09:59:20.000Z">
<meta property="article:modified_time" content="2021-07-12T14:20:59.662Z">
<meta property="article:author" content="Yanxuanshaozhu">
<meta property="article:tag" content="Web Scraper">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://yanxuanshaozhu.github.io/2021/07/05/Learn%20Python%20Web%20Scraper/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Learn Python Web Scraper | My Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">My Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">20</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">20</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://yanxuanshaozhu.github.io/2021/07/05/Learn%20Python%20Web%20Scraper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Yanxuanshaozhu">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Learn Python Web Scraper
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-05 17:59:20" itemprop="dateCreated datePublished" datetime="2021-07-05T17:59:20+08:00">2021-07-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-07-12 22:20:59" itemprop="dateModified" datetime="2021-07-12T22:20:59+08:00">2021-07-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Science/" itemprop="url" rel="index"><span itemprop="name">Computer Science</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>20k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>18 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="chapter-1-开发环境配置">Chapter 1 开发环境配置</h1>
<a id="more"></a>
<h2 id="安装python">1 安装Python</h2>
<p>​ 略</p>
<h2 id="安装请求库">2 安装请求库</h2>
<ol type="1">
<li>爬虫简单分成三步：抓取页面，分析页面，存储数据</li>
<li>抓取页面的时候，需要模拟浏览器向服务器发出请求，需要用到一些Python库来实现HTTP请求的操作</li>
<li><code>request</code>库
<ol type="1">
<li><code>pip install requests</code></li>
</ol></li>
<li><code>Selenium</code>库
<ol type="1">
<li><code>pip install selenium</code></li>
<li>Selenium 是一个自动化测试工具，利用它可以驱动浏览器执行特定的操作。对于一些JavaScript渲染的页面来说，这种方法十分有效</li>
</ol></li>
<li><code>ChromeDriver</code>
<ol type="1">
<li>使用ChromeDriver以及Chrome配合Selenium进行网页抓取</li>
<li>ChromeDriver版本要与Chrome版本匹配</li>
<li>ChromeDriver要配置环境变量</li>
</ol></li>
<li><code>aiohttp</code>
<ol type="1">
<li><code>pip install aiohttp</code></li>
<li>requests库是阻塞式HTTP请求库，aiohttp是一个提供了异步Web服务的库</li>
</ol></li>
</ol>
<h2 id="安装解析库">3 安装解析库</h2>
<ol type="1">
<li><code>lxml</code>库
<ol type="1">
<li><code>pip install lxml</code></li>
<li>lxml是一个Python解析库，支持HTML和XMl的解析，支持XPath解析方式，并且解析效率很高</li>
</ol></li>
<li><code>Beautiful Soup</code>库
<ol type="1">
<li><code>pip install beautifulsoup4</code></li>
<li>beautifulsoup4是Python的一个HTML和XMl的解析库，可以用它来方便的从网页中提取数据，它有强大的API和多样的解析方式</li>
</ol></li>
<li><code>pyquery</code>库
<ol type="1">
<li><code>pip install pyquery</code></li>
<li>提供了和jQuery类似的语法来解析HTML文档，支持CSS选择器</li>
</ol></li>
<li><code>tesserocr</code>
<ol type="1">
<li>tesserocr是Python的一个OCR识别库，可以用来解决验证码问题</li>
</ol></li>
</ol>
<h2 id="安装数据库">4 安装数据库</h2>
<ol type="1">
<li>关系型：MySQL</li>
<li>非关系型：MongoDB(存储类似JSON对象)，Redis</li>
</ol>
<h2 id="安装存储库">5 安装存储库</h2>
<ol type="1">
<li>Python利用存储库来进行和数据库的交互</li>
<li>Python：PyMySQL</li>
<li>MongoDB：PyMongo</li>
<li>Redis:：redis-py</li>
</ol>
<h2 id="安装web库">6 安装Web库</h2>
<ol type="1">
<li>通过Web服务提供一个API接口，比如可以把代理放在Redis中，向Web服务的代理池API请求获取代理</li>
<li><code>Flask</code>
<ol type="1">
<li><code>pip install flask</code></li>
</ol></li>
<li><code>Tornado</code>
<ol type="1">
<li><code>pip install tornado</code></li>
<li>支持异步的I/O框架</li>
</ol></li>
</ol>
<h2 id="安装app爬取库">7 安装APP爬取库</h2>
<ol type="1">
<li>APP中页面加载出来需要通过请求服务器接口获取数据。在爬虫的时候，一般通过抓包技术来获取数据</li>
<li>APP爬虫时，简单接口可以使用Charles或者mitmproxy，复杂接口需要使用mitmdump对抓到的请求和响应进行实时保存和处理，规模化采集自动化工具是Appium(类似Selenium)</li>
<li><code>Charles</code>
<ol type="1">
<li>对于HTTPS抓包，需要在PC和手机上配之SSL证书</li>
<li>需要手动设置监听的端口和地址，监听所有HTTPS的话配置为：<code>"*"</code></li>
</ol></li>
<li><code>mitmproxy</code>
<ol type="1">
<li><code>pip install mitmproxy</code></li>
<li>mitmproxy支持HTTP和HTTPS抓包，它有两个关联组件。其中mitmdump是命令行接口，可以对接Python脚本，实现监听后的处理，mitmweb是一个Web程序，可以观察到mitmproxy捕获的请求</li>
<li>对于HTTPS请求，需要设置CA证书</li>
</ol></li>
<li><code>APPium</code>
<ol type="1">
<li>直接通过GitHub安装即可</li>
</ol></li>
</ol>
<h2 id="安装爬虫框架">8 安装爬虫框架</h2>
<ol type="1">
<li>规模化爬虫框架可以让使用者不用关心功能实现，只用你关心爬虫逻辑</li>
<li><code>pyspider</code>
<ol type="1">
<li><code>pip install pyspider</code></li>
<li>功能强大，支持JavaScript渲染页面的爬取(需要PhantomJS)</li>
</ol></li>
<li><code>Scrapy</code>
<ol type="1">
<li><code>conda install Scrapy</code></li>
<li>功能强大，依赖库多(如<code>lxml</code>, <code>Twisted</code>, <code>pyOpenSSL</code>)</li>
<li><code>Scrapy-Splash</code>是支持JavaScript渲染的工具</li>
<li><code>Scrapy-Redis</code>是分布式扩展模块，可以方便的搭建分布式爬虫</li>
</ol></li>
</ol>
<h2 id="安装部署相关库">9 安装部署相关库</h2>
<ol type="1">
<li><code>Docker</code>
<ol type="1">
<li>可以讲爬虫制作为Docker镜像，只要主机安装了Docker，就可以直接运行爬虫，无需担心环境配置和版本问题</li>
<li>Docker是一中容器技术，可以将应用和运行环境打包，形成一个独立的应用。这个独立应用可以直接被分发到任何一个支持Docker的环境中，通过简单的命令就可以启动运行。Docker和虚拟化技术相比更加轻量级，资源管理的粒度更细。</li>
<li>安装Docker
<ul>
<li>Windows使用<code>Docker for Windows</code></li>
<li>Linux使用官方一键安装脚本</li>
<li>Mac使用<code>Docker for Mac</code></li>
</ul></li>
</ol></li>
<li><code>Scrapyd</code>
<ol type="1">
<li><code>pip install scrapyd</code></li>
<li>Scrapyd是一个用于部署Scrapy项目的工具，通过它可以将项目上传到云主机，并通过API来控制它的运行</li>
<li>将代码打包成EGG文件，并上传到云主机的过程可以手动实现，也可以使用Scrapyd-Client实现
<ol type="1">
<li><code>pip install scrapyd-client</code></li>
<li>通过<code>scrapyd-deploy</code>来完成项目部署</li>
</ol></li>
<li>通过Scrapy API来查看当前主机的Scrapy项目
<ol type="1">
<li><code>pip install python-scrapyd-api</code></li>
<li>使用 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapyd_api <span class="keyword">import</span> ScrapydAPI</span><br><span class="line">scrapyd = ScrapydAPI(<span class="string">&#x27;http://localhost:6800&#x27;</span>)</span><br><span class="line">print(scrapyd.list_projects())</span><br></pre></td></tr></table></figure></li>
</ol></li>
<li>通过Scrapyrt的HTTP接口而不是Scrapy命令来进行任务调度
<ol type="1">
<li><code>pip install scrapyrt</code></li>
<li>启动: <code>scrapyrt</code>，默认端口号9080</li>
<li>以指定端口启动: <code>scrapyrt -p xxxxx</code></li>
</ol></li>
</ol></li>
</ol>
<h1 id="chapter-2-爬虫基础">Chapter 2 爬虫基础</h1>
<h2 id="http基本原理">1 HTTP基本原理</h2>
<ol type="1">
<li>URI和URL
<ol type="1">
<li>URI(uniform resource identifier)
<ul>
<li>URL(uniform resource locator): 可以唯一确定一个网络资源</li>
<li>URN(uniform resource name)</li>
</ul></li>
</ol></li>
<li>超文本
<ul>
<li>我们看到的网页内容是由HTML代码解析而成的，HTML代码是一种超文本</li>
</ul></li>
<li>HTTP和HTTPS
<ol type="1">
<li>HTTP(Hyper Text Transfer Protocol)
<ul>
<li>从网络传输超文本数据到本地浏览器的传送协议</li>
<li>由万维网协会(World Wide Web Consortium)和Internet工作小组(Internet Envineering Task Force)共同制定的规范</li>
</ul></li>
<li>HTTPS(Hyper Text Transfer Protocol over Secure Socket Layer)
<ul>
<li>以安全为目的的HTTP通道，是HTTP下加入SSL层进行加密</li>
<li>建立了一个信息安全的通道来保证数据传输的安全</li>
<li>可以确认网站的真实性: 使用HTTPS的网站都可以通过浏览器地址栏的锁头标志查看网站认证之后的真实信息，也可以通过CA(Certificate Authority)机构颁发的安全签章来查询</li>
<li>在爬取使用HTTPS的网站时，需要设置忽略证书的选项，否则会提示SSL链接错误</li>
</ul></li>
</ol></li>
<li>HTTP请求过程
<ol type="1">
<li>浏览器输入URL会向服务器发送请求</li>
<li>服务器收到请求后进行处理和解析</li>
<li>服务器返回对应的响应，响应里包括页面源代码等内容</li>
<li>浏览器对响应进行解析，将网页呈现出来</li>
</ol></li>
<li>请求
<ol type="1">
<li>请求方法(Request Method)
<ul>
<li><code>GET</code>请求: 请求参数直接包含到URL里面，提交的数据最多1024字节</li>
<li><code>POST</code>请求: 请求参数大多在表单提交，URL中无法直接看到，数据量没有限制</li>
<li>还有比如<code>HEAD</code>, <code>PUT</code>, <code>DELETE</code>, <code>CONNECT</code>, <code>OPTIONS</code>, <code>TRACE</code>等请求</li>
</ul></li>
<li>请求的网址(Request URL)</li>
<li>请求头(Request Head)
<ul>
<li><code>Cookie</code>:网站为了辨别用户进行会话跟踪而储存在用户本地的数据，它的主要功能是维持当前访问的会话，每次访问服务器，都会在请求头中加入Cookie，以供服务器进行识别。比如登录状态的保持</li>
<li><code>User-Aget(UA)</code>: 特殊的字符串头，可以是服务器识别用户的操作系统，浏览器版本等信息。爬虫时加上这个信息可以伪装浏览器，否则很容易被识别出来</li>
<li><code>Referer</code>: 标识请求从哪个页面发过来的</li>
<li>其他：<code>Accept</code>, <code>Accept-Language</code>, <code>Accept-Encoding</code>, <code>Host</code>, <code>Content-Type(POST请求注意写对此项)</code></li>
</ul></li>
<li>请求体(Request Body)
<ul>
<li>对POST请求，请求体包含了表单中的内容，对于GET请求，请求体为空</li>
</ul></li>
</ol></li>
<li>响应
<ol type="1">
<li>响应状态码(Response Status Code)
<ul>
<li>200: 成功，服务器成功处理请求</li>
<li>401: 未授权，请求没有进行身份验证或者验证未通过</li>
<li>403: 禁止访问，服务器拒绝此请求</li>
<li>404: 未找到，服务器找不到请求的页面</li>
<li>500: 服务器内部错误，服务器遇到错误，无法完成请求</li>
</ul></li>
<li>响应头(Response Head)
<ul>
<li>服务器对请求的应答信息</li>
<li>如: <code>Date</code>, <code>Last-Modified</code>, <code>Content-Encoding</code>, <code>Server</code>, <code>Content-Type</code>, <code>Set-Cookie(将此内容放到Cookies中，下次请求需要携带)</code>, <code>Expires</code></li>
</ul></li>
<li>响应体(Response Body)
<ul>
<li>响应的正文内容，爬虫需要解析的对象</li>
</ul></li>
</ol></li>
</ol>
<h2 id="网页基础">2 网页基础</h2>
<ol type="1">
<li>网页的组成
<ol type="1">
<li>三部分：HTML(骨架)，CSS(肌肉)，JavaScript(皮肤)</li>
<li>HTML(Hyper Text Marking Language)
<ul>
<li>不同类型的元素通过不同类型的标签来表示，各种标签通过排列组合以及嵌套形成了网页的框架</li>
</ul></li>
<li>CSS(Cascading Style Sheet)
<ul>
<li>当HTML引用了多个CSS文件，且他们有冲突的时候会按照层叠顺序进行处理</li>
<li>可以在HTML文件中通过<code>link</code>引入</li>
</ul></li>
<li>JavaScript
<ul>
<li>一中脚本语言，给网页提供动态效果</li>
<li>在HTML文件中通过<code>script</code>引入</li>
</ul></li>
</ol></li>
<li>网页的结构 <figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;IE=edge&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, initial-scale=1.0&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Document<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li>节点树和节点之间的关系
<ol type="1">
<li>DOM(Document Object Model)，文档对象模型，W3C DOM是中立于平台和语言的接口，它允许程序和脚本动态的访问和更新文档的内容，结构和样式</li>
<li>W3C DOM的三个不同部分:
<ul>
<li>核心DOM: 针对任何结构化文档的标准模型</li>
<li>XML DOM: 针对XMl文档的标准模型</li>
<li>HTML DOM： 针对HTML文档的标准模型</li>
</ul></li>
<li>HTML DOM:
<ul>
<li>整个文档是一个文档节点</li>
<li>每个HTML元素是一个元素节点</li>
<li>HTML元素内的文本是文本节点</li>
<li>每个HTML属性是属性节点</li>
<li>注释是注释节点</li>
<li>所有节点构成了HTML DOM树，他们可以通过JavaScript访问</li>
</ul></li>
</ol></li>
<li>CSS选择器
<ol type="1">
<li><code>.class</code> class选择器</li>
<li><code>#id</code> id选择器</li>
<li><code>*</code> 选择所有节点</li>
<li><code>element</code> 选择所有element节点</li>
<li><code>element1, element2</code> 选择所有element1节点和所有element2节点</li>
<li><code>element1 element2</code> 选择element1节点内部的所有element2节点</li>
<li><code>element1&gt;element2</code> 选择父节点为element1节点的所有element2节点</li>
<li><code>element1+element2</code> 选择紧邻element1节点之后的所有element2节点</li>
<li>...</li>
</ol></li>
</ol>
<h2 id="爬虫基本原理">3 爬虫基本原理</h2>
<ol type="1">
<li>爬虫概述
<ol type="1">
<li>简单来说，爬虫就是获取网页并提取和保存信息的自动化程序。</li>
<li>获取网页
<ul>
<li>就是获取网页的源代码</li>
<li>关键在于构造请求并发送给服务器，然后收到响应并解析出来</li>
</ul></li>
<li>提取信息
<ul>
<li>得到源代码之后需要从中提取我们想要的数据，最通用的方是就是使用正则表达式进行提取</li>
<li>还可以根据网页节点属性，CSS选择器，XPath提取等</li>
</ul></li>
<li>保存数据
<ul>
<li>可以保存成TXT文本或者JSON文本</li>
<li>也可以保存在数据库中</li>
</ul></li>
</ol></li>
<li>能抓取什么样的数据
<ol type="1">
<li>常规网页，得到的是HTML源代码</li>
<li>大多数API接口返回的是JSON字符串</li>
<li>图片，视频，音频等返回的是二进制数据</li>
<li>如CSS文件，JS文件，配置文件等也可以通过爬虫得到</li>
</ol></li>
<li>JavaScript渲染页面
<ol type="1">
<li>很多网页使用Ajax(Asynchronous Javascript And XML)，前端模块化工具来构建，整个网页由JavaScript渲染出来(在HTML中引入script，这样Javascript文件会改变HTML中节点，向其添加内容,最后得到完整页面)，原始HTML就是一个空壳，这会导致直接爬虫的源代码和浏览器中看到的不一样</li>
<li>普通爬虫库不会读取JavaScript文件，可以分析后台Ajax接口，也可以使用Selenium，Splash等库来实现模拟JavaScript渲染</li>
</ol></li>
<li>会话和Cookies
<ol type="1">
<li>动态网页和静态网页
<ul>
<li>静态网页：加载速度快，编写简单，可维护性差，不能根据URL灵活多变的显示内容</li>
<li>动态网页: 可以解析URL中参数的变化，关联数据库并动态呈现不同的页面内容，还可以实现用户登录，注册等功能</li>
</ul></li>
<li>无状态HTTP
<ul>
<li>HTTP对事务处理没有记忆能力，如果后续请求依赖于前面的请求，则必须重传前面的请求，这十分浪费资源</li>
<li>维持HTTP连接状态的技术：会话和Cookies</li>
<li>会话: 在服务端，用于保存用户的会话信息
<ul>
<li>用户在应用程序的不同Web页面跳转时，会话一直维持</li>
<li>如果用户还没创建会话，则Web服务器将自动创建一个会话</li>
<li>当会话过期或被放弃以后，服务器将终止会话</li>
</ul></li>
<li>Cookies: 在客户端(浏览器)，是网站为了辨别用户身份，进行会话追踪而储存在用户本地终端的数据
<ul>
<li>用户第一次请求，响应头里会有<code>Set-Cookies</code>，用来标记用户身份，客户端会把Cookies保存到本地</li>
<li>以后请求的时候，会把Cookies放入请求头，服务器检查Cookies可以找到对应会话是什么，再判断会话来识别用户状态(比如是否登录等)</li>
<li>Cookies结构
<ul>
<li><code>Name</code>: 一经创建不能修改</li>
<li><code>Value</code>: 如果是Unicode数据，需要字符编码，如果是二进制数据，需要BASE64编码</li>
<li><code>Domain</code>: 可以访问Cookie的域名</li>
<li><code>Max Age</code>: 如果正数，则Max Age秒之后失效，如果负数，则关闭浏览器Cookie立刻失效</li>
<li><code>Secure</code>: 默认false，true的时候需要HTTPS</li>
<li>...</li>
</ul></li>
</ul></li>
<li>关闭浏览器会话并不会消逝，服务器不会删除这个会话。只是大部分会话都是Max Age为负数，Cookies消失后下次就找不到会话了。如果是设置为Max Age很大的正数，则可以保存比如登录状态等。有时服务器会设置失效时间，超过失效时间以后，服务器才会删除这个会话。</li>
</ul></li>
</ol></li>
</ol>
<h2 id="代理基本原理">5 代理基本原理</h2>
<ol type="1">
<li>前言
<ul>
<li>如果不设置代理，服务器检测到某个IP单位时间内请求次数超过阈值的话，就会禁止访问，响应状态403，这就是封IP，还有可能需要输入验证码，这些都需要代理来伪装IP</li>
</ul></li>
<li>基本原理
<ul>
<li>在客户端和服务器之间加入代理服务器，服务器识别的是代理服务器</li>
</ul></li>
<li>代理分类
<ol type="1">
<li>根据协议区分
<ul>
<li>FTP代理: 主要用于访问FTP服务器</li>
<li>HTTP代理: 主要用于访问网页</li>
<li>SSL/TSL代理: 主要用于访问加密网站</li>
<li>RTSP代理: 主要用于访问Real流媒体服务器</li>
<li>Telnet代理: 主要用于telnet远程控制</li>
<li>POP3/SMTP代理: 主要用于POP3/SMPT方式收发邮件</li>
<li>SOCKS代理: 单纯传递数据包，不关心具体协议和用法</li>
</ul></li>
<li>根据匿名程度划分
<ul>
<li>高度匿名代理: 原封不动转发数据包，完全模拟普通客户端</li>
<li>普通匿名代理: 会在数据包进行改动，服务器可能发现代理</li>
<li>透明代理: 改动数据包，并告知服务器客户端的真实IP</li>
<li>间谍代理</li>
</ul></li>
</ol></li>
</ol>
<h1 id="chapter-3-基本库的使用">Chapter 3 基本库的使用</h1>
<h2 id="使用urllib">1 使用<code>urllib</code></h2>
<ol type="1">
<li>前言
<ol type="1">
<li>urllib是Python内置的HTTP请求库，不需要额外安装就可以使用</li>
<li>它由4个模块构成
<ol type="1">
<li><code>request</code>: 基本的HTTP请求模块，可以模拟发送请求</li>
<li><code>error</code>: 异常处理模块</li>
<li><code>parse</code>: 工具模块，提供了很多URL处理方法，比如拆分，解析，合并等</li>
<li><code>robotparser</code>: 主要用来识别网站的<code>robots.txt</code>文件，然后判断网站是否可以爬</li>
</ol></li>
</ol></li>
<li>发送请求
<ol type="1">
<li><code>urlopen()</code>
<ul>
<li>可以用来模拟浏览器请求发起过程，并且可以处理授权验证(authentication)，重定向(redirection)，浏览器Cookies等内容</li>
<li>返回一个<code>http.client.HTTPResponse</code>对象，它是一个字节流，需要解码，其charset是响应头Content-Type规定的</li>
<li>基本用法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"></span><br><span class="line">response = urlopen(<span class="string">&#x27;https://www.python.org&#x27;</span>) </span><br><span class="line">charset = response.headers.get_content_charset()  </span><br><span class="line">resource = responser.read().decode(charset)</span><br><span class="line"></span><br><span class="line">print(response.status)</span><br><span class="line">print(response.getheaders()) <span class="comment"># return tuples (header, value)</span></span><br><span class="line">print(response.getheader(<span class="string">&#x27;Server&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
<li>传递参数<code>data</code>变为POST模式 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">data = byptes(urlencode(&#123;<span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;hello&#x27;</span>&#125;), encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="comment"># 传入参数 key = word, value = hello，使用urlencode把字典变成字符串，然后再用bytes变成字节流</span></span><br><span class="line">response = urlopen(url=<span class="string">&#x27;https://httpbin.org/post&#x27;</span>, data=data)</span><br></pre></td></tr></table></figure></li>
<li>传递参数<code>timeout</code>设置超时时间，单位秒,如果超时会抛出<code>URLError</code>异常 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urlopen(url=<span class="string">&#x27;https://www.google.com&#x27;</span>,timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">&#x27;Time Out!&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><code>Request()</code>
<ul>
<li>可以传入headers和host等来进行伪装 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen, Request</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.httpbin.org/post&#x27;</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;httpbin.org&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 另一种写法是用request.add_header(&#x27;key&#x27;,&#x27;value&#x27;)</span></span><br><span class="line"><span class="built_in">dict</span> = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;America&#x27;</span>&#125;</span><br><span class="line">data = <span class="built_in">bytes</span>(urlencode(<span class="built_in">dict</span>), encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">request = Request(url=url,headers=headers,data=data,method=<span class="string">&#x27;POST&#x27;</span>)</span><br><span class="line">response = urlopen(request)</span><br><span class="line">print(response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>使用Handler进行高级操作
<ul>
<li><code>urllib.request.BaseHandler</code>类是其他Handler类的父类</li>
<li>在遇到需要登录验证的时候，使用<code>urllib.request.HTTPBasicAuthHandler</code>来处理</li>
<li>在爬虫需要添加代理的时候，使用<code>urllib.request.ProxyHandler</code>来处理</li>
<li>处理Cookies的时候，使用<code>urllib.request.HTTPCookieProcessor</code></li>
</ul></li>
</ol></li>
<li>处理异常
<ol type="1">
<li>前言
<ul>
<li><code>urllib.error</code>模块定义了由request模块产生的异常。如果出现了问题，request模块就会抛出error模块中定义的异常</li>
</ul></li>
<li><code>URLError</code>
<ul>
<li>URLError继承了OSError类，是error异常模块的基类，所有由request模块产生的异常都可以由它来处理</li>
<li>它有1个<code>reason</code>属性，返回错误的原因 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urlopen(url=<span class="string">&#x27;https://www.google.com&#x27;</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><code>HTTPError</code>
<ul>
<li>HTTPError是URLError的子类，专门处理HTTP请求错误</li>
<li>它有3个属性:
<ul>
<li><code>code</code>: 返回HTTP响应状态码</li>
<li><code>reason</code>: 返回错误的原因</li>
<li><code>headers</code>: 返回请求头</li>
</ul></li>
<li>应该先捕获子类HTTPError，再捕获父类URLError <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> HTTPError</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urlopen(url=<span class="string">&#x27;https://test.github.io/about_me.html&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.code, e.reason, e.headers, sep=<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&#x27;Connected successfully!&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
<li>有时候reason不一定是字符串，可能是一个对象 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urlopen(url=<span class="string">&#x27;https://test.github.io/index.html&#x27;</span>,timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">&#x27;Time Out!&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol></li>
<li>解析连接
<ol type="1">
<li>前言
<ul>
<li><code>urllib.parse</code>模块定义了处理URL的标准接口</li>
</ul></li>
<li><code>urlparse()</code>
<ul>
<li>实现了URL的识别和分段</li>
<li>返回一个<code>urllib.parse.ParseResult</code>对象，它是一个长度为6的元组，可以用index或者名字调用item</li>
<li>URL标准格式: <code>scheme://netloc/path[;params][?query]#comment</code>
<ul>
<li><code>scheme</code>:协议，<code>://</code>之前的</li>
<li><code>netloc</code>：域名，第一个<code>/</code>之前的</li>
<li><code>path</code>: 访问路径，第一个<code>/</code>之后的</li>
<li><code>params</code>: 参数， 用<code>;</code>之后的</li>
<li><code>query</code>: 查询条件，<code>?</code>之后的</li>
<li><code>comment</code>: 锚点，页面内部的某个节点, <code>#</code>之后的 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.google.com;user?id=5#comment&#x27;</span></span><br><span class="line">result = urlparse(url=url)  </span><br><span class="line">print(result)</span><br><span class="line">print(result[<span class="number">0</span>]) <span class="comment"># print(result.scheme)</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>参数
<ul>
<li><code>url</code>: URLString</li>
<li><code>scheme</code>: 在URL未指定scheme的时候需要这个参数</li>
<li><code>allow_fragments</code>: 如果是False，则fragment会和前面一项(params和query是可选的，因此前一项可能直接就是path)连在一起，并且结果的fragment为空 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;www.google.com;user?id=5#comment&#x27;</span></span><br><span class="line">result = urlparse(url=url,scheme=<span class="string">&#x27;https&#x27;</span>,allow_fragments=<span class="literal">False</span>)  </span><br><span class="line">print(result)</span><br><span class="line">print(result[<span class="number">0</span>]) <span class="comment"># https</span></span><br><span class="line">print(result.query) <span class="comment"># id=5#comment</span></span><br><span class="line">print(result.fragment) <span class="comment"># None</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
<li><code>urlunparse()</code>
<ul>
<li>把一个长度为6的iterable object整合成一个URL，就是urlparse()反过来</li>
</ul></li>
<li><code>urlsplit()</code>
<ul>
<li>和urlparse()的区别在于不会解析params部分，如果有这一部分的话，直接放在path里面</li>
<li>返回<code>urllib.parse.SplitResult</code>对象，它是一个长度为5的元组，可以用index或者名字调用item</li>
</ul></li>
<li><code>urlunsplit()</code>
<ul>
<li>把一个长度为5的iterable object整合成一个URL，就是urlsplit()反过来</li>
</ul></li>
<li><code>urljoin()</code>
<ul>
<li>根据base_url的scheme, netloc, path对目标URL缺失部分(目标url已有但和base_url不同的以目标URL为准)进行补全，base_url的params, query, fragment对目标url没有影响</li>
<li>返回一个string</li>
</ul></li>
<li><code>urlencode()</code>
<ul>
<li>在构造GET请求参数的时候十分方便，可以把字典转化成query_string <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;america&#x27;</span></span><br><span class="line">    <span class="string">&#x27;age&#x27;</span>: <span class="string">&#x27;200&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;https://www.google.com&#x27;</span> + urlencode(params)</span><br><span class="line">print(url)       <span class="comment"># https://www.google.com?name=america&amp;age=200</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><code>parse_qs()</code>
<ul>
<li>把GET请求query_string转成字典 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line"></span><br><span class="line">query = <span class="string">&#x27;name=america&amp;age=200&#x27;</span></span><br><span class="line">print(parse_qs(query))       <span class="comment"># &#123;&#x27;name&#x27; : &#x27;america&#x27;, &#x27;age&#x27;: 200&#125;</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><code>parse_qsl()</code>
<ul>
<li>把GET请求query_string转成列表，列表中每一项是(key,value)形式的元组 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qsl</span><br><span class="line"></span><br><span class="line">query = <span class="string">&#x27;name=america&amp;age=200&#x27;</span></span><br><span class="line">print(parse_qsl(query))       <span class="comment"># [(&#x27;name&#x27;, &#x27;america&#x27;), (&#x27;age&#x27;, 200)]</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><code>quote()</code>
<ul>
<li>把任意编码内容转换成URL编码的格式，可以用来解决中文乱码的问题 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line">word = <span class="string">&#x27;乱码&#x27;</span></span><br><span class="line">url = <span class="string">&#x27;https://wwww.google.com?wd=&#x27;</span> + quote(word)  <span class="comment"># https://www.google.com?wd=%E4%B9%B1%E7%A0%81</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><code>unquote()</code>
<ul>
<li>quote的反面，它可以进行URL编码 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> unquote</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.google.com?wd=%E4%B9%B1%E7%A0%81&#x27;</span></span><br><span class="line">print(unquote(url))  <span class="comment"># https://www.google.com?wd=乱码</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol></li>
<li>Robots协议
<ol type="1">
<li>前言
<ul>
<li>Robots协议(Robots Exclusion Protocol)，用来告诉爬虫和搜索引擎哪些页面可以爬取，哪些不可以爬取。它通常是一个<code>robots.txt</code>的文本文件，一般放在网站的根目录下面</li>
</ul></li>
<li>例子
<ul>
<li>禁止所有爬虫 <figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">User-agent: * </span><br><span class="line">Disallow: /</span><br></pre></td></tr></table></figure></li>
<li>允许所有爬虫 <figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">User-agent: * </span><br><span class="line">Disallow: </span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>爬虫名称
<ul>
<li>BaiduSpider</li>
<li>Googlebot</li>
<li>bingbot</li>
<li>Slurp(Yahoo)</li>
<li>msnbot</li>
</ul></li>
<li><code>robotparser</code>
<ul>
<li>可以使用<code>urllib.robotparser.RobotFileParser()</code>来判断是否有权限爬取某个网页 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line">rp = RobotFileParser(url=<span class="string">&#x27;https://www.google.com/robots.txt&#x27;</span>)</span><br><span class="line"><span class="comment"># 开始解析，必须有rp.read()这一行，否则下面判断都是False</span></span><br><span class="line">rp.read()</span><br><span class="line">print(rp.can_fetch(useragent=<span class="string">&#x27;*&#x27;</span>, url=<span class="string">&#x27;https:www.google.com&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol></li>
</ol>
<h2 id="使用requests">2 使用<code>requests</code></h2>
<ol type="1">
<li>基本用法
<ol type="1">
<li>使用urllib的时候，登陆验证，Cookies，设置代理都需要Opener和Handler，但是使用requests可以简单的搞定这些问题</li>
<li>GET请求
<ul>
<li>urllib中的urlopen()方法实际上是GET方是请求网页，而requests中的get()也可以实现同样的功能</li>
<li>get()返回<code>requests.models.Response</code> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">rp = requests.get(url=<span class="string">&#x27;https://www.google.com&#x27;</span>)</span><br><span class="line">print(rp.status_code)  <span class="comment"># 响应状态码</span></span><br><span class="line">print(rp.text)    <span class="comment"># 响应体内容</span></span><br><span class="line">print(rp.cookies)  <span class="comment"># Cookies</span></span><br><span class="line">print(rp.headers)  <span class="comment"># 响应头</span></span><br><span class="line">print(rp.url)  <span class="comment"># URL</span></span><br><span class="line">print(rp.history)  <span class="comment"># 请求历史</span></span><br></pre></td></tr></table></figure></li>
<li>使用data参数引入query_string，使用headers引入headers <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">query = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;america&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="number">200</span>&#125;</span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) Gecko/20100101 Firefox/47.0&#x27;</span>&#125;</span><br><span class="line">rp = requests.get(url=<span class="string">&#x27;https://www.google.com&#x27;</span>, params=query, headers=headers)</span><br></pre></td></tr></table></figure></li>
<li>抓取图片，视频，音频等二进制数据 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> get</span><br><span class="line"></span><br><span class="line">rp = get(url=<span class="string">&#x27;https://www.github.com/facivon.ico&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;favion.ico&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(rp.content)</span><br></pre></td></tr></table></figure></li>
<li>类似的有<code>post()</code>, <code>put()</code>, <code>delete()</code>等方法来实现POST，PUT，DELETE等请求</li>
</ul></li>
<li>POST请求
<ul>
<li>和urllib中Request()类似，requests中post()可以实现POST请求 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">data = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;germey&#x27;</span>, <span class="string">&#x27;age&#x27;</span>: <span class="string">&#x27;22&#x27;</span>&#125;</span><br><span class="line">r = requests.post(<span class="string">&quot;http://httpbin.org/post&quot;</span>, data=data)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>响应
<ul>
<li>requests.codes有内置状态码，可以用来和response.status_code比较判断状态</li>
<li>举例
<ul>
<li>200: <code>requests.codes.ok</code></li>
<li>403: <code>requests.codes.forbidden</code></li>
<li>404: <code>requests.codes.not_found</code></li>
<li>500: <code>requests.codes.internal_server_error</code></li>
</ul></li>
</ul></li>
</ol></li>
<li>高级用法
<ol type="1">
<li>文件上传 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">files = &#123;<span class="string">&#x27;file&#x27;</span>: <span class="built_in">open</span>(<span class="string">&#x27;favicon.ico&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>)&#125;</span><br><span class="line">r = requests.post(<span class="string">&#x27;http://httpbin.org/post&#x27;</span>, files=files)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure></li>
<li>Cookies
<ul>
<li>获取Cookies <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> request <span class="keyword">import</span> get</span><br><span class="line"></span><br><span class="line">rp = get(url=<span class="string">&#x27;https:www.google.com&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> rp.cookies.items():</span><br><span class="line">    print(key, <span class="string">&#x27;===&gt;&#x27;</span>, value)</span><br></pre></td></tr></table></figure></li>
<li>把cookies复制到headers中维持登录</li>
</ul></li>
<li>会话维持
<ul>
<li>每次调用get()或者post()都是开启新的会话，相当于开了两个浏览器，如果想维持同一个登录状态，除非手动在headers里面设置同一个cookies</li>
<li>另一个简单的维持同一个会话的方法是使用<code>requests.Session</code>，使用Session对象对同一个host发起多个请求的时候，他们复用一个TCP连接，这会提高效率 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Session, get</span><br><span class="line"></span><br><span class="line">session = Session()</span><br><span class="line">session.get(url=<span class="string">&#x27;https://www.google.com&#x27;</span>)</span><br><span class="line">rp = session.get(url=<span class="string">&#x27;https:www.google.com?q=test&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>SSL证书验证
<ul>
<li>使用requests中方法发送HTTPS请求的时候，它会检查SSL证书，这个由verify参数控制，默认值为True，如果证书没有官方CA机构信任，会出现证书验证错误的结果<code>requests.exceptions.SSLError</code>，可以手动设置verify为False，不过这时候会有警告，可以手动忽略警告 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> requests.packages <span class="keyword">import</span> urllib3</span><br><span class="line"></span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">rp = requests.get(<span class="string">&#x27;https://www.google.com&#x27;</span>, verify=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>代理设置
<ul>
<li>基本设置 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> get</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://10.10.1.10:3128&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;https://10.10.1.10:1080&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">rp = get(url=<span class="string">&#x27;https://www.google.com&#x27;</span>,proxies=proxies)</span><br></pre></td></tr></table></figure></li>
<li>代理需要登录验证的设置格式: <code>scheme://user:password@host:port</code> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> get</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">        <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://user:pwd@10.10.1.10:3128&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;https://user:pwd@10.10.1.10:1080&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">rp = get(url=<span class="string">&#x27;https://www.google.com&#x27;</span>,proxies=proxies)</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>超时设置
<ul>
<li>基本总时间设置 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> get</span><br><span class="line"></span><br><span class="line">rp = get(url=<span class="string">&#x27;https://www.google.com&#x27;</span>, timeout=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
<li>分成(connect, read)元组设置 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> get</span><br><span class="line"></span><br><span class="line">rp = get(url=<span class="string">&#x27;https://www.google.com&#x27;</span>, timeout=(<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure></li>
<li>永久等待: 不设置或设置为None <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> get</span><br><span class="line"></span><br><span class="line">rp1 = get(url=<span class="string">&#x27;https://www.google.com&#x27;</span>, timeout=<span class="literal">None</span>)</span><br><span class="line">rp2 = get(url=<span class="string">&#x27;https://www.google.com&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li>Prepared Request
<ul>
<li>和urllib中的Request类似，requests中的Prepare Request可以接受参数 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> requests <span class="keyword">import</span> Request, Session</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://httpbin.org/post&#x27;</span></span><br><span class="line">data = &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;germey&#x27;</span>&#125;</span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">s = Session()</span><br><span class="line">req = Request(<span class="string">&#x27;POST&#x27;</span>, url, data=data, headers=headers)</span><br><span class="line">prepped = s.prepare_request(req)</span><br><span class="line">r = s.send(prepped)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure> ## 3 正则表达式</li>
</ul></li>
</ol></li>
<li>可以使用regular expression 进行模式匹配
<ul>
<li>比如匹配URL: <code>[a-zA-Z]+://[^\s]*</code></li>
</ul></li>
<li>规则
<ul>
<li><code>\w</code>: 数字或字母或下划线 <code>\w</code> = <code>[a-zA-Z0-9_]</code></li>
<li><code>\W</code>：非数字非字母非下划线</li>
<li><code>\s</code>：空白字符 <code>\s</code> = <code>[\t\n\r\f]</code></li>
<li><code>\S</code>: 非空白字符</li>
<li><code>\d</code>: 数字 <code>\d</code> = <code>[0-9]</code></li>
<li><code>\D</code>: 非数字</li>
<li><code>^</code>: 一行的开头</li>
<li><code>$</code>: 一行的结尾</li>
<li><code>.</code>: 任意字符，除了换行符，如果需要匹配换行符要在函数中传入<code>re.S</code></li>
<li><code>[...]</code>: 一组字符</li>
<li><code>[^...]</code>: 非这一组字符中任意一个的字符</li>
<li><code>+</code>: 1次或多次，如<code>x+</code> = <code>x&#123;1,&#125;</code>匹配1个或多个的所有x，贪婪匹配，若非贪婪匹配使用<code>+?</code></li>
<li><code>*</code>: 0次或多次，如<code>x*</code> = <code>x&#123;0,&#125;</code>匹配0个或多个的所有x，贪婪匹配，若非贪婪匹配使用<code>*?</code></li>
<li><code>?</code>: 0次或1次，<code>x?</code> = <code>x&#123;0,1&#125;</code>匹配0个或1个下，如果想要匹配问号，使用<code>\?</code></li>
<li><code>&#123;n&#125;</code>: n次</li>
<li><code>&#123;n,&#125;</code>: 至少n次</li>
<li><code>&#123;n,m&#125;</code>: 至少n次，之多m次</li>
</ul></li>
<li><code>re</code>库
<ol type="1">
<li><code>match()</code>
<ul>
<li>从目标字符串起始位置开始匹配pattern，如果匹配到返回匹配的内容和匹配内容在原字符串的范围，未匹配到返回None <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> re <span class="keyword">import</span> match</span><br><span class="line"></span><br><span class="line">content = <span class="string">&#x27;Hello 123 4567 World_This is Regex Demo&#x27;</span></span><br><span class="line">pattern = <span class="string">&#x27;^Hello\s\d&#123;3&#125;\s\d&#123;4&#125;\s\w&#123;10&#125;&#x27;</span></span><br><span class="line">result = match(pattern, content)</span><br><span class="line">print(result.group()) <span class="comment"># Hello 123 4567 World_This</span></span><br><span class="line">print(result.span()) <span class="comment"># (0, 25)</span></span><br></pre></td></tr></table></figure></li>
<li>使用<code>()</code>分组提取匹配的内容 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> re <span class="keyword">import</span> match</span><br><span class="line"></span><br><span class="line">content = <span class="string">&#x27;Hello 123 4567 World_This is Regex Demo&#x27;</span></span><br><span class="line">pattern = <span class="string">&#x27;^Hello\s\(d&#123;3&#125;)\s(\d&#123;4&#125;)\s\w&#123;10&#125;&#x27;</span></span><br><span class="line">result = match(pattern, content)</span><br><span class="line">print(result.group()) <span class="comment"># Hello 123 4567 World_This</span></span><br><span class="line">print(result.span()) <span class="comment"># (0, 25) 氛围包含0，不包含25</span></span><br><span class="line">print(result.group(<span class="number">0</span>))  <span class="comment"># Hello 123 4567 World_This</span></span><br><span class="line">print(result.span(<span class="number">0</span>)) <span class="comment"># (0, 25)</span></span><br><span class="line">print(result.group(<span class="number">1</span>))  <span class="comment"># 123</span></span><br><span class="line">print(result.span(<span class="number">1</span>)) <span class="comment"># (6, 9)</span></span><br><span class="line">print(result.group(<span class="number">2</span>))  <span class="comment"># 4567</span></span><br><span class="line">print(result.span(<span class="number">3</span>)) <span class="comment"># (10, 14)</span></span><br></pre></td></tr></table></figure></li>
<li>在字符串中间部分尽可能使用非贪婪匹配，防止错误</li>
<li>修饰符，需要传入函数中作为参数
<ul>
<li><code>re.I</code>: 匹配对大小写不敏感</li>
<li><code>re.S</code>: 使得<code>.</code>可以匹配换行符</li>
<li>...</li>
</ul></li>
<li>转义字符，直接加入<code>\</code>即可</li>
</ul></li>
<li><code>search()</code>
<ul>
<li>从左边开始依次寻找匹配字符串，如果有匹配结果，则返回第一个匹配的字符串和位置，如果没找到，返回None</li>
</ul></li>
<li><code>findall()</code>
<ul>
<li>和search类似，但是会返回所有匹配到的结果及范围，或者None</li>
<li>所有匹配结果都是tuple，他们放在一个list里面</li>
</ul></li>
<li><code>sub()</code>
<ul>
<li>对字符串进行修改和替换 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> re <span class="keyword">import</span> sub</span><br><span class="line"></span><br><span class="line">content = <span class="string">&#x27;123sfjsddsfdsfdfsdfds&#x27;</span></span><br><span class="line"><span class="comment"># 删除的话只要替换成空字符串就行了</span></span><br><span class="line">content = sub(<span class="string">&#x27;\d+&#x27;</span>, <span class="string">&#x27;&#x27;</span>, conetnt)</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><code>compile()</code>
<ul>
<li>如果要重复使用一个pattern，没必要每次都手写，可以通过compile把string形式的regular expression变成一个<code>re.Pattern</code>对象，再所有需要传入pattern的地方都可以直接复用这个对象</li>
</ul></li>
</ol></li>
</ol>
<h1 id="chapter-4-解析库的使用">Chapter 4 解析库的使用</h1>
<h2 id="使用xpath">1 使用<code>XPath</code></h2>
<ol type="1">
<li>前言
<ul>
<li>XPath, 全称是XML Path Language，是一门在XML文档中查找信息的语言。最初用来搜寻XML文档，但是同样适用于HTML文档的搜索</li>
<li>XPath提供了简洁明了的路径选择表达式和众多内置函数</li>
</ul></li>
<li>常见规则及举例
<ol type="1">
<li><p>规则</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">表达式</th>
<th style="text-align: center;">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">nodename</td>
<td style="text-align: center;">选择所有叫做"nodename"的节点</td>
</tr>
<tr class="even">
<td style="text-align: center;">/</td>
<td style="text-align: center;">从根节点中选择</td>
</tr>
<tr class="odd">
<td style="text-align: center;">//</td>
<td style="text-align: center;">从当前节点选择，无论它们在文档中什么位置</td>
</tr>
<tr class="even">
<td style="text-align: center;">.</td>
<td style="text-align: center;">选择当前节点</td>
</tr>
<tr class="odd">
<td style="text-align: center;">..</td>
<td style="text-align: center;">选择当前节点的父节点</td>
</tr>
<tr class="even">
<td style="text-align: center;">@</td>
<td style="text-align: center;">选择属性</td>
</tr>
</tbody>
</table></li>
<li><p>举例</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">表达式</th>
<th style="text-align: center;">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">bookstore</td>
<td style="text-align: center;">选择所有叫做"bookstore"的节点</td>
</tr>
<tr class="even">
<td style="text-align: center;">/bookstore</td>
<td style="text-align: center;">选择根节点bookstore</td>
</tr>
<tr class="odd">
<td style="text-align: center;">bookstore/book</td>
<td style="text-align: center;">选择bookstore的所有孩子节点book</td>
</tr>
<tr class="even">
<td style="text-align: center;">//book</td>
<td style="text-align: center;">选择文档中的所有book节点</td>
</tr>
<tr class="odd">
<td style="text-align: center;">bookstore//book</td>
<td style="text-align: center;">选择bookstore的所有子孙节点book</td>
</tr>
<tr class="even">
<td style="text-align: center;">//<span class="citation" data-cites="lang">@lang</span></td>
<td style="text-align: center;">选择文档中所有叫做"lang"的属性</td>
</tr>
<tr class="odd">
<td style="text-align: center;">//title<span class="citation" data-cites="lang">[@lang='eng']</span></td>
<td style="text-align: center;">选择文档中所有属性lang的值为eng的title节点</td>
</tr>
</tbody>
</table></li>
<li><p>读取HTML数据准备解析</p>
<ul>
<li>lxml会自动对输入的文件进行语法检查和修正，补完成一个完整合法的HTML文件格式</li>
<li>得到的是<code>lxml.etree._ElementTree</code>对象，它是一个bytestream，需要decode才可读</li>
<li>读取string <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">text = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    &lt;div&gt;</span></span><br><span class="line"><span class="string">        &lt;ul&gt;</span></span><br><span class="line"><span class="string">            &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">            &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;</span></span><br><span class="line"><span class="string">        &lt;/ul&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">html = etree.HTML(text) <span class="comment"># 形成lxml.etree._ElementTree对象共后续解析使用</span></span><br><span class="line"><span class="comment"># 下面是转成人类可读格式</span></span><br><span class="line">html_string = etree.tostring(html)</span><br><span class="line">print(html_string.decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
<li>读取HTML文件 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="comment"># 这个文件内容就是上面的字符串里的标签</span></span><br><span class="line">html = etree.parse(<span class="string">&#x27;./test.html&#x27;</span>, etree.HTMLParser())</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>使用<code>xpath()</code>获取节点</p>
<ul>
<li>xpath会得到一个列表，其中每个元素都是满足要求的节点，每一个节点是<code>lxml.etree._Element</code>对象</li>
<li>所有节点 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">&#x27;./test.html&#x27;</span>, etree.HTMLParser())</span><br><span class="line"><span class="comment"># 这里改成 results = html.xpath(&#x27;//li&#x27;)就是获取所有li节点</span></span><br><span class="line">results = html.xpath(<span class="string">&#x27;//*&#x27;</span>)  </span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure></li>
<li>子节点和子孙节点 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">html = etree.parse(<span class="string">&#x27;./test.html&#x27;</span>, etree.HTMLParser())</span><br><span class="line">results = html.xpath(<span class="string">&#x27;//li/a&#x27;</span>) <span class="comment"># 获取所有li节点的a子节点</span></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result)</span><br><span class="line"></span><br><span class="line">results_another = html.xpath(<span class="string">&#x27;//ul//a&#x27;</span>) <span class="comment"># 获取所有ul节点的a子孙节点，效果和上面一样</span></span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol></li>
</ol>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Yanxuanshaozhu
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://yanxuanshaozhu.github.io/2021/07/05/Learn%20Python%20Web%20Scraper/" title="Learn Python Web Scraper">https://yanxuanshaozhu.github.io/2021/07/05/Learn Python Web Scraper/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Web-Scraper/" rel="tag"># Web Scraper</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/06/04/Princeton%20Algorithms%20Part%20Two%20Notes/" rel="prev" title="Princeton Algorithms Part Two Notes">
      <i class="fa fa-chevron-left"></i> Princeton Algorithms Part Two Notes
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#chapter-1-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="nav-number">1.</span> <span class="nav-text">Chapter 1 开发环境配置</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85python"><span class="nav-number">1.1.</span> <span class="nav-text">1 安装Python</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E8%AF%B7%E6%B1%82%E5%BA%93"><span class="nav-number">1.2.</span> <span class="nav-text">2 安装请求库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E8%A7%A3%E6%9E%90%E5%BA%93"><span class="nav-number">1.3.</span> <span class="nav-text">3 安装解析库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">1.4.</span> <span class="nav-text">4 安装数据库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E5%AD%98%E5%82%A8%E5%BA%93"><span class="nav-number">1.5.</span> <span class="nav-text">5 安装存储库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85web%E5%BA%93"><span class="nav-number">1.6.</span> <span class="nav-text">6 安装Web库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85app%E7%88%AC%E5%8F%96%E5%BA%93"><span class="nav-number">1.7.</span> <span class="nav-text">7 安装APP爬取库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6"><span class="nav-number">1.8.</span> <span class="nav-text">8 安装爬虫框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E7%9B%B8%E5%85%B3%E5%BA%93"><span class="nav-number">1.9.</span> <span class="nav-text">9 安装部署相关库</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#chapter-2-%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80"><span class="nav-number">2.</span> <span class="nav-text">Chapter 2 爬虫基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#http%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="nav-number">2.1.</span> <span class="nav-text">1 HTTP基本原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E9%A1%B5%E5%9F%BA%E7%A1%80"><span class="nav-number">2.2.</span> <span class="nav-text">2 网页基础</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="nav-number">2.3.</span> <span class="nav-text">3 爬虫基本原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%90%86%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="nav-number">2.4.</span> <span class="nav-text">5 代理基本原理</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#chapter-3-%E5%9F%BA%E6%9C%AC%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">3.</span> <span class="nav-text">Chapter 3 基本库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8urllib"><span class="nav-number">3.1.</span> <span class="nav-text">1 使用urllib</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8requests"><span class="nav-number">3.2.</span> <span class="nav-text">2 使用requests</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#chapter-4-%E8%A7%A3%E6%9E%90%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">4.</span> <span class="nav-text">Chapter 4 解析库的使用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8xpath"><span class="nav-number">4.1.</span> <span class="nav-text">1 使用XPath</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yanxuanshaozhu"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Yanxuanshaozhu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yanxuanshaozhu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yanxuanshaozhu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:mrlixm.cn@gmail.com" title="E-Mail → mailto:mrlixm.cn@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yanxuanshaozhu</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">332k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">5:02</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
